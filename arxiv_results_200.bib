@article{arxiv:REALM:_RAG-Driven_Enhancement_of_Multimodal_Electronic_Health_Records
__Analysis_via_Large_Language_Models,
    title = {REALM: RAG-Driven Enhancement of Multimodal Electronic Health Records
  Analysis via Large Language Models},
    author = {Yinghao Zhu, Changyu Ren, Shiyun Xie, Shukai Liu, Hangyuan Ji, Zixiang Wang, Tao Sun, Long He, Zhoujun Li, Xi Zhu, Chengwei Pan},
    abstract = {  The integration of multimodal Electronic Health Records (EHR) data has
significantly improved clinical predictive capabilities. Leveraging clinical
notes and multivariate time-series EHR, existing models often lack the medical
context relevent to clinical tasks, prompting the incorporation of external
knowledge, particularly from the knowledge graph (KG). Previous approaches with
KG knowledge have primarily focused on structured knowledge extraction,
neglecting unstructured data modalities and semantic high dimensional medical
knowledge. In response, we propose REALM, a Retrieval-Augmented Generation
(RAG) driven framework to enhance multimodal EHR representations that address
these limitations. Firstly, we apply Large Language Model (LLM) to encode long
context clinical notes and GRU model to encode time-series EHR data. Secondly,
we prompt LLM to extract task-relevant medical entities and match entities in
professionally labeled external knowledge graph (PrimeKG) with corresponding
medical knowledge. By matching and aligning with clinical standards, our
framework eliminates hallucinations and ensures consistency. Lastly, we propose
an adaptive multimodal fusion network to integrate extracted knowledge with
multimodal EHR data. Our extensive experiments on MIMIC-III mortality and
readmission tasks showcase the superior performance of our REALM framework over
baselines, emphasizing the effectiveness of each module. REALM framework
contributes to refining the use of multimodal EHR data in healthcare and
bridging the gap with nuanced medical context essential for informed clinical
predictions.
},
    year = {2024},
    month = {02},
    url = {http://arxiv.org/pdf/2402.07016v1},
    journal = {arXiv preprint}
}

@article{arxiv:EMERGE:_Enhancing_Multimodal_Electronic_Health_Records_Predictive
__Modeling_with_Retrieval-Augmented_Generation,
    title = {EMERGE: Enhancing Multimodal Electronic Health Records Predictive
  Modeling with Retrieval-Augmented Generation},
    author = {Yinghao Zhu, Changyu Ren, Zixiang Wang, Xiaochen Zheng, Shiyun Xie, Junlan Feng, Xi Zhu, Zhoujun Li, Liantao Ma, Chengwei Pan},
    abstract = {  The integration of multimodal Electronic Health Records (EHR) data has
significantly advanced clinical predictive capabilities. Existing models, which
utilize clinical notes and multivariate time-series EHR data, often fall short
of incorporating the necessary medical context for accurate clinical tasks,
while previous approaches with knowledge graphs (KGs) primarily focus on
structured knowledge extraction. In response, we propose EMERGE, a
Retrieval-Augmented Generation (RAG) driven framework to enhance multimodal EHR
predictive modeling. We extract entities from both time-series data and
clinical notes by prompting Large Language Models (LLMs) and align them with
professional PrimeKG, ensuring consistency. In addition to triplet
relationships, we incorporate entities' definitions and descriptions for richer
semantics. The extracted knowledge is then used to generate task-relevant
summaries of patients' health statuses. Finally, we fuse the summary with other
modalities using an adaptive multimodal fusion network with cross-attention.
Extensive experiments on the MIMIC-III and MIMIC-IV datasets' in-hospital
mortality and 30-day readmission tasks demonstrate the superior performance of
the EMERGE framework over baseline models. Comprehensive ablation studies and
analysis highlight the efficacy of each designed module and robustness to data
sparsity. EMERGE contributes to refining the utilization of multimodal EHR data
in healthcare, bridging the gap with nuanced medical contexts essential for
informed clinical predictions. We have publicly released the code at
https://github.com/yhzhu99/EMERGE.
},
    year = {2024},
    month = {05},
    url = {http://arxiv.org/pdf/2406.00036v2},
    journal = {arXiv preprint}
}

@article{arxiv:Is_larger_always_better?_Evaluating_and_prompting_large_language_models
__for_non-generative_medical_tasks,
    title = {Is larger always better? Evaluating and prompting large language models
  for non-generative medical tasks},
    author = {Yinghao Zhu, Junyi Gao, Zixiang Wang, Weibin Liao, Xiaochen Zheng, Lifang Liang, Yasha Wang, Chengwei Pan, Ewen M. Harrison, Liantao Ma},
    abstract = {  The use of Large Language Models (LLMs) in medicine is growing, but their
ability to handle both structured Electronic Health Record (EHR) data and
unstructured clinical notes is not well-studied. This study benchmarks various
models, including GPT-based LLMs, BERT-based models, and traditional clinical
predictive models, for non-generative medical tasks utilizing renowned
datasets. We assessed 14 language models (9 GPT-based and 5 BERT-based) and 7
traditional predictive models using the MIMIC dataset (ICU patient records) and
the TJH dataset (early COVID-19 EHR data), focusing on tasks such as mortality
and readmission prediction, disease hierarchy reconstruction, and biomedical
sentence matching, comparing both zero-shot and finetuned performance. Results
indicated that LLMs exhibited robust zero-shot predictive capabilities on
structured EHR data when using well-designed prompting strategies, frequently
surpassing traditional models. However, for unstructured medical texts, LLMs
did not outperform finetuned BERT models, which excelled in both supervised and
unsupervised tasks. Consequently, while LLMs are effective for zero-shot
learning on structured data, finetuned BERT models are more suitable for
unstructured texts, underscoring the importance of selecting models based on
specific task requirements and data characteristics to optimize the application
of NLP technology in healthcare.
},
    year = {2024},
    month = {07},
    url = {http://arxiv.org/pdf/2407.18525v1},
    journal = {arXiv preprint}
}

@article{arxiv:EHRmonize:_A_Framework_for_Medical_Concept_Abstraction_from_Electronic
__Health_Records_using_Large_Language_Models,
    title = {EHRmonize: A Framework for Medical Concept Abstraction from Electronic
  Health Records using Large Language Models},
    author = {Jo√£o Matos, Jack Gallifant, Jian Pei, A. Ian Wong},
    abstract = {  Electronic health records (EHRs) contain vast amounts of complex data, but
harmonizing and processing this information remains a challenging and costly
task requiring significant clinical expertise. While large language models
(LLMs) have shown promise in various healthcare applications, their potential
for abstracting medical concepts from EHRs remains largely unexplored. We
introduce EHRmonize, a framework leveraging LLMs to abstract medical concepts
from EHR data. Our study uses medication data from two real-world EHR databases
to evaluate five LLMs on two free-text extraction and six binary classification
tasks across various prompting strategies. GPT-4o's with 10-shot prompting
achieved the highest performance in all tasks, accompanied by Claude-3.5-Sonnet
in a subset of tasks. GPT-4o achieved an accuracy of 97% in identifying generic
route names, 82% for generic drug names, and 100% in performing binary
classification of antibiotics. While EHRmonize significantly enhances
efficiency, reducing annotation time by an estimated 60%, we emphasize that
clinician oversight remains essential. Our framework, available as a Python
package, offers a promising tool to assist clinicians in EHR data abstraction,
potentially accelerating healthcare research and improving data harmonization
processes.
},
    year = {2024},
    month = {06},
    url = {http://arxiv.org/pdf/2407.00242v1},
    journal = {arXiv preprint}
}

@article{arxiv:ColaCare:_Enhancing_Electronic_Health_Record_Modeling_through_Large
__Language_Model-Driven_Multi-Agent_Collaboration,
    title = {ColaCare: Enhancing Electronic Health Record Modeling through Large
  Language Model-Driven Multi-Agent Collaboration},
    author = {Zixiang Wang, Yinghao Zhu, Huiya Zhao, Xiaochen Zheng, Dehao Sui, Tianlong Wang, Wen Tang, Yasha Wang, Ewen Harrison, Chengwei Pan, Junyi Gao, Liantao Ma},
    abstract = {  We introduce ColaCare, a framework that enhances Electronic Health Record
(EHR) modeling through multi-agent collaboration driven by Large Language
Models (LLMs). Our approach seamlessly integrates domain-specific expert models
with LLMs to bridge the gap between structured EHR data and text-based
reasoning. Inspired by the Multidisciplinary Team (MDT) approach used in
clinical settings, ColaCare employs two types of agents: DoctorAgents and a
MetaAgent, which collaboratively analyze patient data. Expert models process
and generate predictions from numerical EHR data, while LLM agents produce
reasoning references and decision-making reports within the MDT-driven
collaborative consultation framework. The MetaAgent orchestrates the
discussion, facilitating consultations and evidence-based debates among
DoctorAgents, simulating diverse expertise in clinical decision-making. We
additionally incorporate the Merck Manual of Diagnosis and Therapy (MSD)
medical guideline within a retrieval-augmented generation (RAG) module for
medical evidence support, addressing the challenge of knowledge currency.
Extensive experiments conducted on three EHR datasets demonstrate ColaCare's
superior performance in clinical mortality outcome and readmission prediction
tasks, underscoring its potential to revolutionize clinical decision support
systems and advance personalized precision medicine. All code, case studies and
a questionnaire are available at the project website:
https://colacare.netlify.app.
},
    year = {2024},
    month = {10},
    url = {http://arxiv.org/pdf/2410.02551v2},
    journal = {arXiv preprint}
}

@article{arxiv:Retrospective_Comparative_Analysis_of_Prostate_Cancer_In-Basket
__Messages:_Responses_from_Closed-Domain_LLM_vs._Clinical_Teams,
    title = {Retrospective Comparative Analysis of Prostate Cancer In-Basket
  Messages: Responses from Closed-Domain LLM vs. Clinical Teams},
    author = {Yuexing Hao, Jason M. Holmes, Jared Hobson, Alexandra Bennett, Daniel K. Ebner, David M. Routman, Satomi Shiraishi, Samir H. Patel, Nathan Y. Yu, Chris L. Hallemeier, Brooke E. Ball, Mark R. Waddle, Wei Liu},
    abstract = {  In-basket message interactions play a crucial role in physician-patient
communication, occurring during all phases (pre-, during, and post) of a
patient's care journey. However, responding to these patients' inquiries has
become a significant burden on healthcare workflows, consuming considerable
time for clinical care teams. To address this, we introduce RadOnc-GPT, a
specialized Large Language Model (LLM) powered by GPT-4 that has been designed
with a focus on radiotherapeutic treatment of prostate cancer with advanced
prompt engineering, and specifically designed to assist in generating
responses. We integrated RadOnc-GPT with patient electronic health records
(EHR) from both the hospital-wide EHR database and an internal,
radiation-oncology-specific database. RadOnc-GPT was evaluated on 158
previously recorded in-basket message interactions. Quantitative natural
language processing (NLP) analysis and two grading studies with clinicians and
nurses were used to assess RadOnc-GPT's responses. Our findings indicate that
RadOnc-GPT slightly outperformed the clinical care team in "Clarity" and
"Empathy," while achieving comparable scores in "Completeness" and
"Correctness." RadOnc-GPT is estimated to save 5.2 minutes per message for
nurses and 2.4 minutes for clinicians, from reading the inquiry to sending the
response. Employing RadOnc-GPT for in-basket message draft generation has the
potential to alleviate the workload of clinical care teams and reduce
healthcare costs by producing high-quality, timely responses.
},
    year = {2024},
    month = {09},
    url = {http://arxiv.org/pdf/2409.18290v1},
    journal = {arXiv preprint}
}

@article{arxiv:A_GEN_AI_Framework_for_Medical_Note_Generation,
    title = {A GEN AI Framework for Medical Note Generation},
    author = {Hui Yi Leong, Yi Fan Gao, Shuai Ji, Bora Kalaycioglu, Uktu Pamuksuz},
    abstract = {  The increasing administrative burden of medical documentation, particularly
through Electronic Health Records (EHR), significantly reduces the time
available for direct patient care and contributes to physician burnout. To
address this issue, we propose MediNotes, an advanced generative AI framework
designed to automate the creation of SOAP (Subjective, Objective, Assessment,
Plan) notes from medical conversations. MediNotes integrates Large Language
Models (LLMs), Retrieval-Augmented Generation (RAG), and Automatic Speech
Recognition (ASR) to capture and process both text and voice inputs in real
time or from recorded audio, generating structured and contextually accurate
medical notes. The framework also incorporates advanced techniques like
Quantized Low-Rank Adaptation (QLoRA) and Parameter-Efficient Fine-Tuning
(PEFT) for efficient model fine-tuning in resource-constrained environments.
Additionally, MediNotes offers a query-based retrieval system, allowing
healthcare providers and patients to access relevant medical information
quickly and accurately. Evaluations using the ACI-BENCH dataset demonstrate
that MediNotes significantly improves the accuracy, efficiency, and usability
of automated medical documentation, offering a robust solution to reduce the
administrative burden on healthcare professionals while improving the quality
of clinical workflows.
},
    year = {2024},
    month = {09},
    url = {http://arxiv.org/pdf/2410.01841v1},
    journal = {arXiv preprint}
}

@article{arxiv:RGAR:_Recurrence_Generation-augmented_Retrieval_for_Factual-aware
__Medical_Question_Answering,
    title = {RGAR: Recurrence Generation-augmented Retrieval for Factual-aware
  Medical Question Answering},
    author = {Sichu Liang, Linhai Zhang, Hongyu Zhu, Wenwen Wang, Yulan He, Deyu Zhou},
    abstract = {  Medical question answering requires extensive access to specialized
conceptual knowledge. The current paradigm, Retrieval-Augmented Generation
(RAG), acquires expertise medical knowledge through large-scale corpus
retrieval and uses this knowledge to guide a general-purpose large language
model (LLM) for generating answers. However, existing retrieval approaches
often overlook the importance of factual knowledge, which limits the relevance
of retrieved conceptual knowledge and restricts its applicability in real-world
scenarios, such as clinical decision-making based on Electronic Health Records
(EHRs). This paper introduces RGAR, a recurrence generation-augmented retrieval
framework that retrieves both relevant factual and conceptual knowledge from
dual sources (i.e., EHRs and the corpus), allowing them to interact and refine
each another. Through extensive evaluation across three factual-aware medical
question answering benchmarks, RGAR establishes a new state-of-the-art
performance among medical RAG systems. Notably, the Llama-3.1-8B-Instruct model
with RGAR surpasses the considerably larger, RAG-enhanced GPT-3.5. Our findings
demonstrate the benefit of extracting factual knowledge for retrieval, which
consistently yields improved generation quality.
},
    year = {2025},
    month = {02},
    url = {http://arxiv.org/pdf/2502.13361v1},
    journal = {arXiv preprint}
}

@article{arxiv:Classifying_Cancer_Stage_with_Open-Source_Clinical_Large_Language_Models,
    title = {Classifying Cancer Stage with Open-Source Clinical Large Language Models},
    author = {Chia-Hsuan Chang, Mary M. Lucas, Grace Lu-Yao, Christopher C. Yang},
    abstract = {  Cancer stage classification is important for making treatment and care
management plans for oncology patients. Information on staging is often
included in unstructured form in clinical, pathology, radiology and other
free-text reports in the electronic health record system, requiring extensive
work to parse and obtain. To facilitate the extraction of this information,
previous NLP approaches rely on labeled training datasets, which are
labor-intensive to prepare. In this study, we demonstrate that without any
labeled training data, open-source clinical large language models (LLMs) can
extract pathologic tumor-node-metastasis (pTNM) staging information from
real-world pathology reports. Our experiments compare LLMs and a BERT-based
model fine-tuned using the labeled data. Our findings suggest that while LLMs
still exhibit subpar performance in Tumor (T) classification, with the
appropriate adoption of prompting strategies, they can achieve comparable
performance on Metastasis (M) classification and improved performance on Node
(N) classification.
},
    year = {2024},
    month = {04},
    url = {http://arxiv.org/pdf/2404.01589v1},
    journal = {arXiv preprint}
}

@article{arxiv:Lessons_Learned_on_Information_Retrieval_in_Electronic_Health_Records:_A
__Comparison_of_Embedding_Models_and_Pooling_Strategies,
    title = {Lessons Learned on Information Retrieval in Electronic Health Records: A
  Comparison of Embedding Models and Pooling Strategies},
    author = {Skatje Myers, Timothy A. Miller, Yanjun Gao, Matthew M. Churpek, Anoop Mayampurath, Dmitriy Dligach, Majid Afshar},
    abstract = {  Objective: Applying large language models (LLMs) to the clinical domain is
challenging due to the context-heavy nature of processing medical records.
Retrieval-augmented generation (RAG) offers a solution by facilitating
reasoning over large text sources. However, there are many parameters to
optimize in just the retrieval system alone. This paper presents an ablation
study exploring how different embedding models and pooling methods affect
information retrieval for the clinical domain.
  Methods: Evaluating on three retrieval tasks on two electronic health record
(EHR) data sources, we compared seven models, including medical- and
general-domain models, specialized encoder embedding models, and off-the-shelf
decoder LLMs. We also examine the choice of embedding pooling strategy for each
model, independently on the query and the text to retrieve.
  Results: We found that the choice of embedding model significantly impacts
retrieval performance, with BGE, a comparatively small general-domain model,
consistently outperforming all others, including medical-specific models.
However, our findings also revealed substantial variability across datasets and
query text phrasings. We also determined the best pooling methods for each of
these models to guide future design of retrieval systems.
  Discussion: The choice of embedding model, pooling strategy, and query
formulation can significantly impact retrieval performance and the performance
of these models on other public benchmarks does not necessarily transfer to new
domains. Further studies such as this one are vital for guiding
empirically-grounded development of retrieval frameworks, such as in the
context of RAG, for the clinical domain.
},
    year = {2024},
    month = {09},
    url = {http://arxiv.org/pdf/2409.15163v1},
    journal = {arXiv preprint}
}

@article{arxiv:CancerKG.ORG_A_Web-scale,_Interactive,_Verifiable_Knowledge_Graph-LLM
__Hybrid_for_Assisting_with_Optimal_Cancer_Treatment_and_Care,
    title = {CancerKG.ORG A Web-scale, Interactive, Verifiable Knowledge Graph-LLM
  Hybrid for Assisting with Optimal Cancer Treatment and Care},
    author = {Michael Gubanov, Anna Pyayt, Aleksandra Karolak},
    abstract = {  Here, we describe one of the first Web-scale hybrid Knowledge Graph
(KG)-Large Language Model (LLM), populated with the latest peer-reviewed
medical knowledge on colorectal Cancer. It is currently being evaluated to
assist with both medical research and clinical information retrieval tasks at
Moffitt Cancer Center, which is one of the top Cancer centers in the U.S. and
in the world. Our hybrid is remarkable as it serves the user needs better than
just an LLM, KG or a search-engine in isolation. LLMs as is are known to
exhibit hallucinations and catastrophic forgetting as well as are trained on
outdated corpora. The state of the art KGs, such as PrimeKG, cBioPortal,
ChEMBL, NCBI, and other require manual curation, hence are quickly getting
stale. CancerKG is unsupervised and is capable of automatically ingesting and
organizing the latest medical findings. To alleviate the LLMs shortcomings, the
verified KG serves as a Retrieval Augmented Generation (RAG) guardrail.
CancerKG exhibits 5 different advanced user interfaces, each tailored to serve
different data modalities better and more convenient for the user.
},
    year = {2024},
    month = {12},
    url = {http://arxiv.org/pdf/2501.00223v1},
    journal = {arXiv preprint}
}

@article{arxiv:AIPatient:_Simulating_Patients_with_EHRs_and_LLM_Powered_Agentic
__Workflow,
    title = {AIPatient: Simulating Patients with EHRs and LLM Powered Agentic
  Workflow},
    author = {Huizi Yu, Jiayan Zhou, Lingyao Li, Shan Chen, Jack Gallifant, Anye Shi, Xiang Li, Wenyue Hua, Mingyu Jin, Guang Chen, Yang Zhou, Zhao Li, Trisha Gupte, Ming-Li Chen, Zahra Azizi, Yongfeng Zhang, Themistocles L. Assimes, Xin Ma, Danielle S. Bitterman, Lin Lu, Lizhou Fan},
    abstract = {  Simulated patient systems play a crucial role in modern medical education and
research, providing safe, integrative learning environments and enabling
clinical decision-making simulations. Large Language Models (LLM) could advance
simulated patient systems by replicating medical conditions and patient-doctor
interactions with high fidelity and low cost. However, ensuring the
effectiveness and trustworthiness of these systems remains a challenge, as they
require a large, diverse, and precise patient knowledgebase, along with a
robust and stable knowledge diffusion to users. Here, we developed AIPatient,
an advanced simulated patient system with AIPatient Knowledge Graph (AIPatient
KG) as the input and the Reasoning Retrieval-Augmented Generation (Reasoning
RAG) agentic workflow as the generation backbone. AIPatient KG samples data
from Electronic Health Records (EHRs) in the Medical Information Mart for
Intensive Care (MIMIC)-III database, producing a clinically diverse and
relevant cohort of 1,495 patients with high knowledgebase validity (F1 0.89).
Reasoning RAG leverages six LLM powered agents spanning tasks including
retrieval, KG query generation, abstraction, checker, rewrite, and
summarization. This agentic framework reaches an overall accuracy of 94.15% in
EHR-based medical Question Answering (QA), outperforming benchmarks that use
either no agent or only partial agent integration. Our system also presents
high readability (median Flesch Reading Ease 77.23; median Flesch Kincaid Grade
5.6), robustness (ANOVA F-value 0.6126, p>0.1), and stability (ANOVA F-value
0.782, p>0.1). The promising performance of the AIPatient system highlights its
potential to support a wide range of applications, including medical education,
model evaluation, and system integration.
},
    year = {2024},
    month = {09},
    url = {http://arxiv.org/pdf/2409.18924v2},
    journal = {arXiv preprint}
}

@article{arxiv:medIKAL:_Integrating_Knowledge_Graphs_as_Assistants_of_LLMs_for_Enhanced
__Clinical_Diagnosis_on_EMRs,
    title = {medIKAL: Integrating Knowledge Graphs as Assistants of LLMs for Enhanced
  Clinical Diagnosis on EMRs},
    author = {Mingyi Jia, Junwen Duan, Yan Song, Jianxin Wang},
    abstract = {  Electronic Medical Records (EMRs), while integral to modern healthcare,
present challenges for clinical reasoning and diagnosis due to their complexity
and information redundancy. To address this, we proposed medIKAL (Integrating
Knowledge Graphs as Assistants of LLMs), a framework that combines Large
Language Models (LLMs) with knowledge graphs (KGs) to enhance diagnostic
capabilities. medIKAL assigns weighted importance to entities in medical
records based on their type, enabling precise localization of candidate
diseases within KGs. It innovatively employs a residual network-like approach,
allowing initial diagnosis by the LLM to be merged into KG search results.
Through a path-based reranking algorithm and a fill-in-the-blank style prompt
template, it further refined the diagnostic process. We validated medIKAL's
effectiveness through extensive experiments on a newly introduced open-sourced
Chinese EMR dataset, demonstrating its potential to improve clinical diagnosis
in real-world settings.
},
    year = {2024},
    month = {06},
    url = {http://arxiv.org/pdf/2406.14326v3},
    journal = {arXiv preprint}
}

@article{arxiv:MedRAG:_Enhancing_Retrieval-augmented_Generation_with_Knowledge
__Graph-Elicited_Reasoning_for_Healthcare_Copilot,
    title = {MedRAG: Enhancing Retrieval-augmented Generation with Knowledge
  Graph-Elicited Reasoning for Healthcare Copilot},
    author = {Xuejiao Zhao, Siyan Liu, Su-Yin Yang, Chunyan Miao},
    abstract = {  Retrieval-augmented generation (RAG) is a well-suited technique for
retrieving privacy-sensitive Electronic Health Records (EHR). It can serve as a
key module of the healthcare copilot, helping reduce misdiagnosis for
healthcare practitioners and patients. However, the diagnostic accuracy and
specificity of existing heuristic-based RAG models used in the medical domain
are inadequate, particularly for diseases with similar manifestations. This
paper proposes MedRAG, a RAG model enhanced by knowledge graph (KG)-elicited
reasoning for the medical domain that retrieves diagnosis and treatment
recommendations based on manifestations. MedRAG systematically constructs a
comprehensive four-tier hierarchical diagnostic KG encompassing critical
diagnostic differences of various diseases. These differences are dynamically
integrated with similar EHRs retrieved from an EHR database, and reasoned
within a large language model. This process enables more accurate and specific
decision support, while also proactively providing follow-up questions to
enhance personalized medical decision-making. MedRAG is evaluated on both a
public dataset DDXPlus and a private chronic pain diagnostic dataset (CPDD)
collected from Tan Tock Seng Hospital, and its performance is compared against
various existing RAG methods. Experimental results show that, leveraging the
information integration and relational abilities of the KG, our MedRAG provides
more specific diagnostic insights and outperforms state-of-the-art models in
reducing misdiagnosis rates. Our code will be available at
https://github.com/SNOWTEAM2023/MedRAG
},
    year = {2025},
    month = {02},
    url = {http://arxiv.org/pdf/2502.04413v1},
    journal = {arXiv preprint}
}

@article{arxiv:DualMAR:_Medical-Augmented_Representation_from_Dual-Expertise
__Perspectives,
    title = {DualMAR: Medical-Augmented Representation from Dual-Expertise
  Perspectives},
    author = {Pengfei Hu, Chang Lu, Fei Wang, Yue Ning},
    abstract = {  Electronic Health Records (EHR) has revolutionized healthcare data management
and prediction in the field of AI and machine learning. Accurate predictions of
diagnosis and medications significantly mitigate health risks and provide
guidance for preventive care. However, EHR driven models often have limited
scope on understanding medical-domain knowledge and mostly rely on
simple-and-sole ontologies. In addition, due to the missing features and
incomplete disease coverage of EHR, most studies only focus on basic analysis
on conditions and medication. We propose DualMAR, a framework that enhances EHR
prediction tasks through both individual observation data and public knowledge
bases. First, we construct a bi-hierarchical Diagnosis Knowledge Graph (KG)
using verified public clinical ontologies and augment this KG via Large
Language Models (LLMs); Second, we design a new proxy-task learning on lab
results in EHR for pretraining, which further enhance KG representation and
patient embeddings. By retrieving radial and angular coordinates upon polar
space, DualMAR enables accurate predictions based on rich hierarchical and
semantic embeddings from KG. Experiments also demonstrate that DualMAR
outperforms state-of-the-art models, validating its effectiveness in EHR
prediction and KG integration in medical domains.
},
    year = {2024},
    month = {10},
    url = {http://arxiv.org/pdf/2410.19955v1},
    journal = {arXiv preprint}
}

@article{arxiv:Question-Answering_Based_Summarization_of_Electronic_Health_Records
__using_Retrieval_Augmented_Generation,
    title = {Question-Answering Based Summarization of Electronic Health Records
  using Retrieval Augmented Generation},
    author = {Walid Saba, Suzanne Wendelken, James. Shanahan},
    abstract = {  Summarization of electronic health records (EHRs) can substantially minimize
'screen time' for both patients as well as medical personnel. In recent years
summarization of EHRs have employed machine learning pipelines using state of
the art neural models. However, these models have produced less than adequate
results that are attributed to the difficulty of obtaining sufficient annotated
data for training. Moreover, the requirement to consider the entire content of
an EHR in summarization has resulted in poor performance due to the fact that
attention mechanisms in modern large language models (LLMs) adds a quadratic
complexity in terms of the size of the input. We propose here a method that
mitigates these shortcomings by combining semantic search, retrieval augmented
generation (RAG) and question-answering using the latest LLMs. In our approach
summarization is the extraction of answers to specific questions that are
deemed important by subject-matter experts (SMEs). Our approach is quite
efficient; requires minimal to no training; does not suffer from the
'hallucination' problem of LLMs; and it ensures diversity, since the summary
will not have repeated content but diverse answers to specific questions.
},
    year = {2024},
    month = {01},
    url = {http://arxiv.org/pdf/2401.01469v1},
    journal = {arXiv preprint}
}

@article{arxiv:Automated_radiotherapy_treatment_planning_guided_by_GPT-4Vision,
    title = {Automated radiotherapy treatment planning guided by GPT-4Vision},
    author = {Sheng Liu, Oscar Pastor-Serrano, Yizheng Chen, Matthew Gopaulchan, Weixing Liang, Mark Buyyounouski, Erqi Pollom, Quynh-Thu Le, Michael Gensheimer, Peng Dong, Yong Yang, James Zou, Lei Xing},
    abstract = {  Radiotherapy treatment planning is a time-consuming and potentially
subjective process that requires the iterative adjustment of model parameters
to balance multiple conflicting objectives. Recent advancements in large
foundation models offer promising avenues for addressing the challenges in
planning and clinical decision-making. This study introduces GPT-RadPlan, a
fully automated treatment planning framework that harnesses prior radiation
oncology knowledge encoded in multi-modal large language models, such as
GPT-4Vision (GPT-4V) from OpenAI. GPT-RadPlan is made aware of planning
protocols as context and acts as an expert human planner, capable of guiding a
treatment planning process. Via in-context learning, we incorporate clinical
protocols for various disease sites as prompts to enable GPT-4V to acquire
treatment planning domain knowledge. The resulting GPT-RadPlan agent is
integrated into our in-house inverse treatment planning system through an API.
The efficacy of the automated planning system is showcased using multiple
prostate and head & neck cancer cases, where we compared GPT-RadPlan results to
clinical plans. In all cases, GPT-RadPlan either outperformed or matched the
clinical plans, demonstrating superior target coverage and organ-at-risk
sparing. Consistently satisfying the dosimetric objectives in the clinical
protocol, GPT-RadPlan represents the first multimodal large language model
agent that mimics the behaviors of human planners in radiation oncology
clinics, achieving remarkable results in automating the treatment planning
process without the need for additional training.
},
    year = {2024},
    month = {06},
    url = {http://arxiv.org/pdf/2406.15609v2},
    journal = {arXiv preprint}
}

@article{arxiv:GAMedX:_Generative_AI-based_Medical_Entity_Data_Extractor_Using_Large
__Language_Models,
    title = {GAMedX: Generative AI-based Medical Entity Data Extractor Using Large
  Language Models},
    author = {Mohammed-Khalil Ghali, Abdelrahman Farrag, Hajar Sakai, Hicham El Baz, Yu Jin, Sarah Lam},
    abstract = {  In the rapidly evolving field of healthcare and beyond, the integration of
generative AI in Electronic Health Records (EHRs) represents a pivotal
advancement, addressing a critical gap in current information extraction
techniques. This paper introduces GAMedX, a Named Entity Recognition (NER)
approach utilizing Large Language Models (LLMs) to efficiently extract entities
from medical narratives and unstructured text generated throughout various
phases of the patient hospital visit. By addressing the significant challenge
of processing unstructured medical text, GAMedX leverages the capabilities of
generative AI and LLMs for improved data extraction. Employing a unified
approach, the methodology integrates open-source LLMs for NER, utilizing
chained prompts and Pydantic schemas for structured output to navigate the
complexities of specialized medical jargon. The findings reveal significant
ROUGE F1 score on one of the evaluation datasets with an accuracy of 98\%. This
innovation enhances entity extraction, offering a scalable, cost-effective
solution for automated forms filling from unstructured data. As a result,
GAMedX streamlines the processing of unstructured narratives, and sets a new
standard in NER applications, contributing significantly to theoretical and
practical advancements beyond the medical technology sphere.
},
    year = {2024},
    month = {05},
    url = {http://arxiv.org/pdf/2405.20585v1},
    journal = {arXiv preprint}
}

@article{arxiv:Application_of_NotebookLM,_a_Large_Language_Model_with
__Retrieval-Augmented_Generation,_for_Lung_Cancer_Staging,
    title = {Application of NotebookLM, a Large Language Model with
  Retrieval-Augmented Generation, for Lung Cancer Staging},
    author = {Ryota Tozuka, Hisashi Johno, Akitomo Amakawa, Junichi Sato, Mizuki Muto, Shoichiro Seki, Atsushi Komaba, Hiroshi Onishi},
    abstract = {  Purpose: In radiology, large language models (LLMs), including ChatGPT, have
recently gained attention, and their utility is being rapidly evaluated.
However, concerns have emerged regarding their reliability in clinical
applications due to limitations such as hallucinations and insufficient
referencing. To address these issues, we focus on the latest technology,
retrieval-augmented generation (RAG), which enables LLMs to reference reliable
external knowledge (REK). Specifically, this study examines the utility and
reliability of a recently released RAG-equipped LLM (RAG-LLM), NotebookLM, for
staging lung cancer.
  Materials and methods: We summarized the current lung cancer staging
guideline in Japan and provided this as REK to NotebookLM. We then tasked
NotebookLM with staging 100 fictional lung cancer cases based on CT findings
and evaluated its accuracy. For comparison, we performed the same task using a
gold-standard LLM, GPT-4 Omni (GPT-4o), both with and without the REK.
  Results: NotebookLM achieved 86% diagnostic accuracy in the lung cancer
staging experiment, outperforming GPT-4o, which recorded 39% accuracy with the
REK and 25% without it. Moreover, NotebookLM demonstrated 95% accuracy in
searching reference locations within the REK.
  Conclusion: NotebookLM successfully performed lung cancer staging by
utilizing the REK, demonstrating superior performance compared to GPT-4o.
Additionally, it provided highly accurate reference locations within the REK,
allowing radiologists to efficiently evaluate the reliability of NotebookLM's
responses and detect possible hallucinations. Overall, this study highlights
the potential of NotebookLM, a RAG-LLM, in image diagnosis.
},
    year = {2024},
    month = {10},
    url = {http://arxiv.org/pdf/2410.10869v1},
    journal = {arXiv preprint}
}

@article{arxiv:Towards_Efficient_Patient_Recruitment_for_Clinical_Trials:_Application
__of_a_Prompt-Based_Learning_Model,
    title = {Towards Efficient Patient Recruitment for Clinical Trials: Application
  of a Prompt-Based Learning Model},
    author = {Mojdeh Rahmanian, Seyed Mostafa Fakhrahmad, Seyedeh Zahra Mousavi},
    abstract = {  Objective: Clinical trials are essential for advancing pharmaceutical
interventions, but they face a bottleneck in selecting eligible participants.
Although leveraging electronic health records (EHR) for recruitment has gained
popularity, the complex nature of unstructured medical texts presents
challenges in efficiently identifying participants. Natural Language Processing
(NLP) techniques have emerged as a solution with a recent focus on transformer
models. In this study, we aimed to evaluate the performance of a prompt-based
large language model for the cohort selection task from unstructured medical
notes collected in the EHR. Methods: To process the medical records, we
selected the most related sentences of the records to the eligibility criteria
needed for the trial. The SNOMED CT concepts related to each eligibility
criterion were collected. Medical records were also annotated with MedCAT based
on the SNOMED CT ontology. Annotated sentences including concepts matched with
the criteria-relevant terms were extracted. A prompt-based large language model
(Generative Pre-trained Transformer (GPT) in this study) was then used with the
extracted sentences as the training set. To assess its effectiveness, we
evaluated the model's performance using the dataset from the 2018 n2c2
challenge, which aimed to classify medical records of 311 patients based on 13
eligibility criteria through NLP techniques. Results: Our proposed model showed
the overall micro and macro F measures of 0.9061 and 0.8060 which were among
the highest scores achieved by the experiments performed with this dataset.
Conclusion: The application of a prompt-based large language model in this
study to classify patients based on eligibility criteria received promising
scores. Besides, we proposed a method of extractive summarization with the aid
of SNOMED CT ontology that can be also applied to other medical texts.
},
    year = {2024},
    month = {04},
    url = {http://arxiv.org/pdf/2404.16198v1},
    journal = {arXiv preprint}
}

@article{arxiv:Generating_patient_cohorts_from_electronic_health_records_using_two-step
__retrieval-augmented_text-to-SQL_generation,
    title = {Generating patient cohorts from electronic health records using two-step
  retrieval-augmented text-to-SQL generation},
    author = {Angelo Ziletti, Leonardo D'Ambrosi},
    abstract = {  Clinical cohort definition is crucial for patient recruitment and
observational studies, yet translating inclusion/exclusion criteria into SQL
queries remains challenging and manual. We present an automated system
utilizing large language models that combines criteria parsing, two-level
retrieval augmented generation with specialized knowledge bases, medical
concept standardization, and SQL generation to retrieve patient cohorts with
patient funnels. The system achieves 0.75 F1-score in cohort identification on
EHR data, effectively capturing complex temporal and logical relationships.
These results demonstrate the feasibility of automated cohort generation for
epidemiological research.
},
    year = {2025},
    month = {02},
    url = {http://arxiv.org/pdf/2502.21107v1},
    journal = {arXiv preprint}
}

@article{arxiv:Developing_an_Artificial_Intelligence_Tool_for_Personalized_Breast
__Cancer_Treatment_Plans_based_on_the_NCCN_Guidelines,
    title = {Developing an Artificial Intelligence Tool for Personalized Breast
  Cancer Treatment Plans based on the NCCN Guidelines},
    author = {Abdul M. Mohammed, Iqtidar Mansoor, Sarah Blythe, Dennis Trujillo},
    abstract = {  Cancer treatments require personalized approaches based on a patient's
clinical condition, medical history, and evidence-based guidelines. The
National Comprehensive Cancer Network (NCCN) provides frequently updated,
complex guidelines through visuals like flowcharts and diagrams, which can be
time consuming for oncologists to stay current with treatment protocols. This
study presents an AI (Artificial Intelligence)-driven methodology to accurately
automate treatment regimens following NCCN guidelines for breast cancer
patients.
  We proposed two AI-driven methods: Agentic-RAG (Retrieval-Augmented
Generation) and Graph-RAG. Agentic-RAG used a three-step Large Language Model
(LLM) process to select clinical titles from NCCN guidelines, retrieve matching
JSON content, and iteratively refine recommendations based on insufficiency
checks. Graph-RAG followed a Microsoft-developed framework with proprietary
prompts, where JSON data was converted to text via an LLM, summarized, and
mapped into graph structures representing key treatment relationships. Final
recommendations were generated by querying relevant graph summaries. Both were
evaluated using a set of patient descriptions, each with four associated
questions.
  As shown in Table 1, Agentic RAG achieved a 100% adherence (24/24) with no
hallucinations or incorrect treatments. Graph-RAG had 95.8% adherence (23/24)
with one incorrect treatment and no hallucinations. Chat GPT-4 showed 91.6%
adherence (22/24) with two wrong treatments and no hallucinations. Both Agentic
RAG and Graph-RAG provided detailed treatment recommendations with accurate
references to relevant NCCN document page numbers.
},
    year = {2025},
    month = {01},
    url = {http://arxiv.org/pdf/2502.15698v1},
    journal = {arXiv preprint}
}

@article{arxiv:Accuracy_and_Consistency_of_LLMs_in_the_Registered_Dietitian_Exam:_The
__Impact_of_Prompt_Engineering_and_Knowledge_Retrieval,
    title = {Accuracy and Consistency of LLMs in the Registered Dietitian Exam: The
  Impact of Prompt Engineering and Knowledge Retrieval},
    author = {Iman Azimi, Mohan Qi, Li Wang, Amir M. Rahmani, Youlin Li},
    abstract = {  Large language models (LLMs) are fundamentally transforming human-facing
applications in the health and well-being domains: boosting patient engagement,
accelerating clinical decision-making, and facilitating medical education.
Although state-of-the-art LLMs have shown superior performance in several
conversational applications, evaluations within nutrition and diet applications
are still insufficient. In this paper, we propose to employ the Registered
Dietitian (RD) exam to conduct a standard and comprehensive evaluation of
state-of-the-art LLMs, GPT-4o, Claude 3.5 Sonnet, and Gemini 1.5 Pro, assessing
both accuracy and consistency in nutrition queries. Our evaluation includes
1050 RD exam questions encompassing several nutrition topics and proficiency
levels. In addition, for the first time, we examine the impact of Zero-Shot
(ZS), Chain of Thought (CoT), Chain of Thought with Self Consistency (CoT-SC),
and Retrieval Augmented Prompting (RAP) on both accuracy and consistency of the
responses. Our findings revealed that while these LLMs obtained acceptable
overall performance, their results varied considerably with different prompts
and question domains. GPT-4o with CoT-SC prompting outperformed the other
approaches, whereas Gemini 1.5 Pro with ZS recorded the highest consistency.
For GPT-4o and Claude 3.5, CoT improved the accuracy, and CoT-SC improved both
accuracy and consistency. RAP was particularly effective for GPT-4o to answer
Expert level questions. Consequently, choosing the appropriate LLM and
prompting technique, tailored to the proficiency level and specific domain, can
mitigate errors and potential risks in diet and nutrition chatbots.
},
    year = {2024},
    month = {08},
    url = {http://arxiv.org/pdf/2408.02964v2},
    journal = {arXiv preprint}
}

@article{arxiv:Hybrid_Student-Teacher_Large_Language_Model_Refinement_for_Cancer
__Toxicity_Symptom_Extraction,
    title = {Hybrid Student-Teacher Large Language Model Refinement for Cancer
  Toxicity Symptom Extraction},
    author = {Reza Khanmohammadi, Ahmed I. Ghanem, Kyle Verdecchia, Ryan Hall, Mohamed Elshaikh, Benjamin Movsas, Hassan Bagher-Ebadian, Bing Luo, Indrin J. Chetty, Tuka Alhanai, Kundan Thind, Mohammad M. Ghassemi},
    abstract = {  Large Language Models (LLMs) offer significant potential for clinical symptom
extraction, but their deployment in healthcare settings is constrained by
privacy concerns, computational limitations, and operational costs. This study
investigates the optimization of compact LLMs for cancer toxicity symptom
extraction using a novel iterative refinement approach. We employ a
student-teacher architecture, utilizing Zephyr-7b-beta and Phi3-mini-128 as
student models and GPT-4o as the teacher, to dynamically select between prompt
refinement, Retrieval-Augmented Generation (RAG), and fine-tuning strategies.
Our experiments on 294 clinical notes covering 12 post-radiotherapy toxicity
symptoms demonstrate the effectiveness of this approach. The RAG method proved
most efficient, improving average accuracy scores from 0.32 to 0.73 for
Zephyr-7b-beta and from 0.40 to 0.87 for Phi3-mini-128 during refinement. In
the test set, both models showed an approximate 0.20 increase in accuracy
across symptoms. Notably, this improvement was achieved at a cost 45 times
lower than GPT-4o for Zephyr and 79 times lower for Phi-3. These results
highlight the potential of iterative refinement techniques in enhancing the
capabilities of compact LLMs for clinical applications, offering a balance
between performance, cost-effectiveness, and privacy preservation in healthcare
settings.
},
    year = {2024},
    month = {08},
    url = {http://arxiv.org/pdf/2408.04775v1},
    journal = {arXiv preprint}
}

@article{arxiv:GraphCare:_Enhancing_Healthcare_Predictions_with_Personalized_Knowledge
__Graphs,
    title = {GraphCare: Enhancing Healthcare Predictions with Personalized Knowledge
  Graphs},
    author = {Pengcheng Jiang, Cao Xiao, Adam Cross, Jimeng Sun},
    abstract = {  Clinical predictive models often rely on patients' electronic health records
(EHR), but integrating medical knowledge to enhance predictions and
decision-making is challenging. This is because personalized predictions
require personalized knowledge graphs (KGs), which are difficult to generate
from patient EHR data. To address this, we propose \textsc{GraphCare}, an
open-world framework that uses external KGs to improve EHR-based predictions.
Our method extracts knowledge from large language models (LLMs) and external
biomedical KGs to build patient-specific KGs, which are then used to train our
proposed Bi-attention AugmenTed (BAT) graph neural network (GNN) for healthcare
predictions. On two public datasets, MIMIC-III and MIMIC-IV, \textsc{GraphCare}
surpasses baselines in four vital healthcare prediction tasks: mortality,
readmission, length of stay (LOS), and drug recommendation. On MIMIC-III, it
boosts AUROC by 17.6\% and 6.6\% for mortality and readmission, and F1-score by
7.9\% and 10.8\% for LOS and drug recommendation, respectively. Notably,
\textsc{GraphCare} demonstrates a substantial edge in scenarios with limited
data availability. Our findings highlight the potential of using external KGs
in healthcare prediction tasks and demonstrate the promise of
\textsc{GraphCare} in generating personalized KGs for promoting personalized
medicine.
},
    year = {2023},
    month = {05},
    url = {http://arxiv.org/pdf/2305.12788v3},
    journal = {arXiv preprint}
}

@article{arxiv:VeriFact:_Verifying_Facts_in_LLM-Generated_Clinical_Text_with_Electronic
__Health_Records,
    title = {VeriFact: Verifying Facts in LLM-Generated Clinical Text with Electronic
  Health Records},
    author = {Philip Chung, Akshay Swaminathan, Alex J. Goodell, Yeasul Kim, S. Momsen Reincke, Lichy Han, Ben Deverett, Mohammad Amin Sadeghi, Abdel-Badih Ariss, Marc Ghanem, David Seong, Andrew A. Lee, Caitlin E. Coombes, Brad Bradshaw, Mahir A. Sufian, Hyo Jung Hong, Teresa P. Nguyen, Mohammad R. Rasouli, Komal Kamra, Mark A. Burbridge, James C. McAvoy, Roya Saffary, Stephen P. Ma, Dev Dash, James Xie, Ellen Y. Wang, Clifford A. Schmiesing, Nigam Shah, Nima Aghaeepour},
    abstract = {  Methods to ensure factual accuracy of text generated by large language models
(LLM) in clinical medicine are lacking. VeriFact is an artificial intelligence
system that combines retrieval-augmented generation and LLM-as-a-Judge to
verify whether LLM-generated text is factually supported by a patient's medical
history based on their electronic health record (EHR). To evaluate this system,
we introduce VeriFact-BHC, a new dataset that decomposes Brief Hospital Course
narratives from discharge summaries into a set of simple statements with
clinician annotations for whether each statement is supported by the patient's
EHR clinical notes. Whereas highest agreement between clinicians was 88.5%,
VeriFact achieves up to 92.7% agreement when compared to a denoised and
adjudicated average human clinican ground truth, suggesting that VeriFact
exceeds the average clinician's ability to fact-check text against a patient's
medical record. VeriFact may accelerate the development of LLM-based EHR
applications by removing current evaluation bottlenecks.
},
    year = {2025},
    month = {01},
    url = {http://arxiv.org/pdf/2501.16672v1},
    journal = {arXiv preprint}
}

@article{arxiv:When_Raw_Data_Prevails:_Are_Large_Language_Model_Embeddings_Effective_in
__Numerical_Data_Representation_for_Medical_Machine_Learning_Applications?,
    title = {When Raw Data Prevails: Are Large Language Model Embeddings Effective in
  Numerical Data Representation for Medical Machine Learning Applications?},
    author = {Yanjun Gao, Skatje Myers, Shan Chen, Dmitriy Dligach, Timothy A Miller, Danielle Bitterman, Matthew Churpek, Majid Afshar},
    abstract = {  The introduction of Large Language Models (LLMs) has advanced data
representation and analysis, bringing significant progress in their use for
medical questions and answering. Despite these advancements, integrating
tabular data, especially numerical data pivotal in clinical contexts, into LLM
paradigms has not been thoroughly explored. In this study, we examine the
effectiveness of vector representations from last hidden states of LLMs for
medical diagnostics and prognostics using electronic health record (EHR) data.
We compare the performance of these embeddings with that of raw numerical EHR
data when used as feature inputs to traditional machine learning (ML)
algorithms that excel at tabular data learning, such as eXtreme Gradient
Boosting. We focus on instruction-tuned LLMs in a zero-shot setting to
represent abnormal physiological data and evaluating their utilities as feature
extractors to enhance ML classifiers for predicting diagnoses, length of stay,
and mortality. Furthermore, we examine prompt engineering techniques on
zero-shot and few-shot LLM embeddings to measure their impact comprehensively.
Although findings suggest the raw data features still prevails in medical ML
tasks, zero-shot LLM embeddings demonstrate competitive results, suggesting a
promising avenue for future research in medical applications.
},
    year = {2024},
    month = {08},
    url = {http://arxiv.org/pdf/2408.11854v2},
    journal = {arXiv preprint}
}

@article{arxiv:Universal_Abstraction:_Harnessing_Frontier_Models_to_Structure
__Real-World_Data_at_Scale,
    title = {Universal Abstraction: Harnessing Frontier Models to Structure
  Real-World Data at Scale},
    author = {Cliff Wong, Sam Preston, Qianchu Liu, Zelalem Gero, Jass Bagga, Sheng Zhang, Shrey Jain, Theodore Zhao, Yu Gu, Yanbo Xu, Sid Kiblawi, Roshanthi Weerasinghe, Rom Leidner, Kristina Young, Brian Piening, Carlo Bifulco, Tristan Naumann, Mu Wei, Hoifung Poon},
    abstract = {  The vast majority of real-world patient information resides in unstructured
clinical text, and the process of medical abstraction seeks to extract and
normalize structured information from this unstructured input. However,
traditional medical abstraction methods can require significant manual efforts
that can include crafting rules or annotating training labels, limiting
scalability. In this paper, we propose UniMedAbstractor (UMA), a zero-shot
medical abstraction framework leveraging Large Language Models (LLMs) through a
modular and customizable prompt template. We refer to our approach as universal
abstraction as it can quickly scale to new attributes through its universal
prompt template without curating attribute-specific training labels or rules.
We evaluate UMA for oncology applications, focusing on fifteen key attributes
representing the cancer patient journey, from short-context attributes (e.g.,
performance status, treatment) to complex long-context attributes requiring
longitudinal reasoning (e.g., tumor site, histology, TNM staging). Experiments
on real-world data show UMA's strong performance and generalizability. Compared
to supervised and heuristic baselines, UMA with GPT-4o achieves on average an
absolute 2-point F1/accuracy improvement for both short-context and
long-context attribute abstraction. For pathologic T staging, UMA even
outperforms the supervised model by 20 points in accuracy.
},
    year = {2025},
    month = {02},
    url = {http://arxiv.org/pdf/2502.00943v1},
    journal = {arXiv preprint}
}

@article{arxiv:Improving_Retrieval-Augmented_Generation_in_Medicine_with_Iterative
__Follow-up_Questions,
    title = {Improving Retrieval-Augmented Generation in Medicine with Iterative
  Follow-up Questions},
    author = {Guangzhi Xiong, Qiao Jin, Xiao Wang, Minjia Zhang, Zhiyong Lu, Aidong Zhang},
    abstract = {  The emergent abilities of large language models (LLMs) have demonstrated
great potential in solving medical questions. They can possess considerable
medical knowledge, but may still hallucinate and are inflexible in the
knowledge updates. While Retrieval-Augmented Generation (RAG) has been proposed
to enhance the medical question-answering capabilities of LLMs with external
knowledge bases, it may still fail in complex cases where multiple rounds of
information-seeking are required. To address such an issue, we propose
iterative RAG for medicine (i-MedRAG), where LLMs can iteratively ask follow-up
queries based on previous information-seeking attempts. In each iteration of
i-MedRAG, the follow-up queries will be answered by a conventional RAG system
and they will be further used to guide the query generation in the next
iteration. Our experiments show the improved performance of various LLMs
brought by i-MedRAG compared with conventional RAG on complex questions from
clinical vignettes in the United States Medical Licensing Examination (USMLE),
as well as various knowledge tests in the Massive Multitask Language
Understanding (MMLU) dataset. Notably, our zero-shot i-MedRAG outperforms all
existing prompt engineering and fine-tuning methods on GPT-3.5, achieving an
accuracy of 69.68% on the MedQA dataset. In addition, we characterize the
scaling properties of i-MedRAG with different iterations of follow-up queries
and different numbers of queries per iteration. Our case studies show that
i-MedRAG can flexibly ask follow-up queries to form reasoning chains, providing
an in-depth analysis of medical questions. To the best of our knowledge, this
is the first-of-its-kind study on incorporating follow-up queries into medical
RAG. The implementation of i-MedRAG is available at
https://github.com/Teddy-XiongGZ/MedRAG.
},
    year = {2024},
    month = {08},
    url = {http://arxiv.org/pdf/2408.00727v3},
    journal = {arXiv preprint}
}

@article{arxiv:Large_Language_Models_for_Biomedical_Knowledge_Graph_Construction:
__Information_extraction_from_EMR_notes,
    title = {Large Language Models for Biomedical Knowledge Graph Construction:
  Information extraction from EMR notes},
    author = {Vahan Arsenyan, Spartak Bughdaryan, Fadi Shaya, Kent Small, Davit Shahnazaryan},
    abstract = {  The automatic construction of knowledge graphs (KGs) is an important research
area in medicine, with far-reaching applications spanning drug discovery and
clinical trial design. These applications hinge on the accurate identification
of interactions among medical and biological entities. In this study, we
propose an end-to-end machine learning solution based on large language models
(LLMs) that utilize electronic medical record notes to construct KGs. The
entities used in the KG construction process are diseases, factors, treatments,
as well as manifestations that coexist with the patient while experiencing the
disease. Given the critical need for high-quality performance in medical
applications, we embark on a comprehensive assessment of 12 LLMs of various
architectures, evaluating their performance and safety attributes. To gauge the
quantitative efficacy of our approach by assessing both precision and recall,
we manually annotate a dataset provided by the Macula and Retina Institute. We
also assess the qualitative performance of LLMs, such as the ability to
generate structured outputs or the tendency to hallucinate. The results
illustrate that in contrast to encoder-only and encoder-decoder, decoder-only
LLMs require further investigation. Additionally, we provide guided prompt
design to utilize such LLMs. The application of the proposed methodology is
demonstrated on age-related macular degeneration.
},
    year = {2023},
    month = {01},
    url = {http://arxiv.org/pdf/2301.12473v2},
    journal = {arXiv preprint}
}

@article{arxiv:Leveraging_Medical_Knowledge_Graphs_Into_Large_Language_Models_for
__Diagnosis_Prediction:_Design_and_Application_Study,
    title = {Leveraging Medical Knowledge Graphs Into Large Language Models for
  Diagnosis Prediction: Design and Application Study},
    author = {Yanjun Gao, Ruizhe Li, Emma Croxford, John Caskey, Brian W Patterson, Matthew Churpek, Timothy Miller, Dmitriy Dligach, Majid Afshar},
    abstract = {  Electronic Health Records (EHRs) and routine documentation practices play a
vital role in patients' daily care, providing a holistic record of health,
diagnoses, and treatment. However, complex and verbose EHR narratives overload
healthcare providers, risking diagnostic inaccuracies. While Large Language
Models (LLMs) have showcased their potential in diverse language tasks, their
application in the healthcare arena needs to ensure the minimization of
diagnostic errors and the prevention of patient harm. In this paper, we outline
an innovative approach for augmenting the proficiency of LLMs in the realm of
automated diagnosis generation, achieved through the incorporation of a medical
knowledge graph (KG) and a novel graph model: Dr.Knows, inspired by the
clinical diagnostic reasoning process. We derive the KG from the National
Library of Medicine's Unified Medical Language System (UMLS), a robust
repository of biomedical knowledge. Our method negates the need for
pre-training and instead leverages the KG as an auxiliary instrument aiding in
the interpretation and summarization of complex medical concepts. Using
real-world hospital datasets, our experimental results demonstrate that the
proposed approach of combining LLMs with KG has the potential to improve the
accuracy of automated diagnosis generation. More importantly, our approach
offers an explainable diagnostic pathway, edging us closer to the realization
of AI-augmented diagnostic decision support systems.
},
    year = {2023},
    month = {08},
    url = {http://arxiv.org/pdf/2308.14321v2},
    journal = {arXiv preprint}
}

@article{arxiv:CMQCIC-Bench:_A_Chinese_Benchmark_for_Evaluating_Large_Language_Models
__in_Medical_Quality_Control_Indicator_Calculation,
    title = {CMQCIC-Bench: A Chinese Benchmark for Evaluating Large Language Models
  in Medical Quality Control Indicator Calculation},
    author = {Guangya Yu, Yanhao Li, Zongying Jiang, Yuxiong Jin, Li Dai, Yupian Lin, Ruihui Hou, Weiyan Zhang, Yongqi Fan, Qi Ye, Jingping Liu, Tong Ruan},
    abstract = {  Medical quality control indicators are essential to assess the qualifications
of healthcare institutions for medical services. With the impressive
performance of large language models (LLMs) like GPT-4 in the medical field,
leveraging these technologies for the Medical Quality Control Indicator
Calculation (MQCIC) presents a promising approach. In this work, (1) we
introduce a real-world task MQCIC and propose an open-source Chinese electronic
medical records (EMRs)-based dataset (CMQCIC-Bench) comprising 785 instances
and 76 indicators. (2) We propose a semi-automatic method to enhance the rule
representation. Then we propose the Clinical Facts-based Inferential Rule
(CF-IR) method that disentangles the clinical fact verification and inferential
rule reasoning actions. (3) We conduct comprehensive experiments on 20
representative LLMs, covering general and medical models. Our findings reveal
that CF-IR outperforms Chain-of-Thought methods in MQCIC tasks. (4) We conduct
an error analysis and investigate the capabilities of clinical fact
verification and inferential rule reasoning, providing insights to improve
performance in the MQCIC further. The dataset and code is available in this
repo https://anonymous.4open.science/r/C-MQCIC-1151.
},
    year = {2025},
    month = {02},
    url = {http://arxiv.org/pdf/2502.11703v1},
    journal = {arXiv preprint}
}

@article{arxiv:Onco-Retriever:_Generative_Classifier_for_Retrieval_of_EHR_Records_in
__Oncology,
    title = {Onco-Retriever: Generative Classifier for Retrieval of EHR Records in
  Oncology},
    author = {Shashi Kant Gupta, Aditya Basu, Bradley Taylor, Anai Kothari, Hrituraj Singh},
    abstract = {  Retrieving information from EHR systems is essential for answering specific
questions about patient journeys and improving the delivery of clinical care.
Despite this fact, most EHR systems still rely on keyword-based searches. With
the advent of generative large language models (LLMs), retrieving information
can lead to better search and summarization capabilities. Such retrievers can
also feed Retrieval-augmented generation (RAG) pipelines to answer any query.
However, the task of retrieving information from EHR real-world clinical data
contained within EHR systems in order to solve several downstream use cases is
challenging due to the difficulty in creating query-document support pairs. We
provide a blueprint for creating such datasets in an affordable manner using
large language models. Our method results in a retriever that is 30-50 F-1
points better than propriety counterparts such as Ada and Mistral for oncology
data elements. We further compare our model, called Onco-Retriever, against
fine-tuned PubMedBERT model as well. We conduct an extensive manual evaluation
on real-world EHR data along with latency analysis of the different models and
provide a path forward for healthcare organizations to build domain-specific
retrievers.
},
    year = {2024},
    month = {04},
    url = {http://arxiv.org/pdf/2404.06680v1},
    journal = {arXiv preprint}
}

@article{arxiv:KG-MTT-BERT:_Knowledge_Graph_Enhanced_BERT_for_Multi-Type_Medical_Text
__Classification,
    title = {KG-MTT-BERT: Knowledge Graph Enhanced BERT for Multi-Type Medical Text
  Classification},
    author = {Yong He, Cheng Wang, Shun Zhang, Nan Li, Zhaorong Li, Zhenyu Zeng},
    abstract = {  Medical text learning has recently emerged as a promising area to improve
healthcare due to the wide adoption of electronic health record (EHR) systems.
The complexity of the medical text such as diverse length, mixed text types,
and full of medical jargon, poses a great challenge for developing effective
deep learning models. BERT has presented state-of-the-art results in many NLP
tasks, such as text classification and question answering. However, the
standalone BERT model cannot deal with the complexity of the medical text,
especially the lengthy clinical notes. Herein, we develop a new model called
KG-MTT-BERT (Knowledge Graph Enhanced Multi-Type Text BERT) by extending the
BERT model for long and multi-type text with the integration of the medical
knowledge graph. Our model can outperform all baselines and other
state-of-the-art models in diagnosis-related group (DRG) classification, which
requires comprehensive medical text for accurate classification. We also
demonstrated that our model can effectively handle multi-type text and the
integration of medical knowledge graph can significantly improve the
performance.
},
    year = {2022},
    month = {10},
    url = {http://arxiv.org/pdf/2210.03970v1},
    journal = {arXiv preprint}
}

@article{arxiv:SPeC:_A_Soft_Prompt-Based_Calibration_on_Performance_Variability_of
__Large_Language_Model_in_Clinical_Notes_Summarization,
    title = {SPeC: A Soft Prompt-Based Calibration on Performance Variability of
  Large Language Model in Clinical Notes Summarization},
    author = {Yu-Neng Chuang, Ruixiang Tang, Xiaoqian Jiang, Xia Hu},
    abstract = {  Electronic health records (EHRs) store an extensive array of patient
information, encompassing medical histories, diagnoses, treatments, and test
outcomes. These records are crucial for enabling healthcare providers to make
well-informed decisions regarding patient care. Summarizing clinical notes
further assists healthcare professionals in pinpointing potential health risks
and making better-informed decisions. This process contributes to reducing
errors and enhancing patient outcomes by ensuring providers have access to the
most pertinent and current patient data. Recent research has shown that
incorporating prompts with large language models (LLMs) substantially boosts
the efficacy of summarization tasks. However, we show that this approach also
leads to increased output variance, resulting in notably divergent outputs even
when prompts share similar meanings. To tackle this challenge, we introduce a
model-agnostic Soft Prompt-Based Calibration (SPeC) pipeline that employs soft
prompts to diminish variance while preserving the advantages of prompt-based
summarization. Experimental findings on multiple clinical note tasks and LLMs
indicate that our method not only bolsters performance but also effectively
curbs variance for various LLMs, providing a more uniform and dependable
solution for summarizing vital medical information.
},
    year = {2023},
    month = {03},
    url = {http://arxiv.org/pdf/2303.13035v3},
    journal = {arXiv preprint}
}

@article{arxiv:Towards_Reliable_Medical_Question_Answering:_Techniques_and_Challenges
__in_Mitigating_Hallucinations_in_Language_Models,
    title = {Towards Reliable Medical Question Answering: Techniques and Challenges
  in Mitigating Hallucinations in Language Models},
    author = {Duy Khoa Pham, Bao Quoc Vo},
    abstract = {  The rapid advancement of large language models (LLMs) has significantly
impacted various domains, including healthcare and biomedicine. However, the
phenomenon of hallucination, where LLMs generate outputs that deviate from
factual accuracy or context, poses a critical challenge, especially in
high-stakes domains. This paper conducts a scoping study of existing techniques
for mitigating hallucinations in knowledge-based task in general and especially
for medical domains. Key methods covered in the paper include
Retrieval-Augmented Generation (RAG)-based techniques, iterative feedback
loops, supervised fine-tuning, and prompt engineering. These techniques, while
promising in general contexts, require further adaptation and optimization for
the medical domain due to its unique demands for up-to-date, specialized
knowledge and strict adherence to medical guidelines. Addressing these
challenges is crucial for developing trustworthy AI systems that enhance
clinical decision-making and patient safety as well as accuracy of biomedical
scientific research.
},
    year = {2024},
    month = {08},
    url = {http://arxiv.org/pdf/2408.13808v1},
    journal = {arXiv preprint}
}

@article{arxiv:FIND:_Fine-grained_Information_Density_Guided_Adaptive
__Retrieval-Augmented_Generation_for_Disease_Diagnosis,
    title = {FIND: Fine-grained Information Density Guided Adaptive
  Retrieval-Augmented Generation for Disease Diagnosis},
    author = {Mingyi Jia, Junwen Duan, Yan Song, Jianxin Wang},
    abstract = {  Retrieval-Augmented Large Language Models (LLMs), which integrate external
knowledge into LLMs, have shown remarkable performance in various medical
domains, including clinical diagnosis. However, existing RAG methods struggle
to effectively assess task difficulty to make retrieval decisions, thereby
failing to meet the clinical requirements for balancing efficiency and
accuracy. So in this paper, we propose FIND (\textbf{F}ine-grained
\textbf{In}formation \textbf{D}ensity Guided Adaptive RAG), a novel framework
that improves the reliability of RAG in disease diagnosis scenarios. FIND
incorporates a fine-grained adaptive control module to determine whether
retrieval is necessary based on the information density of the input. By
optimizing the retrieval process and implementing a knowledge filtering module,
FIND ensures that the retrieval is better suited to clinical scenarios.
Experiments on three Chinese electronic medical record datasets demonstrate
that FIND significantly outperforms various baseline methods, highlighting its
effectiveness in clinical diagnosis tasks.
},
    year = {2025},
    month = {02},
    url = {http://arxiv.org/pdf/2502.14614v1},
    journal = {arXiv preprint}
}

@article{arxiv:Open-Source_Retrieval_Augmented_Generation_Framework_for_Retrieving
__Accurate_Medication_Insights_from_Formularies_for_African_Healthcare_Workers,
    title = {Open-Source Retrieval Augmented Generation Framework for Retrieving
  Accurate Medication Insights from Formularies for African Healthcare Workers},
    author = {Axum AI,  :, J. Owoyemi, S. Abubakar, A. Owoyemi, T. O. Togunwa, F. C. Madubuko, S. Oyatoye, Z. Oyetolu, K. Akyea, A. O. Mohammed, A. Adebakin},
    abstract = {  Accessing accurate medication insights is vital for enhancing patient safety,
minimizing errors, and supporting clinical decision-making. However, healthcare
professionals in Africa often rely on manual and time-consuming processes to
retrieve drug information, exacerbated by limited access to pharmacists due to
brain drain and healthcare disparities. This paper presents "Drug Insights," an
open-source Retrieval-Augmented Generation (RAG) chatbot designed to streamline
medication lookup for healthcare workers in Africa. By leveraging a corpus of
Nigerian pharmaceutical data and advanced AI technologies, including Pinecone
databases and GPT models, the system delivers accurate, context-specific
responses with minimal hallucination. The chatbot integrates prompt engineering
and S-BERT evaluation to optimize retrieval and response generation.
Preliminary tests, including pharmacist feedback, affirm the tool's potential
to improve drug information access while highlighting areas for enhancement,
such as UI/UX refinement and extended corpus integration.
},
    year = {2025},
    month = {01},
    url = {http://arxiv.org/pdf/2502.15722v1},
    journal = {arXiv preprint}
}

@article{arxiv:Ontology-Constrained_Generation_of_Domain-Specific_Clinical_Summaries,
    title = {Ontology-Constrained Generation of Domain-Specific Clinical Summaries},
    author = {Gaya Mehenni, Amal Zouaq},
    abstract = {  Large Language Models (LLMs) offer promising solutions for text
summarization. However, some domains require specific information to be
available in the summaries. Generating these domain-adapted summaries is still
an open challenge. Similarly, hallucinations in generated content is a major
drawback of current approaches, preventing their deployment. This study
proposes a novel approach that leverages ontologies to create domain-adapted
summaries both structured and unstructured. We employ an ontology-guided
constrained decoding process to reduce hallucinations while improving
relevance. When applied to the medical domain, our method shows potential in
summarizing Electronic Health Records (EHRs) across different specialties,
allowing doctors to focus on the most relevant information to their domain.
Evaluation on the MIMIC-III dataset demonstrates improvements in generating
domain-adapted summaries of clinical notes and hallucination reduction.
},
    year = {2024},
    month = {11},
    url = {http://arxiv.org/pdf/2411.15666v1},
    journal = {arXiv preprint}
}

@article{arxiv:A_Large_Language_Model_Pipeline_for_Breast_Cancer_Oncology,
    title = {A Large Language Model Pipeline for Breast Cancer Oncology},
    author = {Tristen Pool, Dennis Trujillo},
    abstract = {  Large language models (LLMs) have demonstrated potential in the innovation of
many disciplines. However, how they can best be developed for oncology remains
underdeveloped. State-of-the-art OpenAI models were fine-tuned on a clinical
dataset and clinical guidelines text corpus for two important cancer treatment
factors, adjuvant radiation therapy and chemotherapy, using a novel Langchain
prompt engineering pipeline. A high accuracy (0.85+) was achieved in the
classification of adjuvant radiation therapy and chemotherapy for breast cancer
patients. Furthermore, a confidence interval was formed from observational data
on the quality of treatment from human oncologists to estimate the proportion
of scenarios in which the model must outperform the original oncologist in its
treatment prediction to be a better solution overall as 8.2% to 13.3%. Due to
indeterminacy in the outcomes of cancer treatment decisions, future
investigation, potentially a clinical trial, would be required to determine if
this threshold was met by the models. Nevertheless, with 85% of U.S. cancer
patients receiving treatment at local community facilities, these kinds of
models could play an important part in expanding access to quality care with
outcomes that lie, at minimum, close to a human oncologist.
},
    year = {2024},
    month = {06},
    url = {http://arxiv.org/pdf/2406.06455v2},
    journal = {arXiv preprint}
}

@article{arxiv:MedCT:_A_Clinical_Terminology_Graph_for_Generative_AI_Applications_in
__Healthcare,
    title = {MedCT: A Clinical Terminology Graph for Generative AI Applications in
  Healthcare},
    author = {Ye Chen, Dongdong Huang, Haoyun Xu, Cong Fu, Lin Sheng, Qingli Zhou, Yuqiang Shen, Kai Wang},
    abstract = {  We introduce the world's first clinical terminology for the Chinese
healthcare community, namely MedCT, accompanied by a clinical foundation model
MedBERT and an entity linking model MedLink. The MedCT system enables
standardized and programmable representation of Chinese clinical data,
successively stimulating the development of new medicines, treatment pathways,
and better patient outcomes for the populous Chinese community. Moreover, the
MedCT knowledge graph provides a principled mechanism to minimize the
hallucination problem of large language models (LLMs), therefore achieving
significant levels of accuracy and safety in LLM-based clinical applications.
By leveraging the LLMs' emergent capabilities of generativeness and
expressiveness, we were able to rapidly built a production-quality terminology
system and deployed to real-world clinical field within three months, while
classical terminologies like SNOMED CT have gone through more than twenty years
development. Our experiments show that the MedCT system achieves
state-of-the-art (SOTA) performance in semantic matching and entity linking
tasks, not only for Chinese but also for English. We also conducted a
longitudinal field experiment by applying MedCT and LLMs in a representative
spectrum of clinical tasks, including electronic health record (EHR)
auto-generation and medical document search for diagnostic decision making. Our
study shows a multitude of values of MedCT for clinical workflows and patient
outcomes, especially in the new genre of clinical LLM applications. We present
our approach in sufficient engineering detail, such that implementing a
clinical terminology for other non-English societies should be readily
reproducible. We openly release our terminology, models and algorithms, along
with real-world clinical datasets for the development.
},
    year = {2025},
    month = {01},
    url = {http://arxiv.org/pdf/2501.06465v2},
    journal = {arXiv preprint}
}

@article{arxiv:Utilizing_ChatGPT_to_Enhance_Clinical_Trial_Enrollment,
    title = {Utilizing ChatGPT to Enhance Clinical Trial Enrollment},
    author = {Georgios Peikos, Symeon Symeonidis, Pranav Kasela, Gabriella Pasi},
    abstract = {  Clinical trials are a critical component of evaluating the effectiveness of
new medical interventions and driving advancements in medical research.
Therefore, timely enrollment of patients is crucial to prevent delays or
premature termination of trials. In this context, Electronic Health Records
(EHRs) have emerged as a valuable tool for identifying and enrolling eligible
participants. In this study, we propose an automated approach that leverages
ChatGPT, a large language model, to extract patient-related information from
unstructured clinical notes and generate search queries for retrieving
potentially eligible clinical trials. Our empirical evaluation, conducted on
two benchmark retrieval collections, shows improved retrieval performance
compared to existing approaches when several general-purposed and task-specific
prompts are used. Notably, ChatGPT-generated queries also outperform
human-generated queries in terms of retrieval performance. These findings
highlight the potential use of ChatGPT to enhance clinical trial enrollment
while ensuring the quality of medical service and minimizing direct risks to
patients.
},
    year = {2023},
    month = {06},
    url = {http://arxiv.org/pdf/2306.02077v1},
    journal = {arXiv preprint}
}

@article{arxiv:RuCCoD:_Towards_Automated_ICD_Coding_in_Russian,
    title = {RuCCoD: Towards Automated ICD Coding in Russian},
    author = {Aleksandr Nesterov, Andrey Sakhovskiy, Ivan Sviridov, Airat Valiev, Vladimir Makharev, Petr Anokhin, Galina Zubkova, Elena Tutubalina},
    abstract = {  This study investigates the feasibility of automating clinical coding in
Russian, a language with limited biomedical resources. We present a new dataset
for ICD coding, which includes diagnosis fields from electronic health records
(EHRs) annotated with over 10,000 entities and more than 1,500 unique ICD
codes. This dataset serves as a benchmark for several state-of-the-art models,
including BERT, LLaMA with LoRA, and RAG, with additional experiments examining
transfer learning across domains (from PubMed abstracts to medical diagnosis)
and terminologies (from UMLS concepts to ICD codes). We then apply the
best-performing model to label an in-house EHR dataset containing patient
histories from 2017 to 2021. Our experiments, conducted on a carefully curated
test set, demonstrate that training with the automated predicted codes leads to
a significant improvement in accuracy compared to manually annotated data from
physicians. We believe our findings offer valuable insights into the potential
for automating clinical coding in resource-limited languages like Russian,
which could enhance clinical efficiency and data accuracy in these contexts.
},
    year = {2025},
    month = {02},
    url = {http://arxiv.org/pdf/2502.21263v1},
    journal = {arXiv preprint}
}

@article{arxiv:Assessing_the_Usability_of_GutGPT:_A_Simulation_Study_of_an_AI_Clinical
__Decision_Support_System_for_Gastrointestinal_Bleeding_Risk,
    title = {Assessing the Usability of GutGPT: A Simulation Study of an AI Clinical
  Decision Support System for Gastrointestinal Bleeding Risk},
    author = {Colleen Chan, Kisung You, Sunny Chung, Mauro Giuffr√®, Theo Saarinen, Niroop Rajashekar, Yuan Pu, Yeo Eun Shin, Loren Laine, Ambrose Wong, Ren√© Kizilcec, Jasjeet Sekhon, Dennis Shung},
    abstract = {  Applications of large language models (LLMs) like ChatGPT have potential to
enhance clinical decision support through conversational interfaces. However,
challenges of human-algorithmic interaction and clinician trust are poorly
understood. GutGPT, a LLM for gastrointestinal (GI) bleeding risk prediction
and management guidance, was deployed in clinical simulation scenarios
alongside the electronic health record (EHR) with emergency medicine
physicians, internal medicine physicians, and medical students to evaluate its
effect on physician acceptance and trust in AI clinical decision support
systems (AI-CDSS). GutGPT provides risk predictions from a validated machine
learning model and evidence-based answers by querying extracted clinical
guidelines. Participants were randomized to GutGPT and an interactive
dashboard, or the interactive dashboard and a search engine. Surveys and
educational assessments taken before and after measured technology acceptance
and content mastery. Preliminary results showed mixed effects on acceptance
after using GutGPT compared to the dashboard or search engine but appeared to
improve content mastery based on simulation performance. Overall, this study
demonstrates LLMs like GutGPT could enhance effective AI-CDSS if implemented
optimally and paired with interactive interfaces.
},
    year = {2023},
    month = {12},
    url = {http://arxiv.org/pdf/2312.10072v1},
    journal = {arXiv preprint}
}

@article{arxiv:Large_Language_Models_for_Medical_OSCE_Assessment:_A_Novel_Approach_to
__Transcript_Analysis,
    title = {Large Language Models for Medical OSCE Assessment: A Novel Approach to
  Transcript Analysis},
    author = {Ameer Hamza Shakur, Michael J. Holcomb, David Hein, Shinyoung Kang, Thomas O. Dalton, Krystle K. Campbell, Daniel J. Scott, Andrew R. Jamieson},
    abstract = {  Grading Objective Structured Clinical Examinations (OSCEs) is a
time-consuming and expensive process, traditionally requiring extensive manual
effort from human experts. In this study, we explore the potential of Large
Language Models (LLMs) to assess skills related to medical student
communication. We analyzed 2,027 video-recorded OSCE examinations from the
University of Texas Southwestern Medical Center (UTSW), spanning four years
(2019-2022), and several different medical cases or "stations." Specifically,
our focus was on evaluating students' ability to summarize patients' medical
history: we targeted the rubric item 'did the student summarize the patients'
medical history?' from the communication skills rubric. After transcribing
speech audio captured by OSCE videos using Whisper-v3, we studied the
performance of various LLM-based approaches for grading students on this
summarization task based on their examination transcripts. Using various
frontier-level open-source and proprietary LLMs, we evaluated different
techniques such as zero-shot chain-of-thought prompting, retrieval augmented
generation, and multi-model ensemble methods. Our results show that frontier
LLM models like GPT-4 achieved remarkable alignment with human graders,
demonstrating a Cohen's kappa agreement of 0.88 and indicating strong potential
for LLM-based OSCE grading to augment the current grading process. Open-source
models also showed promising results, suggesting potential for widespread,
cost-effective deployment. Further, we present a failure analysis identifying
conditions where LLM grading may be less reliable in this context and recommend
best practices for deploying LLMs in medical education settings.
},
    year = {2024},
    month = {10},
    url = {http://arxiv.org/pdf/2410.12858v1},
    journal = {arXiv preprint}
}

@article{arxiv:The_Potential_and_Pitfalls_of_using_a_Large_Language_Model_such_as
__ChatGPT_or_GPT-4_as_a_Clinical_Assistant,
    title = {The Potential and Pitfalls of using a Large Language Model such as
  ChatGPT or GPT-4 as a Clinical Assistant},
    author = {Jingqing Zhang, Kai Sun, Akshay Jagadeesh, Mahta Ghahfarokhi, Deepa Gupta, Ashok Gupta, Vibhor Gupta, Yike Guo},
    abstract = {  Recent studies have demonstrated promising performance of ChatGPT and GPT-4
on several medical domain tasks. However, none have assessed its performance
using a large-scale real-world electronic health record database, nor have
evaluated its utility in providing clinical diagnostic assistance for patients
across a full range of disease presentation. We performed two analyses using
ChatGPT and GPT-4, one to identify patients with specific medical diagnoses
using a real-world large electronic health record database and the other, in
providing diagnostic assistance to healthcare workers in the prospective
evaluation of hypothetical patients. Our results show that GPT-4 across disease
classification tasks with chain of thought and few-shot prompting can achieve
performance as high as 96% F1 scores. For patient assessment, GPT-4 can
accurately diagnose three out of four times. However, there were mentions of
factually incorrect statements, overlooking crucial medical findings,
recommendations for unnecessary investigations and overtreatment. These issues
coupled with privacy concerns, make these models currently inadequate for real
world clinical use. However, limited data and time needed for prompt
engineering in comparison to configuration of conventional machine learning
workflows highlight their potential for scalability across healthcare
applications.
},
    year = {2023},
    month = {07},
    url = {http://arxiv.org/pdf/2307.08152v1},
    journal = {arXiv preprint}
}

@article{arxiv:MedPromptX:_Grounded_Multimodal_Prompting_for_Chest_X-ray_Diagnosis,
    title = {MedPromptX: Grounded Multimodal Prompting for Chest X-ray Diagnosis},
    author = {Mai A. Shaaban, Adnan Khan, Mohammad Yaqub},
    abstract = {  Chest X-ray images are commonly used for predicting acute and chronic
cardiopulmonary conditions, but efforts to integrate them with structured
clinical data face challenges due to incomplete electronic health records
(EHR). This paper introduces MedPromptX, the first clinical decision support
system that integrates multimodal large language models (MLLMs), few-shot
prompting (FP) and visual grounding (VG) to combine imagery with EHR data for
chest X-ray diagnosis. A pre-trained MLLM is utilized to complement the missing
EHR information, providing a comprehensive understanding of patients' medical
history. Additionally, FP reduces the necessity for extensive training of MLLMs
while effectively tackling the issue of hallucination. Nevertheless, the
process of determining the optimal number of few-shot examples and selecting
high-quality candidates can be burdensome, yet it profoundly influences model
performance. Hence, we propose a new technique that dynamically refines
few-shot data for real-time adjustment to new patient scenarios. Moreover, VG
narrows the search area in X-ray images, thereby enhancing the identification
of abnormalities. We also release MedPromptX-VQA, a new in-context visual
question answering dataset encompassing interleaved images and EHR data derived
from MIMIC-IV and MIMIC-CXR-JPG databases. Results demonstrate the SOTA
performance of MedPromptX, achieving an 11% improvement in F1-score compared to
the baselines. Code and data are publicly available on
https://github.com/BioMedIA-MBZUAI/MedPromptX.
},
    year = {2024},
    month = {03},
    url = {http://arxiv.org/pdf/2403.15585v4},
    journal = {arXiv preprint}
}

@article{arxiv:Clinical_Decision_Transformer:_Intended_Treatment_Recommendation_through
__Goal_Prompting,
    title = {Clinical Decision Transformer: Intended Treatment Recommendation through
  Goal Prompting},
    author = {Seunghyun Lee, Da Young Lee, Sujeong Im, Nan Hee Kim, Sung-Min Park},
    abstract = {  With recent achievements in tasks requiring context awareness, foundation
models have been adopted to treat large-scale data from electronic health
record (EHR) systems. However, previous clinical recommender systems based on
foundation models have a limited purpose of imitating clinicians' behavior and
do not directly consider a problem of missing values. In this paper, we propose
Clinical Decision Transformer (CDT), a recommender system that generates a
sequence of medications to reach a desired range of clinical states given as
goal prompts. For this, we conducted goal-conditioned sequencing, which
generated a subsequence of treatment history with prepended future goal state,
and trained the CDT to model sequential medications required to reach that goal
state. For contextual embedding over intra-admission and inter-admissions, we
adopted a GPT-based architecture with an admission-wise attention mask and
column embedding. In an experiment, we extracted a diabetes dataset from an EHR
system, which contained treatment histories of 4788 patients. We observed that
the CDT achieved the intended treatment effect according to goal prompt ranges
(e.g., NormalA1c, LowerA1c, and HigherA1c), contrary to the case with behavior
cloning. To the best of our knowledge, this is the first study to explore
clinical recommendations from the perspective of goal prompting. See
https://clinical-decision-transformer.github.io for code and additional
information.
},
    year = {2023},
    month = {02},
    url = {http://arxiv.org/pdf/2302.00612v1},
    journal = {arXiv preprint}
}

@article{arxiv:Evaluation_of_General_Large_Language_Models_in_Contextually_Assessing
__Semantic_Concepts_Extracted_from_Adult_Critical_Care_Electronic_Health_Record
__Notes,
    title = {Evaluation of General Large Language Models in Contextually Assessing
  Semantic Concepts Extracted from Adult Critical Care Electronic Health Record
  Notes},
    author = {Darren Liu, Cheng Ding, Delgersuren Bold, Monique Bouvier, Jiaying Lu, Benjamin Shickel, Craig S. Jabaley, Wenhui Zhang, Soojin Park, Michael J. Young, Mark S. Wainwright, Gilles Clermont, Parisa Rashidi, Eric S. Rosenthal, Laurie Dimisko, Ran Xiao, Joo Heung Yoon, Carl Yang, Xiao Hu},
    abstract = {  The field of healthcare has increasingly turned its focus towards Large
Language Models (LLMs) due to their remarkable performance. However, their
performance in actual clinical applications has been underexplored. Traditional
evaluations based on question-answering tasks don't fully capture the nuanced
contexts. This gap highlights the need for more in-depth and practical
assessments of LLMs in real-world healthcare settings. Objective: We sought to
evaluate the performance of LLMs in the complex clinical context of adult
critical care medicine using systematic and comprehensible analytic methods,
including clinician annotation and adjudication. Methods: We investigated the
performance of three general LLMs in understanding and processing real-world
clinical notes. Concepts from 150 clinical notes were identified by MetaMap and
then labeled by 9 clinicians. Each LLM's proficiency was evaluated by
identifying the temporality and negation of these concepts using different
prompts for an in-depth analysis. Results: GPT-4 showed overall superior
performance compared to other LLMs. In contrast, both GPT-3.5 and
text-davinci-003 exhibit enhanced performance when the appropriate prompting
strategies are employed. The GPT family models have demonstrated considerable
efficiency, evidenced by their cost-effectiveness and time-saving capabilities.
Conclusion: A comprehensive qualitative performance evaluation framework for
LLMs is developed and operationalized. This framework goes beyond singular
performance aspects. With expert annotations, this methodology not only
validates LLMs' capabilities in processing complex medical data but also
establishes a benchmark for future LLM evaluations across specialized domains.
},
    year = {2024},
    month = {01},
    url = {http://arxiv.org/pdf/2401.13588v1},
    journal = {arXiv preprint}
}

@article{arxiv:Language_Models_and_Retrieval_Augmented_Generation_for_Automated
__Structured_Data_Extraction_from_Diagnostic_Reports,
    title = {Language Models and Retrieval Augmented Generation for Automated
  Structured Data Extraction from Diagnostic Reports},
    author = {Mohamed Sobhi Jabal, Pranav Warman, Jikai Zhang, Kartikeye Gupta, Ayush Jain, Maciej Mazurowski, Walter Wiggins, Kirti Magudia, Evan Calabrese},
    abstract = {  Purpose: To develop and evaluate an automated system for extracting
structured clinical information from unstructured radiology and pathology
reports using open-weights large language models (LMs) and retrieval augmented
generation (RAG), and to assess the effects of model configuration variables on
extraction performance. Methods and Materials: The study utilized two datasets:
7,294 radiology reports annotated for Brain Tumor Reporting and Data System
(BT-RADS) scores and 2,154 pathology reports annotated for isocitrate
dehydrogenase (IDH) mutation status. An automated pipeline was developed to
benchmark the performance of various LMs and RAG configurations. The impact of
model size, quantization, prompting strategies, output formatting, and
inference parameters was systematically evaluated. Results: The best performing
models achieved over 98% accuracy in extracting BT-RADS scores from radiology
reports and over 90% for IDH mutation status extraction from pathology reports.
The top model being medical fine-tuned llama3. Larger, newer, and domain
fine-tuned models consistently outperformed older and smaller models. Model
quantization had minimal impact on performance. Few-shot prompting
significantly improved accuracy. RAG improved performance for complex pathology
reports but not for shorter radiology reports. Conclusions: Open LMs
demonstrate significant potential for automated extraction of structured
clinical data from unstructured clinical reports with local privacy-preserving
application. Careful model selection, prompt engineering, and semi-automated
optimization using annotated data are critical for optimal performance. These
approaches could be reliable enough for practical use in research workflows,
highlighting the potential for human-machine collaboration in healthcare data
extraction.
},
    year = {2024},
    month = {09},
    url = {http://arxiv.org/pdf/2409.10576v2},
    journal = {arXiv preprint}
}

@article{arxiv:RAMIE:_Retrieval-Augmented_Multi-task_Information_Extraction_with_Large
__Language_Models_on_Dietary_Supplements,
    title = {RAMIE: Retrieval-Augmented Multi-task Information Extraction with Large
  Language Models on Dietary Supplements},
    author = {Zaifu Zhan, Shuang Zhou, Mingchen Li, Rui Zhang},
    abstract = {  \textbf{Objective:} We aimed to develop an advanced multi-task large language
model (LLM) framework to extract multiple types of information about dietary
supplements (DS) from clinical records.
  \textbf{Methods:} We used four core DS information extraction tasks - namely,
named entity recognition (NER: 2,949 clinical sentences), relation extraction
(RE: 4,892 sentences), triple extraction (TE: 2,949 sentences), and usage
classification (UC: 2,460 sentences) as our multitasks. We introduced a novel
Retrieval-Augmented Multi-task Information Extraction (RAMIE) Framework,
including: 1) employed instruction fine-tuning techniques with task-specific
prompts, 2) trained LLMs for multiple tasks with improved storage efficiency
and lower training costs, and 3) incorporated retrieval augmentation generation
(RAG) techniques by retrieving similar examples from the training set. We
compared RAMIE's performance to LLMs with instruction fine-tuning alone and
conducted an ablation study to assess the contributions of multi-task learning
and RAG to improved multitasking performance.
  \textbf{Results:} With the aid of the RAMIE framework, Llama2-13B achieved an
F1 score of 87.39 (3.51\% improvement) on the NER task and demonstrated
outstanding performance on the RE task with an F1 score of 93.74 (1.15\%
improvement). For the TE task, Llama2-7B scored 79.45 (14.26\% improvement),
and MedAlpaca-7B achieved the highest F1 score of 93.45 (0.94\% improvement) on
the UC task. The ablation study revealed that while MTL increased efficiency
with a slight trade-off in performance, RAG significantly boosted overall
accuracy.
  \textbf{Conclusion:} This study presents a novel RAMIE framework that
demonstrates substantial improvements in multi-task information extraction for
DS-related data from clinical records. Our framework can potentially be applied
to other domains.
},
    year = {2024},
    month = {11},
    url = {http://arxiv.org/pdf/2411.15700v1},
    journal = {arXiv preprint}
}

@article{arxiv:An_Empirical_Evaluation_of_Prompting_Strategies_for_Large_Language
__Models_in_Zero-Shot_Clinical_Natural_Language_Processing,
    title = {An Empirical Evaluation of Prompting Strategies for Large Language
  Models in Zero-Shot Clinical Natural Language Processing},
    author = {Sonish Sivarajkumar, Mark Kelley, Alyssa Samolyk-Mazzanti, Shyam Visweswaran, Yanshan Wang},
    abstract = {  Large language models (LLMs) have shown remarkable capabilities in Natural
Language Processing (NLP), especially in domains where labeled data is scarce
or expensive, such as clinical domain. However, to unlock the clinical
knowledge hidden in these LLMs, we need to design effective prompts that can
guide them to perform specific clinical NLP tasks without any task-specific
training data. This is known as in-context learning, which is an art and
science that requires understanding the strengths and weaknesses of different
LLMs and prompt engineering approaches. In this paper, we present a
comprehensive and systematic experimental study on prompt engineering for five
clinical NLP tasks: Clinical Sense Disambiguation, Biomedical Evidence
Extraction, Coreference Resolution, Medication Status Extraction, and
Medication Attribute Extraction. We assessed the prompts proposed in recent
literature, including simple prefix, simple cloze, chain of thought, and
anticipatory prompts, and introduced two new types of prompts, namely heuristic
prompting and ensemble prompting. We evaluated the performance of these prompts
on three state-of-the-art LLMs: GPT-3.5, BARD, and LLAMA2. We also contrasted
zero-shot prompting with few-shot prompting, and provide novel insights and
guidelines for prompt engineering for LLMs in clinical NLP. To the best of our
knowledge, this is one of the first works on the empirical evaluation of
different prompt engineering approaches for clinical NLP in this era of
generative AI, and we hope that it will inspire and inform future research in
this area.
},
    year = {2023},
    month = {09},
    url = {http://arxiv.org/pdf/2309.08008v1},
    journal = {arXiv preprint}
}

@article{arxiv:Multi-OphthaLingua:_A_Multilingual_Benchmark_for_Assessing_and_Debiasing
__LLM_Ophthalmological_QA_in_LMICs,
    title = {Multi-OphthaLingua: A Multilingual Benchmark for Assessing and Debiasing
  LLM Ophthalmological QA in LMICs},
    author = {David Restrepo, Chenwei Wu, Zhengxu Tang, Zitao Shuai, Thao Nguyen Minh Phan, Jun-En Ding, Cong-Tinh Dao, Jack Gallifant, Robyn Gayle Dychiao, Jose Carlo Artiaga, Andr√© Hiroshi Bando, Carolina Pelegrini Barbosa Gracitelli, Vincenz Ferrer, Leo Anthony Celi, Danielle Bitterman, Michael G Morley, Luis Filipe Nakayama},
    abstract = {  Current ophthalmology clinical workflows are plagued by over-referrals, long
waits, and complex and heterogeneous medical records. Large language models
(LLMs) present a promising solution to automate various procedures such as
triaging, preliminary tests like visual acuity assessment, and report
summaries. However, LLMs have demonstrated significantly varied performance
across different languages in natural language question-answering tasks,
potentially exacerbating healthcare disparities in Low and Middle-Income
Countries (LMICs). This study introduces the first multilingual
ophthalmological question-answering benchmark with manually curated questions
parallel across languages, allowing for direct cross-lingual comparisons. Our
evaluation of 6 popular LLMs across 7 different languages reveals substantial
bias across different languages, highlighting risks for clinical deployment of
LLMs in LMICs. Existing debiasing methods such as Translation Chain-of-Thought
or Retrieval-augmented generation (RAG) by themselves fall short of closing
this performance gap, often failing to improve performance across all languages
and lacking specificity for the medical domain. To address this issue, We
propose CLARA (Cross-Lingual Reflective Agentic system), a novel inference time
de-biasing method leveraging retrieval augmented generation and
self-verification. Our approach not only improves performance across all
languages but also significantly reduces the multilingual bias gap,
facilitating equitable LLM application across the globe.
},
    year = {2024},
    month = {12},
    url = {http://arxiv.org/pdf/2412.14304v1},
    journal = {arXiv preprint}
}

@article{arxiv:Reasoning-Enhanced_Healthcare_Predictions_with_Knowledge_Graph_Community
__Retrieval,
    title = {Reasoning-Enhanced Healthcare Predictions with Knowledge Graph Community
  Retrieval},
    author = {Pengcheng Jiang, Cao Xiao, Minhao Jiang, Parminder Bhatia, Taha Kass-Hout, Jimeng Sun, Jiawei Han},
    abstract = {  Large language models (LLMs) have demonstrated significant potential in
clinical decision support. Yet LLMs still suffer from hallucinations and lack
fine-grained contextual medical knowledge, limiting their high-stake healthcare
applications such as clinical diagnosis. Traditional retrieval-augmented
generation (RAG) methods attempt to address these limitations but frequently
retrieve sparse or irrelevant information, undermining prediction accuracy. We
introduce KARE, a novel framework that integrates knowledge graph (KG)
community-level retrieval with LLM reasoning to enhance healthcare predictions.
KARE constructs a comprehensive multi-source KG by integrating biomedical
databases, clinical literature, and LLM-generated insights, and organizes it
using hierarchical graph community detection and summarization for precise and
contextually relevant information retrieval. Our key innovations include: (1) a
dense medical knowledge structuring approach enabling accurate retrieval of
relevant information; (2) a dynamic knowledge retrieval mechanism that enriches
patient contexts with focused, multi-faceted medical insights; and (3) a
reasoning-enhanced prediction framework that leverages these enriched contexts
to produce both accurate and interpretable clinical predictions. Extensive
experiments demonstrate that KARE outperforms leading models by up to
10.8-15.0% on MIMIC-III and 12.6-12.7% on MIMIC-IV for mortality and
readmission predictions. In addition to its impressive prediction accuracy, our
framework leverages the reasoning capabilities of LLMs, enhancing the
trustworthiness of clinical predictions.
},
    year = {2024},
    month = {10},
    url = {http://arxiv.org/pdf/2410.04585v1},
    journal = {arXiv preprint}
}

@article{arxiv:Iterative_Prompt_Refinement_for_Radiation_Oncology_Symptom_Extraction
__Using_Teacher-Student_Large_Language_Models,
    title = {Iterative Prompt Refinement for Radiation Oncology Symptom Extraction
  Using Teacher-Student Large Language Models},
    author = {Reza Khanmohammadi, Ahmed I Ghanem, Kyle Verdecchia, Ryan Hall, Mohamed Elshaikh, Benjamin Movsas, Hassan Bagher-Ebadian, Indrin Chetty, Mohammad M. Ghassemi, Kundan Thind},
    abstract = {  This study introduces a novel teacher-student architecture utilizing Large
Language Models (LLMs) to improve prostate cancer radiotherapy symptom
extraction from clinical notes. Mixtral, the student model, initially extracts
symptoms, followed by GPT-4, the teacher model, which refines prompts based on
Mixtral's performance. This iterative process involved 294 single symptom
clinical notes across 12 symptoms, with up to 16 rounds of refinement per
epoch. Results showed significant improvements in extracting symptoms from both
single and multi-symptom notes. For 59 single symptom notes, accuracy increased
from 0.51 to 0.71, precision from 0.52 to 0.82, recall from 0.52 to 0.72, and
F1 score from 0.49 to 0.73. In 375 multi-symptom notes, accuracy rose from 0.24
to 0.43, precision from 0.6 to 0.76, recall from 0.24 to 0.43, and F1 score
from 0.20 to 0.44. These results demonstrate the effectiveness of advanced
prompt engineering in LLMs for radiation oncology use.
},
    year = {2024},
    month = {02},
    url = {http://arxiv.org/pdf/2402.04075v1},
    journal = {arXiv preprint}
}

@article{arxiv:Leveraging_large_language_models_for_structured_information_extraction
__from_pathology_reports,
    title = {Leveraging large language models for structured information extraction
  from pathology reports},
    author = {Jeya Balaji Balasubramanian, Daniel Adams, Ioannis Roxanis, Amy Berrington de Gonzalez, Penny Coulson, Jonas S. Almeida, Montserrat Garc√≠a-Closas},
    abstract = {  Background: Structured information extraction from unstructured
histopathology reports facilitates data accessibility for clinical research.
Manual extraction by experts is time-consuming and expensive, limiting
scalability. Large language models (LLMs) offer efficient automated extraction
through zero-shot prompting, requiring only natural language instructions
without labeled data or training. We evaluate LLMs' accuracy in extracting
structured information from breast cancer histopathology reports, compared to
manual extraction by a trained human annotator.
  Methods: We developed the Medical Report Information Extractor, a web
application leveraging LLMs for automated extraction. We developed a gold
standard extraction dataset to evaluate the human annotator alongside five LLMs
including GPT-4o, a leading proprietary model, and the Llama 3 model family,
which allows self-hosting for data privacy. Our assessment involved 111
histopathology reports from the Breast Cancer Now (BCN) Generations Study,
extracting 51 pathology features specified in the study's data dictionary.
  Results: Evaluation against the gold standard dataset showed that both Llama
3.1 405B (94.7% accuracy) and GPT-4o (96.1%) achieved extraction accuracy
comparable to the human annotator (95.4%; p = 0.146 and p = 0.106,
respectively). While Llama 3.1 70B (91.6%) performed below human accuracy (p
<0.001), its reduced computational requirements make it a viable option for
self-hosting.
  Conclusion: We developed an open-source tool for structured information
extraction that can be customized by non-programmers using natural language.
Its modular design enables reuse for various extraction tasks, producing
standardized, structured data from unstructured text reports to facilitate
analytics through improved accessibility and interoperability.
},
    year = {2025},
    month = {02},
    url = {http://arxiv.org/pdf/2502.12183v1},
    journal = {arXiv preprint}
}

@article{arxiv:Prompt_engineering_paradigms_for_medical_applications:_scoping_review
__and_recommendations_for_better_practices,
    title = {Prompt engineering paradigms for medical applications: scoping review
  and recommendations for better practices},
    author = {Jamil Zaghir, Marco Naguib, Mina Bjelogrlic, Aur√©lie N√©v√©ol, Xavier Tannier, Christian Lovis},
    abstract = {  Prompt engineering is crucial for harnessing the potential of large language
models (LLMs), especially in the medical domain where specialized terminology
and phrasing is used. However, the efficacy of prompt engineering in the
medical domain remains to be explored. In this work, 114 recent studies
(2022-2024) applying prompt engineering in medicine, covering prompt learning
(PL), prompt tuning (PT), and prompt design (PD) are reviewed. PD is the most
prevalent (78 articles). In 12 papers, PD, PL, and PT terms were used
interchangeably. ChatGPT is the most commonly used LLM, with seven papers using
it for processing sensitive clinical data. Chain-of-Thought emerges as the most
common prompt engineering technique. While PL and PT articles typically provide
a baseline for evaluating prompt-based approaches, 64% of PD studies lack
non-prompt-related baselines. We provide tables and figures summarizing
existing work, and reporting recommendations to guide future research
contributions.
},
    year = {2024},
    month = {05},
    url = {http://arxiv.org/pdf/2405.01249v1},
    journal = {arXiv preprint}
}

@article{arxiv:Superhuman_performance_in_urology_board_questions_by_an_explainable
__large_language_model_enabled_for_context_integration_of_the_European
__Association_of_Urology_guidelines:_the_UroBot_study,
    title = {Superhuman performance in urology board questions by an explainable
  large language model enabled for context integration of the European
  Association of Urology guidelines: the UroBot study},
    author = {Martin J. Hetz, Nicolas Carl, Sarah Haggenm√ºller, Christoph Wies, Maurice Stephan Michel, Frederik Wessels, Titus J. Brinker},
    abstract = {  Large Language Models (LLMs) are revolutionizing medical Question-Answering
(medQA) through extensive use of medical literature. However, their performance
is often hampered by outdated training data and a lack of explainability, which
limits clinical applicability. This study aimed to create and assess UroBot, a
urology-specialized chatbot, by comparing it with state-of-the-art models and
the performance of urologists on urological board questions, ensuring full
clinician-verifiability. UroBot was developed using OpenAI's GPT-3.5, GPT-4,
and GPT-4o models, employing retrieval-augmented generation (RAG) and the
latest 2023 guidelines from the European Association of Urology (EAU). The
evaluation included ten runs of 200 European Board of Urology (EBU) In-Service
Assessment (ISA) questions, with performance assessed by the mean Rate of
Correct Answers (RoCA). UroBot-4o achieved an average RoCA of 88.4%, surpassing
GPT-4o by 10.8%, with a score of 77.6%. It was also clinician-verifiable and
exhibited the highest run agreement as indicated by Fleiss' Kappa (k = 0.979).
By comparison, the average performance of urologists on board questions, as
reported in the literature, is 68.7%. UroBot's clinician-verifiable nature and
superior accuracy compared to both existing models and urologists on board
questions highlight its potential for clinical integration. The study also
provides the necessary code and instructions for further development of UroBot.
},
    year = {2024},
    month = {06},
    url = {http://arxiv.org/pdf/2406.01428v2},
    journal = {arXiv preprint}
}

@article{arxiv:SearchRAG:_Can_Search_Engines_Be_Helpful_for_LLM-based_Medical_Question
__Answering?,
    title = {SearchRAG: Can Search Engines Be Helpful for LLM-based Medical Question
  Answering?},
    author = {Yucheng Shi, Tianze Yang, Canyu Chen, Quanzheng Li, Tianming Liu, Xiang Li, Ninghao Liu},
    abstract = {  Large Language Models (LLMs) have shown remarkable capabilities in general
domains but often struggle with tasks requiring specialized knowledge.
Conventional Retrieval-Augmented Generation (RAG) techniques typically retrieve
external information from static knowledge bases, which can be outdated or
incomplete, missing fine-grained clinical details essential for accurate
medical question answering. In this work, we propose SearchRAG, a novel
framework that overcomes these limitations by leveraging real-time search
engines. Our method employs synthetic query generation to convert complex
medical questions into search-engine-friendly queries and utilizes
uncertainty-based knowledge selection to filter and incorporate the most
relevant and informative medical knowledge into the LLM's input. Experimental
results demonstrate that our method significantly improves response accuracy in
medical question answering tasks, particularly for complex questions requiring
detailed and up-to-date knowledge.
},
    year = {2025},
    month = {02},
    url = {http://arxiv.org/pdf/2502.13233v1},
    journal = {arXiv preprint}
}

@article{arxiv:Evaluating_Large_Language_Models_on_a_Highly-specialized_Topic,
__Radiation_Oncology_Physics,
    title = {Evaluating Large Language Models on a Highly-specialized Topic,
  Radiation Oncology Physics},
    author = {Jason Holmes, Zhengliang Liu, Lian Zhang, Yuzhen Ding, Terence T. Sio, Lisa A. McGee, Jonathan B. Ashman, Xiang Li, Tianming Liu, Jiajian Shen, Wei Liu},
    abstract = {  We present the first study to investigate Large Language Models (LLMs) in
answering radiation oncology physics questions. Because popular exams like AP
Physics, LSAT, and GRE have large test-taker populations and ample test
preparation resources in circulation, they may not allow for accurately
assessing the true potential of LLMs. This paper proposes evaluating LLMs on a
highly-specialized topic, radiation oncology physics, which may be more
pertinent to scientific and medical communities in addition to being a valuable
benchmark of LLMs. We developed an exam consisting of 100 radiation oncology
physics questions based on our expertise at Mayo Clinic. Four LLMs, ChatGPT
(GPT-3.5), ChatGPT (GPT-4), Bard (LaMDA), and BLOOMZ, were evaluated against
medical physicists and non-experts. ChatGPT (GPT-4) outperformed all other LLMs
as well as medical physicists, on average. The performance of ChatGPT (GPT-4)
was further improved when prompted to explain first, then answer. ChatGPT
(GPT-3.5 and GPT-4) showed a high level of consistency in its answer choices
across a number of trials, whether correct or incorrect, a characteristic that
was not observed in the human test groups. In evaluating ChatGPTs (GPT-4)
deductive reasoning ability using a novel approach (substituting the correct
answer with "None of the above choices is the correct answer."), ChatGPT
(GPT-4) demonstrated surprising accuracy, suggesting the potential presence of
an emergent ability. Finally, although ChatGPT (GPT-4) performed well overall,
its intrinsic properties did not allow for further improvement when scoring
based on a majority vote across trials. In contrast, a team of medical
physicists were able to greatly outperform ChatGPT (GPT-4) using a majority
vote. This study suggests a great potential for LLMs to work alongside
radiation oncology experts as highly knowledgeable assistants.
},
    year = {2023},
    month = {04},
    url = {http://arxiv.org/pdf/2304.01938v1},
    journal = {arXiv preprint}
}

@article{arxiv:CohortGPT:_An_Enhanced_GPT_for_Participant_Recruitment_in_Clinical_Study,
    title = {CohortGPT: An Enhanced GPT for Participant Recruitment in Clinical Study},
    author = {Zihan Guan, Zihao Wu, Zhengliang Liu, Dufan Wu, Hui Ren, Quanzheng Li, Xiang Li, Ninghao Liu},
    abstract = {  Participant recruitment based on unstructured medical texts such as clinical
notes and radiology reports has been a challenging yet important task for the
cohort establishment in clinical research. Recently, Large Language Models
(LLMs) such as ChatGPT have achieved tremendous success in various downstream
tasks thanks to their promising performance in language understanding,
inference, and generation. It is then natural to test their feasibility in
solving the cohort recruitment task, which involves the classification of a
given paragraph of medical text into disease label(s). However, when applied to
knowledge-intensive problem settings such as medical text classification, where
the LLMs are expected to understand the decision made by human experts and
accurately identify the implied disease labels, the LLMs show a mediocre
performance. A possible explanation is that, by only using the medical text,
the LLMs neglect to use the rich context of additional information that
languages afford. To this end, we propose to use a knowledge graph as auxiliary
information to guide the LLMs in making predictions. Moreover, to further boost
the LLMs adapt to the problem setting, we apply a chain-of-thought (CoT) sample
selection strategy enhanced by reinforcement learning, which selects a set of
CoT samples given each individual medical report. Experimental results and
various ablation studies show that our few-shot learning method achieves
satisfactory performance compared with fine-tuning strategies and gains superb
advantages when the available data is limited. The code and sample dataset of
the proposed CohortGPT model is available at:
https://anonymous.4open.science/r/CohortGPT-4872/
},
    year = {2023},
    month = {07},
    url = {http://arxiv.org/pdf/2307.11346v1},
    journal = {arXiv preprint}
}

@article{arxiv:The_Power_of_Combining_Data_and_Knowledge:_GPT-4o_is_an_Effective
__Interpreter_of_Machine_Learning_Models_in_Predicting_Lymph_Node_Metastasis_of
__Lung_Cancer,
    title = {The Power of Combining Data and Knowledge: GPT-4o is an Effective
  Interpreter of Machine Learning Models in Predicting Lymph Node Metastasis of
  Lung Cancer},
    author = {Danqing Hu, Bing Liu, Xiaofeng Zhu, Nan Wu},
    abstract = {  Lymph node metastasis (LNM) is a crucial factor in determining the initial
treatment for patients with lung cancer, yet accurate preoperative diagnosis of
LNM remains challenging. Recently, large language models (LLMs) have garnered
significant attention due to their remarkable text generation capabilities.
Leveraging the extensive medical knowledge learned from vast corpora, LLMs can
estimate probabilities for clinical problems, though their performance has
historically been inferior to data-driven machine learning models. In this
paper, we propose a novel ensemble method that combines the medical knowledge
acquired by LLMs with the latent patterns identified by machine learning models
to enhance LNM prediction performance. Initially, we developed machine learning
models using patient data. We then designed a prompt template to integrate the
patient data with the predicted probability from the machine learning model.
Subsequently, we instructed GPT-4o, the most advanced LLM developed by OpenAI,
to estimate the likelihood of LNM based on patient data and then adjust the
estimate using the machine learning output. Finally, we collected three outputs
from the GPT-4o using the same prompt and ensembled these results as the final
prediction. Using the proposed method, our models achieved an AUC value of
0.778 and an AP value of 0.426 for LNM prediction, significantly improving
predictive performance compared to baseline machine learning models. The
experimental results indicate that GPT-4o can effectively leverage its medical
knowledge and the probabilities predicted by machine learning models to achieve
more accurate LNM predictions. These findings demonstrate that LLMs can perform
well in clinical risk prediction tasks, offering a new paradigm for integrating
medical knowledge and patient data in clinical predictions.
},
    year = {2024},
    month = {07},
    url = {http://arxiv.org/pdf/2407.17900v5},
    journal = {arXiv preprint}
}

@article{arxiv:LLMs_in_Biomedicine:_A_study_on_clinical_Named_Entity_Recognition,
    title = {LLMs in Biomedicine: A study on clinical Named Entity Recognition},
    author = {Masoud Monajatipoor, Jiaxin Yang, Joel Stremmel, Melika Emami, Fazlolah Mohaghegh, Mozhdeh Rouhsedaghat, Kai-Wei Chang},
    abstract = {  Large Language Models (LLMs) demonstrate remarkable versatility in various
NLP tasks but encounter distinct challenges in biomedical due to the
complexities of language and data scarcity. This paper investigates LLMs
application in the biomedical domain by exploring strategies to enhance their
performance for the NER task. Our study reveals the importance of meticulously
designed prompts in the biomedical. Strategic selection of in-context examples
yields a marked improvement, offering ~15-20\% increase in F1 score across all
benchmark datasets for biomedical few-shot NER. Additionally, our results
indicate that integrating external biomedical knowledge via prompting
strategies can enhance the proficiency of general-purpose LLMs to meet the
specialized needs of biomedical NER. Leveraging a medical knowledge base, our
proposed method, DiRAG, inspired by Retrieval-Augmented Generation (RAG), can
boost the zero-shot F1 score of LLMs for biomedical NER. Code is released at
\url{https://github.com/masoud-monajati/LLM_Bio_NER}
},
    year = {2024},
    month = {04},
    url = {http://arxiv.org/pdf/2404.07376v2},
    journal = {arXiv preprint}
}

@article{arxiv:SM70:_A_Large_Language_Model_for_Medical_Devices,
    title = {SM70: A Large Language Model for Medical Devices},
    author = {Anubhav Bhatti, Surajsinh Parmar, San Lee},
    abstract = {  We are introducing SM70, a 70 billion-parameter Large Language Model that is
specifically designed for SpassMed's medical devices under the brand name
'JEE1' (pronounced as G1 and means 'Life'). This large language model provides
more accurate and safe responses to medical-domain questions. To fine-tune
SM70, we used around 800K data entries from the publicly available dataset
MedAlpaca. The Llama2 70B open-sourced model served as the foundation for SM70,
and we employed the QLoRA technique for fine-tuning. The evaluation is
conducted across three benchmark datasets - MEDQA - USMLE, PUBMEDQA, and USMLE
- each representing a unique aspect of medical knowledge and reasoning. The
performance of SM70 is contrasted with other notable LLMs, including Llama2
70B, Clinical Camel 70 (CC70), GPT 3.5, GPT 4, and Med-Palm, to provide a
comparative understanding of its capabilities within the medical domain. Our
results indicate that SM70 outperforms several established models in these
datasets, showcasing its proficiency in handling a range of medical queries,
from fact-based questions derived from PubMed abstracts to complex clinical
decision-making scenarios. The robust performance of SM70, particularly in the
USMLE and PUBMEDQA datasets, suggests its potential as an effective tool in
clinical decision support and medical information retrieval. Despite its
promising results, the paper also acknowledges the areas where SM70 lags behind
the most advanced model, GPT 4, thereby highlighting the need for further
development, especially in tasks demanding extensive medical knowledge and
intricate reasoning.
},
    year = {2023},
    month = {12},
    url = {http://arxiv.org/pdf/2312.06974v1},
    journal = {arXiv preprint}
}

@article{arxiv:Benchmarking_Generative_AI_for_Scoring_Medical_Student_Interviews_in
__Objective_Structured_Clinical_Examinations_(OSCEs),
    title = {Benchmarking Generative AI for Scoring Medical Student Interviews in
  Objective Structured Clinical Examinations (OSCEs)},
    author = {Jadon Geathers, Yann Hicke, Colleen Chan, Niroop Rajashekar, Justin Sewell, Susannah Cornes, Rene Kizilcec, Dennis Shung},
    abstract = {  Introduction. Objective Structured Clinical Examinations (OSCEs) are widely
used to assess medical students' communication skills, but scoring
interview-based assessments is time-consuming and potentially subject to human
bias. This study explored the potential of large language models (LLMs) to
automate OSCE evaluations using the Master Interview Rating Scale (MIRS).
  Methods. We compared the performance of four state-of-the-art LLMs (GPT-4o,
Claude 3.5, Llama 3.1, and Gemini 1.5 Pro) in evaluating OSCE transcripts
across all 28 items of the MIRS under the conditions of zero-shot,
chain-of-thought (CoT), few-shot, and multi-step prompting. The models were
benchmarked against a dataset of 10 OSCE cases with 174 expert consensus scores
available. Model performance was measured using three accuracy metrics (exact,
off-by-one, thresholded).
  Results. Averaging across all MIRS items and OSCE cases, LLMs performed with
low exact accuracy (0.27 to 0.44), and moderate to high off-by-one accuracy
(0.67 to 0.87) and thresholded accuracy (0.75 to 0.88). A zero temperature
parameter ensured high intra-rater reliability ($\alpha = 0.98$ for GPT-4o).
CoT, few-shot, and multi-step techniques proved valuable when tailored to
specific assessment items. The performance was consistent across MIRS items
independent of encounter phases and communication domains.
  Conclusion. We demonstrated the feasibility of AI-assisted OSCE evaluation
and provided benchmarking of multiple LLMs across multiple prompt techniques.
Our work provides a baseline performance assessment for LLMs that lays a
foundation for future research in automated assessment of clinical
communication skills.
},
    year = {2025},
    month = {01},
    url = {http://arxiv.org/pdf/2501.13957v1},
    journal = {arXiv preprint}
}

@article{arxiv:Evaluating_and_Enhancing_Large_Language_Models_Performance_in
__Domain-specific_Medicine:_Osteoarthritis_Management_with_DocOA,
    title = {Evaluating and Enhancing Large Language Models Performance in
  Domain-specific Medicine: Osteoarthritis Management with DocOA},
    author = {Xi Chen, MingKe You, Li Wang, WeiZhi Liu, Yu Fu, Jie Xu, Shaoting Zhang, Gang Chen, Kang Li, Jian Li},
    abstract = {  The efficacy of large language models (LLMs) in domain-specific medicine,
particularly for managing complex diseases such as osteoarthritis (OA), remains
largely unexplored. This study focused on evaluating and enhancing the clinical
capabilities of LLMs in specific domains, using osteoarthritis (OA) management
as a case study. A domain specific benchmark framework was developed, which
evaluate LLMs across a spectrum from domain-specific knowledge to clinical
applications in real-world clinical scenarios. DocOA, a specialized LLM
tailored for OA management that integrates retrieval-augmented generation (RAG)
and instruction prompts, was developed. The study compared the performance of
GPT-3.5, GPT-4, and a specialized assistant, DocOA, using objective and human
evaluations. Results showed that general LLMs like GPT-3.5 and GPT-4 were less
effective in the specialized domain of OA management, particularly in providing
personalized treatment recommendations. However, DocOA showed significant
improvements. This study introduces a novel benchmark framework which assesses
the domain-specific abilities of LLMs in multiple aspects, highlights the
limitations of generalized LLMs in clinical contexts, and demonstrates the
potential of tailored approaches for developing domain-specific medical LLMs.
},
    year = {2024},
    month = {01},
    url = {http://arxiv.org/pdf/2401.12998v1},
    journal = {arXiv preprint}
}

@article{arxiv:Fine-Tuning_a_Local_LLaMA-3_Large_Language_Model_for_Automated
__Privacy-Preserving_Physician_Letter_Generation_in_Radiation_Oncology,
    title = {Fine-Tuning a Local LLaMA-3 Large Language Model for Automated
  Privacy-Preserving Physician Letter Generation in Radiation Oncology},
    author = {Yihao Hou, Christoph Bert, Ahmed Gomaa, Godehard Lahmer, Daniel Hoefler, Thomas Weissmann, Raphaela Voigt, Philipp Schubert, Charlotte Schmitter, Alina Depardon, Sabine Semrau, Andreas Maier, Rainer Fietkau, Yixing Huang, Florian Putz},
    abstract = {  Generating physician letters is a time-consuming task in daily clinical
practice. This study investigates local fine-tuning of large language models
(LLMs), specifically LLaMA models, for physician letter generation in a
privacy-preserving manner within the field of radiation oncology. Our findings
demonstrate that base LLaMA models, without fine-tuning, are inadequate for
effectively generating physician letters. The QLoRA algorithm provides an
efficient method for local intra-institutional fine-tuning of LLMs with limited
computational resources (i.e., a single 48 GB GPU workstation within the
hospital). The fine-tuned LLM successfully learns radiation oncology-specific
information and generates physician letters in an institution-specific style.
ROUGE scores of the generated summary reports highlight the superiority of the
8B LLaMA-3 model over the 13B LLaMA-2 model. Further multidimensional physician
evaluations of 10 cases reveal that, although the fine-tuned LLaMA-3 model has
limited capacity to generate content beyond the provided input data, it
successfully generates salutations, diagnoses and treatment histories,
recommendations for further treatment, and planned schedules. Overall, clinical
benefit was rated highly by the clinical experts (average score of 3.44 on a
4-point scale). With careful physician review and correction, automated
LLM-based physician letter generation has significant practical value.
},
    year = {2024},
    month = {08},
    url = {http://arxiv.org/pdf/2408.10715v1},
    journal = {arXiv preprint}
}

@article{arxiv:DKEC:_Domain_Knowledge_Enhanced_Multi-Label_Classification_for_Diagnosis
__Prediction,
    title = {DKEC: Domain Knowledge Enhanced Multi-Label Classification for Diagnosis
  Prediction},
    author = {Xueren Ge, Satpathy Abhishek, Ronald Dean Williams, John A. Stankovic, Homa Alemzadeh},
    abstract = {  Multi-label text classification (MLTC) tasks in the medical domain often face
the long-tail label distribution problem. Prior works have explored
hierarchical label structures to find relevant information for few-shot
classes, but mostly neglected to incorporate external knowledge from medical
guidelines. This paper presents DKEC, Domain Knowledge Enhanced Classification
for diagnosis prediction with two innovations: (1) automated construction of
heterogeneous knowledge graphs from external sources to capture semantic
relations among diverse medical entities, (2) incorporating the heterogeneous
knowledge graphs in few-shot classification using a label-wise attention
mechanism. We construct DKEC using three online medical knowledge sources and
evaluate it on a real-world Emergency Medical Services (EMS) dataset and a
public electronic health record (EHR) dataset. Results show that DKEC
outperforms the state-of-the-art label-wise attention networks and transformer
models of different sizes, particularly for the few-shot classes. More
importantly, it helps the smaller language models achieve comparable
performance to large language models.
},
    year = {2023},
    month = {10},
    url = {http://arxiv.org/pdf/2310.07059v2},
    journal = {arXiv preprint}
}

@article{arxiv:Multi-step_Inference_over_Unstructured_Data,
    title = {Multi-step Inference over Unstructured Data},
    author = {Aditya Kalyanpur, Kailash Karthik Saravanakumar, Victor Barres, CJ McFate, Lori Moon, Nati Seifu, Maksim Eremeev, Jose Barrera, Abraham Bautista-Castillo, Eric Brown, David Ferrucci},
    abstract = {  The advent of Large Language Models (LLMs) and Generative AI has
revolutionized natural language applications across various domains. However,
high-stakes decision-making tasks in fields such as medical, legal and finance
require a level of precision, comprehensiveness, and logical consistency that
pure LLM or Retrieval-Augmented-Generation (RAG) approaches often fail to
deliver. At Elemental Cognition (EC), we have developed a neuro-symbolic AI
platform to tackle these problems. The platform integrates fine-tuned LLMs for
knowledge extraction and alignment with a robust symbolic reasoning engine for
logical inference, planning and interactive constraint solving. We describe
Cora, a Collaborative Research Assistant built on this platform, that is
designed to perform complex research and discovery tasks in high-stakes
domains. This paper discusses the multi-step inference challenges inherent in
such domains, critiques the limitations of existing LLM-based methods, and
demonstrates how Cora's neuro-symbolic approach effectively addresses these
issues. We provide an overview of the system architecture, key algorithms for
knowledge extraction and formal reasoning, and present preliminary evaluation
results that highlight Cora's superior performance compared to well-known LLM
and RAG baselines.
},
    year = {2024},
    month = {06},
    url = {http://arxiv.org/pdf/2406.17987v4},
    journal = {arXiv preprint}
}

@article{arxiv:Novel_Development_of_LLM_Driven_mCODE_Data_Model_for_Improved_Clinical
__Trial_Matching_to_Enable_Standardization_and_Interoperability_in_Oncology
__Research,
    title = {Novel Development of LLM Driven mCODE Data Model for Improved Clinical
  Trial Matching to Enable Standardization and Interoperability in Oncology
  Research},
    author = {Aarsh Shekhar, Mincheol Kim},
    abstract = {  Each year, the lack of efficient data standardization and interoperability in
cancer care contributes to the severe lack of timely and effective diagnosis,
while constantly adding to the burden of cost, with cancer costs nationally
reaching over $208 billion in 2023 alone. Traditional methods regarding
clinical trial enrollment and clinical care in oncology are often manual,
time-consuming, and lack a data-driven approach. This paper presents a novel
framework to streamline standardization, interoperability, and exchange of
cancer domains and enhance the integration of oncology-based EHRs across
disparate healthcare systems. This paper utilizes advanced LLMs and Computer
Engineering to streamline cancer clinical trials and discovery. By utilizing
FHIR's resource-based approach and LLM-generated mCODE profiles, we ensure
timely, accurate, and efficient sharing of patient information across disparate
healthcare systems. Our methodology involves transforming unstructured patient
treatment data, PDFs, free-text information, and progress notes into enriched
mCODE profiles, facilitating seamless integration with our novel AI and
ML-based clinical trial matching engine. The results of this study show a
significant improvement in data standardization, with accuracy rates of our
trained LLM peaking at over 92% with datasets consisting of thousands of
patient data. Additionally, our LLM demonstrated an accuracy rate of 87% for
SNOMED-CT, 90% for LOINC, and 84% for RxNorm codes. This trumps the current
status quo, with LLMs such as GPT-4 and Claude's 3.5 peaking at an average of
77%. This paper successfully underscores the potential of our standardization
and interoperability framework, paving the way for more efficient and
personalized cancer treatment.
},
    year = {2024},
    month = {10},
    url = {http://arxiv.org/pdf/2410.19826v1},
    journal = {arXiv preprint}
}

@article{arxiv:MedBioLM:_Optimizing_Medical_and_Biological_QA_with_Fine-Tuned_Large
__Language_Models_and_Retrieval-Augmented_Generation,
    title = {MedBioLM: Optimizing Medical and Biological QA with Fine-Tuned Large
  Language Models and Retrieval-Augmented Generation},
    author = {Seonok Kim},
    abstract = {  Large Language Models (LLMs) have demonstrated impressive capabilities across
natural language processing tasks. However, their application to specialized
domains such as medicine and biology requires further optimization to ensure
factual accuracy, reliability, and contextual depth. We introduce MedBioLM, a
domain-adapted biomedical question-answering model designed to enhance both
short-form and long-form queries. By integrating fine-tuning and
retrieval-augmented generation (RAG), MedBioLM dynamically incorporates
domain-specific knowledge, improving reasoning abilities and factual accuracy.
To evaluate its effectiveness, we fine-tuned the model on diverse biomedical QA
datasets, covering structured multiple-choice assessments and complex clinical
reasoning tasks. Fine-tuning significantly improves accuracy on benchmark
datasets, while RAG enhances factual consistency. These results highlight the
potential of domain-optimized LLMs in advancing biomedical research, medical
education, and clinical decision support.
},
    year = {2025},
    month = {02},
    url = {http://arxiv.org/pdf/2502.03004v1},
    journal = {arXiv preprint}
}

@article{arxiv:Text2MDT:_Extracting_Medical_Decision_Trees_from_Medical_Texts,
    title = {Text2MDT: Extracting Medical Decision Trees from Medical Texts},
    author = {Wei Zhu, Wenfeng Li, Xing Tian, Pengfei Wang, Xiaoling Wang, Jin Chen, Yuanbin Wu, Yuan Ni, Guotong Xie},
    abstract = {  Knowledge of the medical decision process, which can be modeled as medical
decision trees (MDTs), is critical to build clinical decision support systems.
However, the current MDT construction methods rely heavily on time-consuming
and laborious manual annotation. In this work, we propose a novel task,
Text2MDT, to explore the automatic extraction of MDTs from medical texts such
as medical guidelines and textbooks. We normalize the form of the MDT and
create an annotated Text-to-MDT dataset in Chinese with the participation of
medical experts. We investigate two different methods for the Text2MDT tasks:
(a) an end-to-end framework which only relies on a GPT style large language
models (LLM) instruction tuning to generate all the node information and tree
structures. (b) The pipeline framework which decomposes the Text2MDT task to
three subtasks. Experiments on our Text2MDT dataset demonstrate that: (a) the
end-to-end method basd on LLMs (7B parameters or larger) show promising
results, and successfully outperform the pipeline methods. (b) The
chain-of-thought (COT) prompting method \cite{Wei2022ChainOT} can improve the
performance of the fine-tuned LLMs on the Text2MDT test set. (c) the
lightweight pipelined method based on encoder-based pretrained models can
perform comparably with LLMs with model complexity two magnititudes smaller.
Our Text2MDT dataset is open-sourced at
\url{https://tianchi.aliyun.com/dataset/95414}, and the source codes are
open-sourced at \url{https://github.com/michael-wzhu/text2dt}.
},
    year = {2024},
    month = {01},
    url = {http://arxiv.org/pdf/2401.02034v1},
    journal = {arXiv preprint}
}

@article{arxiv:Cancer_Vaccine_Adjuvant_Name_Recognition_from_Biomedical_Literature
__using_Large_Language_Models,
    title = {Cancer Vaccine Adjuvant Name Recognition from Biomedical Literature
  using Large Language Models},
    author = {Hasin Rehana, Jie Zheng, Leo Yeh, Benu Bansal, Nur Bengisu √áam, Christianah Jemiyo, Brett McGregor, Arzucan √ñzg√ºr, Yongqun He, Junguk Hur},
    abstract = {  Motivation: An adjuvant is a chemical incorporated into vaccines that
enhances their efficacy by improving the immune response. Identifying adjuvant
names from cancer vaccine studies is essential for furthering research and
enhancing immunotherapies. However, the manual curation from the constantly
expanding biomedical literature poses significant challenges. This study
explores the automated recognition of vaccine adjuvant names using Large
Language Models (LLMs), specifically Generative Pretrained Transformers (GPT)
and Large Language Model Meta AI (Llama). Methods: We utilized two datasets: 97
clinical trial records from AdjuvareDB and 290 abstracts annotated with the
Vaccine Adjuvant Compendium (VAC). GPT-4o and Llama 3.2 were employed in
zero-shot and few-shot learning paradigms with up to four examples per prompt.
Prompts explicitly targeted adjuvant names, testing the impact of contextual
information such as substances or interventions. Outputs underwent automated
and manual validation for accuracy and consistency. Results: GPT-4o attained
100% Precision across all situations while exhibiting notable improve in Recall
and F1-scores, particularly with incorporating interventions. On the VAC
dataset, GPT-4o achieved a maximum F1-score of 77.32% with interventions,
surpassing Llama-3.2-3B by approximately 2%. On the AdjuvareDB dataset, GPT-4o
reached an F1-score of 81.67% for three-shot prompting with interventions,
surpassing Llama-3.2-3 B's maximum F1-score of 65.62%. Conclusion: Our findings
demonstrate that LLMs excel at identifying adjuvant names, including rare
variations of naming representation. This study emphasizes the capability of
LLMs to enhance cancer vaccine development by efficiently extracting insights.
Future work aims to broaden the framework to encompass various biomedical
literature and enhance model generalizability across various vaccines and
adjuvants.
},
    year = {2025},
    month = {02},
    url = {http://arxiv.org/pdf/2502.09659v1},
    journal = {arXiv preprint}
}

@article{arxiv:Deep_learning-based_NLP_Data_Pipeline_for_EHR_Scanned_Document
__Information_Extraction,
    title = {Deep learning-based NLP Data Pipeline for EHR Scanned Document
  Information Extraction},
    author = {Enshuo Hsu, Ioannis Malagaris, Yong-Fang Kuo, Rizwana Sultana, Kirk Roberts},
    abstract = {  Scanned documents in electronic health records (EHR) have been a challenge
for decades, and are expected to stay in the foreseeable future. Current
approaches for processing often include image preprocessing, optical character
recognition (OCR), and text mining. However, there is limited work that
evaluates the choice of image preprocessing methods, the selection of NLP
models, and the role of document layout. The impact of each element remains
unknown. We evaluated this method on a use case of two key indicators for sleep
apnea, Apnea hypopnea index (AHI) and oxygen saturation (SaO2) values, from
scanned sleep study reports. Our data that included 955 manually annotated
reports was secondarily utilized from a previous study in the University of
Texas Medical Branch. We performed image preprocessing: gray-scaling followed
by 1 iteration of dilating and erode, and 20% contrast increasing. The OCR was
implemented with the Tesseract OCR engine. A total of seven Bag-of-Words models
(Logistic Regression, Ridge Regression, Lasso Regression, Support Vector
Machine, k-Nearest Neighbor, Na\"ive Bayes, and Random Forest) and three deep
learning-based models (BiLSTM, BERT, and Clinical BERT) were evaluated. We also
evaluated the combinations of image preprocessing methods (gray-scaling, dilate
& erode, increased contrast by 20%, increased contrast by 60%), and two deep
learning architectures (with and without structured input that provides
document layout information). Our proposed method using Clinical BERT reached
an AUROC of 0.9743 and document accuracy of 94.76% for AHI, and an AUROC of
0.9523, and document accuracy of 91.61% for SaO2. We demonstrated the proper
use of image preprocessing and document layout could be beneficial to scanned
document processing.
},
    year = {2021},
    month = {09},
    url = {http://arxiv.org/pdf/2110.11864v1},
    journal = {arXiv preprint}
}

@article{arxiv:Evaluating_LLM_Abilities_to_Understand_Tabular_Electronic_Health
__Records:_A_Comprehensive_Study_of_Patient_Data_Extraction_and_Retrieval,
    title = {Evaluating LLM Abilities to Understand Tabular Electronic Health
  Records: A Comprehensive Study of Patient Data Extraction and Retrieval},
    author = {Jesus Lovon, Martin Mouysset, Jo Oleiwan, Jose G. Moreno, Christine Damase-Michel, Lynda Tamine},
    abstract = {  Electronic Health Record (EHR) tables pose unique challenges among which is
the presence of hidden contextual dependencies between medical features with a
high level of data dimensionality and sparsity. This study presents the first
investigation into the abilities of LLMs to comprehend EHRs for patient data
extraction and retrieval. We conduct extensive experiments using the MIMICSQL
dataset to explore the impact of the prompt structure, instruction, context,
and demonstration, of two backbone LLMs, Llama2 and Meditron, based on task
performance. Through quantitative and qualitative analyses, our findings show
that optimal feature selection and serialization methods can enhance task
performance by up to 26.79% compared to naive approaches. Similarly, in-context
learning setups with relevant example selection improve data extraction
performance by 5.95%. Based on our study findings, we propose guidelines that
we believe would help the design of LLM-based models to support health search.
},
    year = {2025},
    month = {01},
    url = {http://arxiv.org/pdf/2501.09384v1},
    journal = {arXiv preprint}
}

@article{arxiv:Natural_Language_Programming_in_Medicine:_Administering_Evidence_Based
__Clinical_Workflows_with_Autonomous_Agents_Powered_by_Generative_Large
__Language_Models,
    title = {Natural Language Programming in Medicine: Administering Evidence Based
  Clinical Workflows with Autonomous Agents Powered by Generative Large
  Language Models},
    author = {Akhil Vaid, Joshua Lampert, Juhee Lee, Ashwin Sawant, Donald Apakama, Ankit Sakhuja, Ali Soroush, Sarah Bick, Ethan Abbott, Hernando Gomez, Michael Hadley, Denise Lee, Isotta Landi, Son Q Duong, Nicole Bussola, Ismail Nabeel, Silke Muehlstedt, Silke Muehlstedt, Robert Freeman, Patricia Kovatch, Brendan Carr, Fei Wang, Benjamin Glicksberg, Edgar Argulian, Stamatios Lerakis, Rohan Khera, David L. Reich, Monica Kraft, Alexander Charney, Girish Nadkarni},
    abstract = {  Generative Large Language Models (LLMs) hold significant promise in
healthcare, demonstrating capabilities such as passing medical licensing exams
and providing clinical knowledge. However, their current use as information
retrieval tools is limited by challenges like data staleness, resource demands,
and occasional generation of incorrect information. This study assessed the
potential of LLMs to function as autonomous agents in a simulated tertiary care
medical center, using real-world clinical cases across multiple specialties.
Both proprietary and open-source LLMs were evaluated, with Retrieval Augmented
Generation (RAG) enhancing contextual relevance. Proprietary models,
particularly GPT-4, generally outperformed open-source models, showing improved
guideline adherence and more accurate responses with RAG. The manual evaluation
by expert clinicians was crucial in validating models' outputs, underscoring
the importance of human oversight in LLM operation. Further, the study
emphasizes Natural Language Programming (NLP) as the appropriate paradigm for
modifying model behavior, allowing for precise adjustments through tailored
prompts and real-world interactions. This approach highlights the potential of
LLMs to significantly enhance and supplement clinical decision-making, while
also emphasizing the value of continuous expert involvement and the flexibility
of NLP to ensure their reliability and effectiveness in healthcare settings.
},
    year = {2024},
    month = {01},
    url = {http://arxiv.org/pdf/2401.02851v2},
    journal = {arXiv preprint}
}

@article{arxiv:Development_and_Testing_of_Retrieval_Augmented_Generation_in_Large
__Language_Models_--_A_Case_Study_Report,
    title = {Development and Testing of Retrieval Augmented Generation in Large
  Language Models -- A Case Study Report},
    author = {YuHe Ke, Liyuan Jin, Kabilan Elangovan, Hairil Rizal Abdullah, Nan Liu, Alex Tiong Heng Sia, Chai Rick Soh, Joshua Yi Min Tung, Jasmine Chiat Ling Ong, Daniel Shu Wei Ting},
    abstract = {  Purpose: Large Language Models (LLMs) hold significant promise for medical
applications. Retrieval Augmented Generation (RAG) emerges as a promising
approach for customizing domain knowledge in LLMs. This case study presents the
development and evaluation of an LLM-RAG pipeline tailored for healthcare,
focusing specifically on preoperative medicine.
  Methods: We developed an LLM-RAG model using 35 preoperative guidelines and
tested it against human-generated responses, with a total of 1260 responses
evaluated. The RAG process involved converting clinical documents into text
using Python-based frameworks like LangChain and Llamaindex, and processing
these texts into chunks for embedding and retrieval. Vector storage techniques
and selected embedding models to optimize data retrieval, using Pinecone for
vector storage with a dimensionality of 1536 and cosine similarity for loss
metrics. Human-generated answers, provided by junior doctors, were used as a
comparison.
  Results: The LLM-RAG model generated answers within an average of 15-20
seconds, significantly faster than the 10 minutes typically required by humans.
Among the basic LLMs, GPT4.0 exhibited the best accuracy of 80.1%. This
accuracy was further increased to 91.4% when the model was enhanced with RAG.
Compared to the human-generated instructions, which had an accuracy of 86.3%,
the performance of the GPT4.0 RAG model demonstrated non-inferiority (p=0.610).
  Conclusions: In this case study, we demonstrated a LLM-RAG model for
healthcare implementation. The pipeline shows the advantages of grounded
knowledge, upgradability, and scalability as important aspects of healthcare
LLM deployment.
},
    year = {2024},
    month = {01},
    url = {http://arxiv.org/pdf/2402.01733v1},
    journal = {arXiv preprint}
}

@article{arxiv:Adaptive_Knowledge_Graphs_Enhance_Medical_Question_Answering:_Bridging
__the_Gap_Between_LLMs_and_Evolving_Medical_Knowledge,
    title = {Adaptive Knowledge Graphs Enhance Medical Question Answering: Bridging
  the Gap Between LLMs and Evolving Medical Knowledge},
    author = {Mohammad Reza Rezaei, Reza Saadati Fard, Jayson Parker, Rahul G. Krishnan, Milad Lankarany},
    abstract = {  Large Language Models (LLMs) have significantly advanced medical
question-answering by leveraging extensive clinical data and medical
literature. However, the rapid evolution of medical knowledge and the
labor-intensive process of manually updating domain-specific resources pose
challenges to the reliability of these systems. To address this, we introduce
Adaptive Medical Graph-RAG (AMG-RAG), a comprehensive framework that automates
the construction and continuous updating of medical knowledge graphs,
integrates reasoning, and retrieves current external evidence, such as PubMed
and WikiSearch. By dynamically linking new findings and complex medical
concepts, AMG-RAG not only improves accuracy but also enhances interpretability
in medical queries.
  Evaluations on the MEDQA and MEDMCQA benchmarks demonstrate the effectiveness
of AMG-RAG, achieving an F1 score of 74.1 percent on MEDQA and an accuracy of
66.34 percent on MEDMCQA, outperforming both comparable models and those 10 to
100 times larger. Notably, these improvements are achieved without increasing
computational overhead, highlighting the critical role of automated knowledge
graph generation and external evidence retrieval in delivering up-to-date,
trustworthy medical insights.
},
    year = {2025},
    month = {02},
    url = {http://arxiv.org/pdf/2502.13010v1},
    journal = {arXiv preprint}
}

@article{arxiv:Document-level_Clinical_Entity_and_Relation_Extraction_via_Knowledge
__Base-Guided_Generation,
    title = {Document-level Clinical Entity and Relation Extraction via Knowledge
  Base-Guided Generation},
    author = {Kriti Bhattarai, Inez Y. Oh, Zachary B. Abrams, Albert M. Lai},
    abstract = {  Generative pre-trained transformer (GPT) models have shown promise in
clinical entity and relation extraction tasks because of their precise
extraction and contextual understanding capability. In this work, we further
leverage the Unified Medical Language System (UMLS) knowledge base to
accurately identify medical concepts and improve clinical entity and relation
extraction at the document level. Our framework selects UMLS concepts relevant
to the text and combines them with prompts to guide language models in
extracting entities. Our experiments demonstrate that this initial concept
mapping and the inclusion of these mapped concepts in the prompts improves
extraction results compared to few-shot extraction tasks on generic language
models that do not leverage UMLS. Further, our results show that this approach
is more effective than the standard Retrieval Augmented Generation (RAG)
technique, where retrieved data is compared with prompt embeddings to generate
results. Overall, we find that integrating UMLS concepts with GPT models
significantly improves entity and relation identification, outperforming the
baseline and RAG models. By combining the precise concept mapping capability of
knowledge-based approaches like UMLS with the contextual understanding
capability of GPT, our method highlights the potential of these approaches in
specialized domains like healthcare.
},
    year = {2024},
    month = {07},
    url = {http://arxiv.org/pdf/2407.10021v1},
    journal = {arXiv preprint}
}

@article{arxiv:Knowledge_Graph-Driven_Retrieval-Augmented_Generation:_Integrating
__Deepseek-R1_with_Weaviate_for_Advanced_Chatbot_Applications,
    title = {Knowledge Graph-Driven Retrieval-Augmented Generation: Integrating
  Deepseek-R1 with Weaviate for Advanced Chatbot Applications},
    author = {Alexandru Lecu, Adrian Groza, Lezan Hawizy},
    abstract = {  Large language models (LLMs) have significantly advanced the field of natural
language generation. However, they frequently generate unverified outputs,
which compromises their reliability in critical applications. In this study, we
propose an innovative framework that combines structured biomedical knowledge
with LLMs through a retrieval-augmented generation technique. Our system
develops a thorough knowledge graph by identifying and refining causal
relationships and named entities from medical abstracts related to age-related
macular degeneration (AMD). Using a vector-based retrieval process and a
locally deployed language model, our framework produces responses that are both
contextually relevant and verifiable, with direct references to clinical
evidence. Experimental results show that this method notably decreases
hallucinations, enhances factual precision, and improves the clarity of
generated responses, providing a robust solution for advanced biomedical
chatbot applications.
},
    year = {2025},
    month = {02},
    url = {http://arxiv.org/pdf/2502.11108v1},
    journal = {arXiv preprint}
}

@article{arxiv:QuaLLM-Health:_An_Adaptation_of_an_LLM-Based_Framework_for_Quantitative
__Data_Extraction_from_Online_Health_Discussions,
    title = {QuaLLM-Health: An Adaptation of an LLM-Based Framework for Quantitative
  Data Extraction from Online Health Discussions},
    author = {Ramez Kouzy, Roxanna Attar-Olyaee, Michael K. Rooney, Comron J. Hassanzadeh, Junyi Jessy Li, Osama Mohamad},
    abstract = {  Health-related discussions on social media like Reddit offer valuable
insights, but extracting quantitative data from unstructured text is
challenging. In this work, we present an adapted framework from QuaLLM into
QuaLLM-Health for extracting clinically relevant quantitative data from Reddit
discussions about glucagon-like peptide-1 (GLP-1) receptor agonists using large
language models (LLMs). We collected 410k posts and comments from five
GLP-1-related communities using the Reddit API in July 2024. After filtering
for cancer-related discussions, 2,059 unique entries remained. We developed
annotation guidelines to manually extract variables such as cancer
survivorship, family cancer history, cancer types mentioned, risk perceptions,
and discussions with physicians. Two domain-experts independently annotated a
random sample of 100 entries to create a gold-standard dataset. We then
employed iterative prompt engineering with OpenAI's "GPT-4o-mini" on the
gold-standard dataset to build an optimized pipeline that allowed us to extract
variables from the large dataset. The optimized LLM achieved accuracies above
0.85 for all variables, with precision, recall and F1 score macro averaged >
0.90, indicating balanced performance. Stability testing showed a 95% match
rate across runs, confirming consistency. Applying the framework to the full
dataset enabled efficient extraction of variables necessary for downstream
analysis, costing under $3 and completing in approximately one hour.
QuaLLM-Health demonstrates that LLMs can effectively and efficiently extract
clinically relevant quantitative data from unstructured social media content.
Incorporating human expertise and iterative prompt refinement ensures accuracy
and reliability. This methodology can be adapted for large-scale analysis of
patient-generated data across various health domains, facilitating valuable
insights for healthcare research.
},
    year = {2024},
    month = {11},
    url = {http://arxiv.org/pdf/2411.17967v1},
    journal = {arXiv preprint}
}

@article{arxiv:Development_and_Testing_of_a_Novel_Large_Language_Model-Based_Clinical
__Decision_Support_Systems_for_Medication_Safety_in_12_Clinical_Specialties,
    title = {Development and Testing of a Novel Large Language Model-Based Clinical
  Decision Support Systems for Medication Safety in 12 Clinical Specialties},
    author = {Jasmine Chiat Ling Ong, Liyuan Jin, Kabilan Elangovan, Gilbert Yong San Lim, Daniel Yan Zheng Lim, Gerald Gui Ren Sng, Yuhe Ke, Joshua Yi Min Tung, Ryan Jian Zhong, Christopher Ming Yao Koh, Keane Zhi Hao Lee, Xiang Chen, Jack Kian Chng, Aung Than, Ken Junyang Goh, Daniel Shu Wei Ting},
    abstract = {  Importance: We introduce a novel Retrieval Augmented Generation (RAG)-Large
Language Model (LLM) framework as a Clinical Decision Support Systems (CDSS) to
support safe medication prescription.
  Objective: To evaluate the efficacy of LLM-based CDSS in correctly
identifying medication errors in different patient case vignettes from diverse
medical and surgical sub-disciplines, against a human expert panel derived
ground truth. We compared performance for under 2 different CDSS practical
healthcare integration modalities: LLM-based CDSS alone (fully autonomous mode)
vs junior pharmacist + LLM-based CDSS (co-pilot, assistive mode).
  Design, Setting, and Participants: Utilizing a RAG model with
state-of-the-art medically-related LLMs (GPT-4, Gemini Pro 1.0 and Med-PaLM 2),
this study used 61 prescribing error scenarios embedded into 23 complex
clinical vignettes across 12 different medical and surgical specialties. A
multidisciplinary expert panel assessed these cases for Drug-Related Problems
(DRPs) using the PCNE classification and graded severity / potential for harm
using revised NCC MERP medication error index. We compared.
  Results RAG-LLM performed better compared to LLM alone. When employed in a
co-pilot mode, accuracy, recall, and F1 scores were optimized, indicating
effectiveness in identifying moderate to severe DRPs. The accuracy of DRP
detection with RAG-LLM improved in several categories but at the expense of
lower precision.
  Conclusions This study established that a RAG-LLM based CDSS significantly
boosts the accuracy of medication error identification when used alongside
junior pharmacists (co-pilot), with notable improvements in detecting severe
DRPs. This study also illuminates the comparative performance of current
state-of-the-art LLMs in RAG-based CDSS systems.
},
    year = {2024},
    month = {01},
    url = {http://arxiv.org/pdf/2402.01741v2},
    journal = {arXiv preprint}
}

@article{arxiv:Performance_of_Large_Language_Models_in_Technical_MRI_Question
__Answering:_A_Comparative_Study,
    title = {Performance of Large Language Models in Technical MRI Question
  Answering: A Comparative Study},
    author = {Alan B McMillan},
    abstract = {  Background: Advances in artificial intelligence, particularly large language
models (LLMs), have the potential to enhance technical expertise in magnetic
resonance imaging (MRI), regardless of operator skill or geographic location.
  Methods: We assessed the accuracy of several LLMs in answering 570 technical
MRI questions derived from a standardized review book. The questions spanned
nine MRI topics, including Basic Principles, Image Production, and Safety.
Closed-source models (e.g., OpenAI's o1 Preview, GPT-4o, GPT-4 Turbo, and
Claude 3.5 Haiku) and open-source models (e.g., Phi 3.5 Mini, Llama 3.1,
smolLM2) were tested. Models were queried using standardized prompts via the
LangChain framework, and responses were graded against correct answers using an
automated scoring protocol. Accuracy, defined as the proportion of correct
answers, was the primary outcome.
  Results: The closed-source o1 Preview model achieved the highest accuracy
(94%), exceeding the random-guess baseline (26.5%). GPT-4o and o1 Mini scored
88%, and GPT-4 Turbo and Claude 3.5 Haiku each scored 84%. Among open-source
models, Phi 3.5 Mini performed well, achieving 78% accuracy, comparable to
several closed-source models. Accuracy was highest in Basic Principles and
Instrumentation categories but lower in Image Weighting and Contrast, History,
and Artifacts and Corrections.
  Conclusions: LLMs exhibit high accuracy in addressing technical MRI
questions, suggesting their potential to standardize and enhance MRI practice.
These models may improve image quality and consistency across varied clinical
environments. Further studies are needed to refine LLMs for clinical use and
integrate them into MRI workflows.
},
    year = {2024},
    month = {11},
    url = {http://arxiv.org/pdf/2411.12238v1},
    journal = {arXiv preprint}
}

@article{arxiv:Unlocking_Historical_Clinical_Trial_Data_with_ALIGN:_A_Compositional
__Large_Language_Model_System_for_Medical_Coding,
    title = {Unlocking Historical Clinical Trial Data with ALIGN: A Compositional
  Large Language Model System for Medical Coding},
    author = {Nabeel Seedat, Caterina Tozzi, Andrea Hita Ardiaca, Mihaela van der Schaar, James Weatherall, Adam Taylor},
    abstract = {  The reuse of historical clinical trial data has significant potential to
accelerate medical research and drug development. However, interoperability
challenges, particularly with missing medical codes, hinders effective data
integration across studies. While Large Language Models (LLMs) offer a
promising solution for automated coding without labeled data, current
approaches face challenges on complex coding tasks. We introduce ALIGN, a novel
compositional LLM-based system for automated, zero-shot medical coding. ALIGN
follows a three-step process: (1) diverse candidate code generation; (2)
self-evaluation of codes and (3) confidence scoring and uncertainty estimation
enabling human deferral to ensure reliability. We evaluate ALIGN on harmonizing
medication terms into Anatomical Therapeutic Chemical (ATC) and medical history
terms into Medical Dictionary for Regulatory Activities (MedDRA) codes
extracted from 22 immunology trials. ALIGN outperformed the LLM baselines,
while also providing capabilities for trustworthy deployment. For MedDRA
coding, ALIGN achieved high accuracy across all levels, matching RAG and
excelling at the most specific levels (87-90% for HLGT). For ATC coding, ALIGN
demonstrated superior performance, particularly at lower hierarchy levels (ATC
Level 4), with 72-73% overall accuracy and 86-89% accuracy for common
medications, outperforming baselines by 7-22%. ALIGN's uncertainty-based
deferral improved accuracy by 17% to 90% accuracy with 30% deferral, notably
enhancing performance on uncommon medications. ALIGN achieves this
cost-efficiently at \$0.0007 and \$0.02 per code for GPT-4o-mini and GPT-4o,
reducing barriers to clinical adoption. ALIGN advances automated medical coding
for clinical trial data, contributing to enhanced data interoperability and
reusability, positioning it as a promising tool to improve clinical research
and accelerate drug development.
},
    year = {2024},
    month = {11},
    url = {http://arxiv.org/pdf/2411.13163v1},
    journal = {arXiv preprint}
}

@article{arxiv:JMLR:_Joint_Medical_LLM_and_Retrieval_Training_for_Enhancing_Reasoning
__and_Professional_Question_Answering_Capability,
    title = {JMLR: Joint Medical LLM and Retrieval Training for Enhancing Reasoning
  and Professional Question Answering Capability},
    author = {Junda Wang, Zhichao Yang, Zonghai Yao, Hong Yu},
    abstract = {  Large Language Models (LLMs) have demonstrated a remarkable potential in
medical knowledge acquisition and question-answering. However, LLMs can
potentially hallucinate and yield factually incorrect outcomes, even with
domain-specific pretraining. Previously, retrieval augmented generation (RAG)
has limited success in addressing hallucinations. Unlike previous methods in
RAG where the retrieval model was trained separately from the LLM, we introduce
JMLR (for Jointly trains LLM and information Retrieval) during the fine-tuning
phase. The synchronized training mechanism enhances JMLR's ability to retrieve
clinical guidelines and leverage medical knowledge to reason and answer
questions and reduces the demand for computational resources. We evaluated JMLR
on the important medical question-answering application. Our experimental
results demonstrate that JMLR-13B (70.5%) outperforms a previous
state-of-the-art open-source model using conventional pre-training and
fine-tuning Meditron-70B (68.9%) and Llama2-13B with RAG (67.7%) on a medical
question-answering dataset. Comprehensive evaluations reveal JMLR-13B enhances
reasoning quality and reduces hallucinations better than Claude3-Opus.
Additionally, JMLR-13B (148 GPU hours) also trains much faster than
Meditron-70B (42630 GPU hours). Through this work, we provide a new and
efficient knowledge enhancement method for healthcare, demonstrating the
potential of integrating retrieval and LLM training for medical
question-answering systems.
},
    year = {2024},
    month = {02},
    url = {http://arxiv.org/pdf/2402.17887v4},
    journal = {arXiv preprint}
}

@article{arxiv:README:_Bridging_Medical_Jargon_and_Lay_Understanding_for_Patient
__Education_through_Data-Centric_NLP,
    title = {README: Bridging Medical Jargon and Lay Understanding for Patient
  Education through Data-Centric NLP},
    author = {Zonghai Yao, Nandyala Siddharth Kantu, Guanghao Wei, Hieu Tran, Zhangqi Duan, Sunjae Kwon, Zhichao Yang, README annotation team, Hong Yu},
    abstract = {  The advancement in healthcare has shifted focus toward patient-centric
approaches, particularly in self-care and patient education, facilitated by
access to Electronic Health Records (EHR). However, medical jargon in EHRs
poses significant challenges in patient comprehension. To address this, we
introduce a new task of automatically generating lay definitions, aiming to
simplify complex medical terms into patient-friendly lay language. We first
created the README dataset, an extensive collection of over 50,000 unique
(medical term, lay definition) pairs and 300,000 mentions, each offering
context-aware lay definitions manually annotated by domain experts. We have
also engineered a data-centric Human-AI pipeline that synergizes data
filtering, augmentation, and selection to improve data quality. We then used
README as the training data for models and leveraged a Retrieval-Augmented
Generation method to reduce hallucinations and improve the quality of model
outputs. Our extensive automatic and human evaluations demonstrate that
open-source mobile-friendly models, when fine-tuned with high-quality data, are
capable of matching or even surpassing the performance of state-of-the-art
closed-source large language models like ChatGPT. This research represents a
significant stride in closing the knowledge gap in patient education and
advancing patient-centric healthcare solutions.
},
    year = {2023},
    month = {12},
    url = {http://arxiv.org/pdf/2312.15561v5},
    journal = {arXiv preprint}
}

@article{arxiv:Autonomous_Artificial_Intelligence_Agents_for_Clinical_Decision_Making
__in_Oncology,
    title = {Autonomous Artificial Intelligence Agents for Clinical Decision Making
  in Oncology},
    author = {Dyke Ferber, Omar S. M. El Nahhas, Georg W√∂lflein, Isabella C. Wiest, Jan Clusmann, Marie-Elisabeth Le√üman, Sebastian Foersch, Jacqueline Lammert, Maximilian Tschochohei, Dirk J√§ger, Manuel Salto-Tellez, Nikolaus Schultz, Daniel Truhn, Jakob Nikolas Kather},
    abstract = {  Multimodal artificial intelligence (AI) systems have the potential to enhance
clinical decision-making by interpreting various types of medical data.
However, the effectiveness of these models across all medical fields is
uncertain. Each discipline presents unique challenges that need to be addressed
for optimal performance. This complexity is further increased when attempting
to integrate different fields into a single model. Here, we introduce an
alternative approach to multimodal medical AI that utilizes the generalist
capabilities of a large language model (LLM) as a central reasoning engine.
This engine autonomously coordinates and deploys a set of specialized medical
AI tools. These tools include text, radiology and histopathology image
interpretation, genomic data processing, web searches, and document retrieval
from medical guidelines. We validate our system across a series of clinical
oncology scenarios that closely resemble typical patient care workflows. We
show that the system has a high capability in employing appropriate tools
(97%), drawing correct conclusions (93.6%), and providing complete (94%), and
helpful (89.2%) recommendations for individual patient cases while consistently
referencing relevant literature (82.5%) upon instruction. This work provides
evidence that LLMs can effectively plan and execute domain-specific models to
retrieve or synthesize new information when used as autonomous agents. This
enables them to function as specialist, patient-tailored clinical assistants.
It also simplifies regulatory compliance by allowing each component tool to be
individually validated and approved. We believe, that our work can serve as a
proof-of-concept for more advanced LLM-agents in the medical domain.
},
    year = {2024},
    month = {04},
    url = {http://arxiv.org/pdf/2404.04667v1},
    journal = {arXiv preprint}
}

@article{arxiv:Small_Models_are_LLM_Knowledge_Triggers_on_Medical_Tabular_Prediction,
    title = {Small Models are LLM Knowledge Triggers on Medical Tabular Prediction},
    author = {Jiahuan Yan, Jintai Chen, Chaowen Hu, Bo Zheng, Yaojun Hu, Jimeng Sun, Jian Wu},
    abstract = {  Recent development in large language models (LLMs) has demonstrated
impressive domain proficiency on unstructured textual or multi-modal tasks.
However, despite with intrinsic world knowledge, their application on
structured tabular data prediction still lags behind, primarily due to the
numerical insensitivity and modality discrepancy that brings a gap between LLM
reasoning and statistical tabular learning. Unlike textual or vision data
(e.g., electronic clinical notes or medical imaging data), tabular data is
often presented in heterogeneous numerical values (e.g., CBC reports). This
ubiquitous data format requires intensive expert annotation, and its numerical
nature limits LLMs' capability to effectively transfer untapped domain
expertise. In this paper, we propose SERSAL, a general self-prompting method by
synergy learning with small models to enhance LLM tabular prediction in an
unsupervised manner. Specifically, SERSAL utilizes the LLM's prior outcomes as
original soft noisy annotations, which are dynamically leveraged to teach a
better small student model. Reversely, the outcomes from the trained small
model are used to teach the LLM to further refine its real capability. This
process can be repeatedly applied to gradually distill refined knowledge for
continuous progress. Comprehensive experiments on widely used medical domain
tabular datasets show that, without access to gold labels, applying SERSAL to
OpenAI GPT reasoning process attains substantial improvement compared to
linguistic prompting methods, which serves as an orthogonal direction for
tabular LLM, and increasing prompting bonus is observed as more powerful LLMs
appear.
},
    year = {2024},
    month = {03},
    url = {http://arxiv.org/pdf/2403.01570v3},
    journal = {arXiv preprint}
}

@article{arxiv:IITK_at_SemEval-2024_Task_2:_Exploring_the_Capabilities_of_LLMs_for_Safe
__Biomedical_Natural_Language_Inference_for_Clinical_Trials,
    title = {IITK at SemEval-2024 Task 2: Exploring the Capabilities of LLMs for Safe
  Biomedical Natural Language Inference for Clinical Trials},
    author = {Shreyasi Mandal, Ashutosh Modi},
    abstract = {  Large Language models (LLMs) have demonstrated state-of-the-art performance
in various natural language processing (NLP) tasks across multiple domains, yet
they are prone to shortcut learning and factual inconsistencies. This research
investigates LLMs' robustness, consistency, and faithful reasoning when
performing Natural Language Inference (NLI) on breast cancer Clinical Trial
Reports (CTRs) in the context of SemEval 2024 Task 2: Safe Biomedical Natural
Language Inference for Clinical Trials. We examine the reasoning capabilities
of LLMs and their adeptness at logical problem-solving. A comparative analysis
is conducted on pre-trained language models (PLMs), GPT-3.5, and Gemini Pro
under zero-shot settings using Retrieval-Augmented Generation (RAG) framework,
integrating various reasoning chains. The evaluation yields an F1 score of
0.69, consistency of 0.71, and a faithfulness score of 0.90 on the test
dataset.
},
    year = {2024},
    month = {04},
    url = {http://arxiv.org/pdf/2404.04510v1},
    journal = {arXiv preprint}
}

@article{arxiv:Integrating_Knowledge_Retrieval_and_Large_Language_Models_for_Clinical
__Report_Correction,
    title = {Integrating Knowledge Retrieval and Large Language Models for Clinical
  Report Correction},
    author = {Jinge Wu, Zhaolong Wu, Ruizhe Li, Abul Hasan, Yunsoo Kim, Jason P. Y. Cheung, Teng Zhang, Honghan Wu},
    abstract = {  This study proposes an approach for error correction in radiology reports,
leveraging large language models (LLMs) and retrieval-augmented generation
(RAG) techniques. The proposed framework employs a novel internal+external
retrieval mechanism to extract relevant medical entities and relations from the
report of interest and an external knowledge source. A three-stage inference
process is introduced, decomposing the task into error detection, localization,
and correction subtasks, which enhances the explainability and performance of
the system. The effectiveness of the approach is evaluated using a benchmark
dataset created by corrupting real-world radiology reports with realistic
errors, guided by domain experts. Experimental results demonstrate the benefits
of the proposed methods, with the combination of internal and external
retrieval significantly improving the accuracy of error detection,
localization, and correction across various state-of-the-art LLMs. The findings
contribute to the development of more robust and reliable error correction
systems for clinical documentation.
},
    year = {2024},
    month = {06},
    url = {http://arxiv.org/pdf/2406.15045v2},
    journal = {arXiv preprint}
}

@article{arxiv:Prompt_Injection_Attacks_on_Large_Language_Models_in_Oncology,
    title = {Prompt Injection Attacks on Large Language Models in Oncology},
    author = {Jan Clusmann, Dyke Ferber, Isabella C. Wiest, Carolin V. Schneider, Titus J. Brinker, Sebastian Foersch, Daniel Truhn, Jakob N. Kather},
    abstract = {  Vision-language artificial intelligence models (VLMs) possess medical
knowledge and can be employed in healthcare in numerous ways, including as
image interpreters, virtual scribes, and general decision support systems.
However, here, we demonstrate that current VLMs applied to medical tasks
exhibit a fundamental security flaw: they can be attacked by prompt injection
attacks, which can be used to output harmful information just by interacting
with the VLM, without any access to its parameters. We performed a quantitative
study to evaluate the vulnerabilities to these attacks in four state of the art
VLMs which have been proposed to be of utility in healthcare: Claude 3 Opus,
Claude 3.5 Sonnet, Reka Core, and GPT-4o. Using a set of N=297 attacks, we show
that all of these models are susceptible. Specifically, we show that embedding
sub-visual prompts in medical imaging data can cause the model to provide
harmful output, and that these prompts are non-obvious to human observers.
Thus, our study demonstrates a key vulnerability in medical VLMs which should
be mitigated before widespread clinical adoption.
},
    year = {2024},
    month = {07},
    url = {http://arxiv.org/pdf/2407.18981v1},
    journal = {arXiv preprint}
}

@article{arxiv:oRetrieval_Augmented_Generation_for_10_Large_Language_Models_and_its
__Generalizability_in_Assessing_Medical_Fitness,
    title = {oRetrieval Augmented Generation for 10 Large Language Models and its
  Generalizability in Assessing Medical Fitness},
    author = {Yu He Ke, Liyuan Jin, Kabilan Elangovan, Hairil Rizal Abdullah, Nan Liu, Alex Tiong Heng Sia, Chai Rick Soh, Joshua Yi Min Tung, Jasmine Chiat Ling Ong, Chang-Fu Kuo, Shao-Chun Wu, Vesela P. Kovacheva, Daniel Shu Wei Ting},
    abstract = {  Large Language Models (LLMs) show potential for medical applications but
often lack specialized clinical knowledge. Retrieval Augmented Generation (RAG)
allows customization with domain-specific information, making it suitable for
healthcare. This study evaluates the accuracy, consistency, and safety of RAG
models in determining fitness for surgery and providing preoperative
instructions. We developed LLM-RAG models using 35 local and 23 international
preoperative guidelines and tested them against human-generated responses. A
total of 3,682 responses were evaluated. Clinical documents were processed
using Llamaindex, and 10 LLMs, including GPT3.5, GPT4, and Claude-3, were
assessed. Fourteen clinical scenarios were analyzed, focusing on seven aspects
of preoperative instructions. Established guidelines and expert judgment were
used to determine correct responses, with human-generated answers serving as
comparisons. The LLM-RAG models generated responses within 20 seconds,
significantly faster than clinicians (10 minutes). The GPT4 LLM-RAG model
achieved the highest accuracy (96.4% vs. 86.6%, p=0.016), with no
hallucinations and producing correct instructions comparable to clinicians.
Results were consistent across both local and international guidelines. This
study demonstrates the potential of LLM-RAG models for preoperative healthcare
tasks, highlighting their efficiency, scalability, and reliability.
},
    year = {2024},
    month = {10},
    url = {http://arxiv.org/pdf/2410.08431v1},
    journal = {arXiv preprint}
}

@article{arxiv:Beyond_Self-Consistency:_Ensemble_Reasoning_Boosts_Consistency_and
__Accuracy_of_LLMs_in_Cancer_Staging,
    title = {Beyond Self-Consistency: Ensemble Reasoning Boosts Consistency and
  Accuracy of LLMs in Cancer Staging},
    author = {Chia-Hsuan Chang, Mary M. Lucas, Yeawon Lee, Christopher C. Yang, Grace Lu-Yao},
    abstract = {  Advances in large language models (LLMs) have encouraged their adoption in
the healthcare domain where vital clinical information is often contained in
unstructured notes. Cancer staging status is available in clinical reports, but
it requires natural language processing to extract the status from the
unstructured text. With the advance in clinical-oriented LLMs, it is promising
to extract such status without extensive efforts in training the algorithms.
Prompting approaches of the pre-trained LLMs that elicit a model's reasoning
process, such as chain-of-thought, may help to improve the trustworthiness of
the generated responses. Using self-consistency further improves model
performance, but often results in inconsistent generations across the multiple
reasoning paths. In this study, we propose an ensemble reasoning approach with
the aim of improving the consistency of the model generations. Using an open
access clinical large language model to determine the pathologic cancer stage
from real-world pathology reports, we show that the ensemble reasoning approach
is able to improve both the consistency and performance of the LLM in
determining cancer stage, thereby demonstrating the potential to use these
models in clinical or other domains where reliability and trustworthiness are
critical.
},
    year = {2024},
    month = {04},
    url = {http://arxiv.org/pdf/2404.13149v1},
    journal = {arXiv preprint}
}

@article{arxiv:Aligning_Large_Language_Models_for_Clinical_Tasks,
    title = {Aligning Large Language Models for Clinical Tasks},
    author = {Supun Manathunga, Isuru Hettigoda},
    abstract = {  Large Language Models (LLMs) have demonstrated remarkable adaptability,
showcasing their capacity to excel in tasks for which they were not explicitly
trained. However, despite their impressive natural language processing (NLP)
capabilities, effective alignment of LLMs remains a crucial challenge when
deploying them for specific clinical applications. The ability to generate
responses with factually accurate content and to engage in non-trivial
reasoning steps are crucial for the LLMs to be eligible for applications in
clinical medicine. Employing a combination of techniques including
instruction-tuning and in-prompt strategies like few-shot and chain-of-thought
prompting has significantly enhanced the performance of LLMs. Our proposed
alignment strategy for medical question-answering, known as
'expand-guess-refine', offers a parameter and data-efficient solution. A
preliminary analysis of this method demonstrated outstanding performance,
achieving a score of 70.63% on a subset of questions sourced from the USMLE
dataset.
},
    year = {2023},
    month = {09},
    url = {http://arxiv.org/pdf/2309.02884v2},
    journal = {arXiv preprint}
}

@article{arxiv:Improving_Representation_Learning_of_Complex_Critical_Care_Data_with
__ICU-BERT,
    title = {Improving Representation Learning of Complex Critical Care Data with
  ICU-BERT},
    author = {Ricardo Santos, Andr√© V. Carreiro, Xi Peng, Hugo Gamboa, Holger Fr√∂hlich},
    abstract = {  The multivariate, asynchronous nature of real-world clinical data, such as
that generated in Intensive Care Units (ICUs), challenges traditional AI-based
decision-support systems. These often assume data regularity and feature
independence and frequently rely on limited data scopes and manual feature
engineering. The potential of generative AI technologies has not yet been fully
exploited to analyze clinical data. We introduce ICU-BERT, a transformer-based
model pre-trained on the MIMIC-IV database using a multi-task scheme to learn
robust representations of complex ICU data with minimal preprocessing. ICU-BERT
employs a multi-token input strategy, incorporating dense embeddings from a
biomedical Large Language Model to learn a generalizable representation of
complex and multivariate ICU data. With an initial evaluation of five tasks and
four additional ICU datasets, ICU-BERT results indicate that ICU-BERT either
compares to or surpasses current performance benchmarks by leveraging
fine-tuning. By integrating structured and unstructured data, ICU-BERT advances
the use of foundational models in medical informatics, offering an adaptable
solution for clinical decision support across diverse applications.
},
    year = {2025},
    month = {02},
    url = {http://arxiv.org/pdf/2502.19593v1},
    journal = {arXiv preprint}
}

@article{arxiv:MALADE:_Orchestration_of_LLM-powered_Agents_with_Retrieval_Augmented
__Generation_for_Pharmacovigilance,
    title = {MALADE: Orchestration of LLM-powered Agents with Retrieval Augmented
  Generation for Pharmacovigilance},
    author = {Jihye Choi, Nils Palumbo, Prasad Chalasani, Matthew M. Engelhard, Somesh Jha, Anivarya Kumar, David Page},
    abstract = {  In the era of Large Language Models (LLMs), given their remarkable text
understanding and generation abilities, there is an unprecedented opportunity
to develop new, LLM-based methods for trustworthy medical knowledge synthesis,
extraction and summarization. This paper focuses on the problem of
Pharmacovigilance (PhV), where the significance and challenges lie in
identifying Adverse Drug Events (ADEs) from diverse text sources, such as
medical literature, clinical notes, and drug labels. Unfortunately, this task
is hindered by factors including variations in the terminologies of drugs and
outcomes, and ADE descriptions often being buried in large amounts of narrative
text. We present MALADE, the first effective collaborative multi-agent system
powered by LLM with Retrieval Augmented Generation for ADE extraction from drug
label data. This technique involves augmenting a query to an LLM with relevant
information extracted from text resources, and instructing the LLM to compose a
response consistent with the augmented data. MALADE is a general LLM-agnostic
architecture, and its unique capabilities are: (1) leveraging a variety of
external sources, such as medical literature, drug labels, and FDA tools (e.g.,
OpenFDA drug information API), (2) extracting drug-outcome association in a
structured format along with the strength of the association, and (3) providing
explanations for established associations. Instantiated with GPT-4 Turbo or
GPT-4o, and FDA drug label data, MALADE demonstrates its efficacy with an Area
Under ROC Curve of 0.90 against the OMOP Ground Truth table of ADEs. Our
implementation leverages the Langroid multi-agent LLM framework and can be
found at https://github.com/jihyechoi77/malade.
},
    year = {2024},
    month = {08},
    url = {http://arxiv.org/pdf/2408.01869v1},
    journal = {arXiv preprint}
}

@article{arxiv:How_well_do_LLMs_cite_relevant_medical_references?_An_evaluation
__framework_and_analyses,
    title = {How well do LLMs cite relevant medical references? An evaluation
  framework and analyses},
    author = {Kevin Wu, Eric Wu, Ally Cassasola, Angela Zhang, Kevin Wei, Teresa Nguyen, Sith Riantawan, Patricia Shi Riantawan, Daniel E. Ho, James Zou},
    abstract = {  Large language models (LLMs) are currently being used to answer medical
questions across a variety of clinical domains. Recent top-performing
commercial LLMs, in particular, are also capable of citing sources to support
their responses. In this paper, we ask: do the sources that LLMs generate
actually support the claims that they make? To answer this, we propose three
contributions. First, as expert medical annotations are an expensive and
time-consuming bottleneck for scalable evaluation, we demonstrate that GPT-4 is
highly accurate in validating source relevance, agreeing 88% of the time with a
panel of medical doctors. Second, we develop an end-to-end, automated pipeline
called \textit{SourceCheckup} and use it to evaluate five top-performing LLMs
on a dataset of 1200 generated questions, totaling over 40K pairs of statements
and sources. Interestingly, we find that between ~50% to 90% of LLM responses
are not fully supported by the sources they provide. We also evaluate GPT-4
with retrieval augmented generation (RAG) and find that, even still, around
30\% of individual statements are unsupported, while nearly half of its
responses are not fully supported. Third, we open-source our curated dataset of
medical questions and expert annotations for future evaluations. Given the
rapid pace of LLM development and the potential harms of incorrect or outdated
medical information, it is crucial to also understand and quantify their
capability to produce relevant, trustworthy medical references.
},
    year = {2024},
    month = {02},
    url = {http://arxiv.org/pdf/2402.02008v1},
    journal = {arXiv preprint}
}

@article{arxiv:The_Sound_of_Healthcare:_Improving_Medical_Transcription_ASR_Accuracy
__with_Large_Language_Models,
    title = {The Sound of Healthcare: Improving Medical Transcription ASR Accuracy
  with Large Language Models},
    author = {Ayo Adedeji, Sarita Joshi, Brendan Doohan},
    abstract = {  In the rapidly evolving landscape of medical documentation, transcribing
clinical dialogues accurately is increasingly paramount. This study explores
the potential of Large Language Models (LLMs) to enhance the accuracy of
Automatic Speech Recognition (ASR) systems in medical transcription. Utilizing
the PriMock57 dataset, which encompasses a diverse range of primary care
consultations, we apply advanced LLMs to refine ASR-generated transcripts. Our
research is multifaceted, focusing on improvements in general Word Error Rate
(WER), Medical Concept WER (MC-WER) for the accurate transcription of essential
medical terms, and speaker diarization accuracy. Additionally, we assess the
role of LLM post-processing in improving semantic textual similarity, thereby
preserving the contextual integrity of clinical dialogues. Through a series of
experiments, we compare the efficacy of zero-shot and Chain-of-Thought (CoT)
prompting techniques in enhancing diarization and correction accuracy. Our
findings demonstrate that LLMs, particularly through CoT prompting, not only
improve the diarization accuracy of existing ASR systems but also achieve
state-of-the-art performance in this domain. This improvement extends to more
accurately capturing medical concepts and enhancing the overall semantic
coherence of the transcribed dialogues. These findings illustrate the dual role
of LLMs in augmenting ASR outputs and independently excelling in transcription
tasks, holding significant promise for transforming medical ASR systems and
leading to more accurate and reliable patient records in healthcare settings.
},
    year = {2024},
    month = {02},
    url = {http://arxiv.org/pdf/2402.07658v1},
    journal = {arXiv preprint}
}

@article{arxiv:XAIQA:_Explainer-Based_Data_Augmentation_for_Extractive_Question
__Answering,
    title = {XAIQA: Explainer-Based Data Augmentation for Extractive Question
  Answering},
    author = {Joel Stremmel, Ardavan Saeedi, Hamid Hassanzadeh, Sanjit Batra, Jeffrey Hertzberg, Jaime Murillo, Eran Halperin},
    abstract = {  Extractive question answering (QA) systems can enable physicians and
researchers to query medical records, a foundational capability for designing
clinical studies and understanding patient medical history. However, building
these systems typically requires expert-annotated QA pairs. Large language
models (LLMs), which can perform extractive QA, depend on high quality data in
their prompts, specialized for the application domain. We introduce a novel
approach, XAIQA, for generating synthetic QA pairs at scale from data naturally
available in electronic health records. Our method uses the idea of a
classification model explainer to generate questions and answers about medical
concepts corresponding to medical codes. In an expert evaluation with two
physicians, our method identifies $2.2\times$ more semantic matches and
$3.8\times$ more clinical abbreviations than two popular approaches that use
sentence transformers to create QA pairs. In an ML evaluation, adding our QA
pairs improves performance of GPT-4 as an extractive QA model, including on
difficult questions. In both the expert and ML evaluations, we examine
trade-offs between our method and sentence transformers for QA pair generation
depending on question difficulty.
},
    year = {2023},
    month = {12},
    url = {http://arxiv.org/pdf/2312.03567v1},
    journal = {arXiv preprint}
}

@article{arxiv:Reducing_Hallucinations_of_Medical_Multimodal_Large_Language_Models_with
__Visual_Retrieval-Augmented_Generation,
    title = {Reducing Hallucinations of Medical Multimodal Large Language Models with
  Visual Retrieval-Augmented Generation},
    author = {Yun-Wei Chu, Kai Zhang, Christopher Malon, Martin Renqiang Min},
    abstract = {  Multimodal Large Language Models (MLLMs) have shown impressive performance in
vision and text tasks. However, hallucination remains a major challenge,
especially in fields like healthcare where details are critical. In this work,
we show how MLLMs may be enhanced to support Visual RAG (V-RAG), a
retrieval-augmented generation framework that incorporates both text and visual
data from retrieved images. On the MIMIC-CXR chest X-ray report generation and
Multicare medical image caption generation datasets, we show that Visual RAG
improves the accuracy of entity probing, which asks whether a medical entities
is grounded by an image. We show that the improvements extend both to frequent
and rare entities, the latter of which may have less positive training data.
Downstream, we apply V-RAG with entity probing to correct hallucinations and
generate more clinically accurate X-ray reports, obtaining a higher RadGraph-F1
score.
},
    year = {2025},
    month = {02},
    url = {http://arxiv.org/pdf/2502.15040v1},
    journal = {arXiv preprint}
}

@article{arxiv:Exploring_LLM_Multi-Agents_for_ICD_Coding,
    title = {Exploring LLM Multi-Agents for ICD Coding},
    author = {Rumeng Li, Xun Wang, Hong Yu},
    abstract = {  To address the limitations of Large Language Models (LLMs) in the
International Classification of Diseases (ICD) coding task, where they often
produce inaccurate and incomplete prediction results due to the
high-dimensional and skewed distribution of the ICD codes, and often lack
interpretability and reliability as well. We introduce an innovative
multi-agent approach for ICD coding which mimics the ICD coding assignment
procedure in real-world settings, comprising five distinct agents: the patient,
physician, coder, reviewer, and adjuster. Each agent utilizes an LLM-based
model tailored to their specific role within the coding process. We also
integrate the system with Electronic Health Record (HER)'s SOAP (subjective,
objective, assessment and plan) structure to boost the performances. We compare
our method with a system of agents designed solely by LLMs and other strong
baselines and evaluate it using the Medical Information Mart for Intensive Care
III (MIMIC-III) dataset. Our multi-agent coding framework significantly
outperforms Zero-shot Chain of Thought (CoT) prompting and self-consistency
with CoT (CoT-SC) in coding common and rare ICD codes. An ablation study
validates the effectiveness of the designated agent roles. it also outperforms
the LLM-designed agent system. Moreover, our method achieves comparable results
to state-of-the-art ICD coding methods that require extensive pre-training or
fine-tuning, and outperforms them in rare code accuracy, and explainability.
Additionally, we demonstrate the method's practical applicability by presenting
its performance in scenarios not limited by the common or rare ICD code
constraints.The proposed multi-agent method for ICD coding effectively mimics
the real-world coding process and improves performance on both common and rare
codes.
},
    year = {2024},
    month = {04},
    url = {http://arxiv.org/pdf/2406.15363v2},
    journal = {arXiv preprint}
}

@article{arxiv:Enhancing_LLMs_for_Identifying_and_Prioritizing_Important_Medical
__Jargons_from_Electronic_Health_Record_Notes_Utilizing_Data_Augmentation,
    title = {Enhancing LLMs for Identifying and Prioritizing Important Medical
  Jargons from Electronic Health Record Notes Utilizing Data Augmentation},
    author = {Won Seok Jang, Sharmin Sultana, Zonghai Yao, Hieu Tran, Zhichao Yang, Sunjae Kwon, Hong Yu},
    abstract = {  OpenNotes enables patients to access EHR notes, but medical jargon can hinder
comprehension. To improve understanding, we evaluated closed- and open-source
LLMs for extracting and prioritizing key medical terms using prompting,
fine-tuning, and data augmentation. We assessed LLMs on 106 expert-annotated
EHR notes, experimenting with (i) general vs. structured prompts, (ii)
zero-shot vs. few-shot prompting, (iii) fine-tuning, and (iv) data
augmentation. To enhance open-source models in low-resource settings, we used
ChatGPT for data augmentation and applied ranking techniques. We incrementally
increased the augmented dataset size (10 to 10,000) and conducted 5-fold
cross-validation, reporting F1 score and Mean Reciprocal Rank (MRR). Our result
show that fine-tuning and data augmentation improved performance over other
strategies. GPT-4 Turbo achieved the highest F1 (0.433), while Mistral7B with
data augmentation had the highest MRR (0.746). Open-source models, when
fine-tuned or augmented, outperformed closed-source models. Notably, the best
F1 and MRR scores did not always align. Few-shot prompting outperformed
zero-shot in vanilla models, and structured prompts yielded different
preferences across models. Fine-tuning improved zero-shot performance but
sometimes degraded few-shot performance. Data augmentation performed comparably
or better than other methods. Our evaluation highlights the effectiveness of
prompting, fine-tuning, and data augmentation in improving model performance
for medical jargon extraction in low-resource scenarios.
},
    year = {2025},
    month = {02},
    url = {http://arxiv.org/pdf/2502.16022v2},
    journal = {arXiv preprint}
}

@article{arxiv:Improving_Patient_Pre-screening_for_Clinical_Trials:_Assisting
__Physicians_with_Large_Language_Models,
    title = {Improving Patient Pre-screening for Clinical Trials: Assisting
  Physicians with Large Language Models},
    author = {Danny M. den Hamer, Perry Schoor, Tobias B. Polak, Daniel Kapitan},
    abstract = {  Physicians considering clinical trials for their patients are met with the
laborious process of checking many text based eligibility criteria. Large
Language Models (LLMs) have shown to perform well for clinical information
extraction and clinical reasoning, including medical tests, but not yet in
real-world scenarios. This paper investigates the use of InstructGPT to assist
physicians in determining eligibility for clinical trials based on a patient's
summarised medical profile. Using a prompting strategy combining one-shot,
selection-inference and chain-of-thought techniques, we investigate the
performance of LLMs on 10 synthetically created patient profiles. Performance
is evaluated at four levels: ability to identify screenable eligibility
criteria from a trial given a medical profile; ability to classify for each
individual criterion whether the patient qualifies; the overall classification
whether a patient is eligible for a clinical trial and the percentage of
criteria to be screened by physician. We evaluated against 146 clinical trials
and a total of 4,135 eligibility criteria. The LLM was able to correctly
identify the screenability of 72% (2,994/4,135) of the criteria. Additionally,
72% (341/471) of the screenable criteria were evaluated correctly. The
resulting trial level classification as eligible or ineligible resulted in a
recall of 0.5. By leveraging LLMs with a physician-in-the-loop, a recall of 1.0
and precision of 0.71 on clinical trial level can be achieved while reducing
the amount of criteria to be checked by an estimated 90%. LLMs can be used to
assist physicians with pre-screening of patients for clinical trials. By
forcing instruction-tuned LLMs to produce chain-of-thought responses, the
reasoning can be made transparent to and the decision process becomes amenable
by physicians, thereby making such a system feasible for use in real-world
scenarios.
},
    year = {2023},
    month = {04},
    url = {http://arxiv.org/pdf/2304.07396v2},
    journal = {arXiv preprint}
}

@article{arxiv:Zero-shot_and_Few-shot_Generation_Strategies_for_Artificial_Clinical
__Records,
    title = {Zero-shot and Few-shot Generation Strategies for Artificial Clinical
  Records},
    author = {Erlend Frayling, Jake Lever, Graham McDonald},
    abstract = {  The challenge of accessing historical patient data for clinical research,
while adhering to privacy regulations, is a significant obstacle in medical
science. An innovative approach to circumvent this issue involves utilising
synthetic medical records that mirror real patient data without compromising
individual privacy. The creation of these synthetic datasets, particularly
without using actual patient data to train Large Language Models (LLMs),
presents a novel solution as gaining access to sensitive patient information to
train models is also a challenge. This study assesses the capability of the
Llama 2 LLM to create synthetic medical records that accurately reflect real
patient information, employing zero-shot and few-shot prompting strategies for
comparison against fine-tuned methodologies that do require sensitive patient
data during training. We focus on generating synthetic narratives for the
History of Present Illness section, utilising data from the MIMIC-IV dataset
for comparison. In this work introduce a novel prompting technique that
leverages a chain-of-thought approach, enhancing the model's ability to
generate more accurate and contextually relevant medical narratives without
prior fine-tuning. Our findings suggest that this chain-of-thought prompted
approach allows the zero-shot model to achieve results on par with those of
fine-tuned models, based on Rouge metrics evaluation.
},
    year = {2024},
    month = {03},
    url = {http://arxiv.org/pdf/2403.08664v2},
    journal = {arXiv preprint}
}

@article{arxiv:Methods_to_Estimate_Large_Language_Model_Confidence,
    title = {Methods to Estimate Large Language Model Confidence},
    author = {Maia Kotelanski, Robert Gallo, Ashwin Nayak, Thomas Savage},
    abstract = {  Large Language Models have difficulty communicating uncertainty, which is a
significant obstacle to applying LLMs to complex medical tasks. This study
evaluates methods to measure LLM confidence when suggesting a diagnosis for
challenging clinical vignettes. GPT4 was asked a series of challenging case
questions using Chain of Thought and Self Consistency prompting. Multiple
methods were investigated to assess model confidence and evaluated on their
ability to predict the models observed accuracy. The methods evaluated were
Intrinsic Confidence, SC Agreement Frequency and CoT Response Length. SC
Agreement Frequency correlated with observed accuracy, yielding a higher Area
under the Receiver Operating Characteristic Curve compared to Intrinsic
Confidence and CoT Length analysis. SC agreement is the most useful proxy for
model confidence, especially for medical diagnosis. Model Intrinsic Confidence
and CoT Response Length exhibit a weaker ability to differentiate between
correct and incorrect answers, preventing them from being reliable and
interpretable markers for model confidence. We conclude GPT4 has a limited
ability to assess its own diagnostic accuracy. SC Agreement Frequency is the
most useful method to measure GPT4 confidence.
},
    year = {2023},
    month = {11},
    url = {http://arxiv.org/pdf/2312.03733v2},
    journal = {arXiv preprint}
}

@article{arxiv:Chain-of-Though_(CoT)_prompting_strategies_for_medical_error_detection
__and_correction,
    title = {Chain-of-Though (CoT) prompting strategies for medical error detection
  and correction},
    author = {Zhaolong Wu, Abul Hasan, Jinge Wu, Yunsoo Kim, Jason P. Y. Cheung, Teng Zhang, Honghan Wu},
    abstract = {  This paper describes our submission to the MEDIQA-CORR 2024 shared task for
automatically detecting and correcting medical errors in clinical notes. We
report results for three methods of few-shot In-Context Learning (ICL)
augmented with Chain-of-Thought (CoT) and reason prompts using a large language
model (LLM). In the first method, we manually analyse a subset of train and
validation dataset to infer three CoT prompts by examining error types in the
clinical notes. In the second method, we utilise the training dataset to prompt
the LLM to deduce reasons about their correctness or incorrectness. The
constructed CoTs and reasons are then augmented with ICL examples to solve the
tasks of error detection, span identification, and error correction. Finally,
we combine the two methods using a rule-based ensemble method. Across the three
sub-tasks, our ensemble method achieves a ranking of 3rd for both sub-task 1
and 2, while securing 7th place in sub-task 3 among all submissions.
},
    year = {2024},
    month = {06},
    url = {http://arxiv.org/pdf/2406.09103v1},
    journal = {arXiv preprint}
}

@article{arxiv:Large_Language_Models_in_Medical_Term_Classification_and_Unexpected
__Misalignment_Between_Response_and_Reasoning,
    title = {Large Language Models in Medical Term Classification and Unexpected
  Misalignment Between Response and Reasoning},
    author = {Xiaodan Zhang, Sandeep Vemulapalli, Nabasmita Talukdar, Sumyeong Ahn, Jiankun Wang, Han Meng, Sardar Mehtab Bin Murtaza, Aakash Ajay Dave, Dmitry Leshchiner, Dimitri F. Joseph, Martin Witteveen-Lane, Dave Chesla, Jiayu Zhou, Bin Chen},
    abstract = {  This study assesses the ability of state-of-the-art large language models
(LLMs) including GPT-3.5, GPT-4, Falcon, and LLaMA 2 to identify patients with
mild cognitive impairment (MCI) from discharge summaries and examines instances
where the models' responses were misaligned with their reasoning. Utilizing the
MIMIC-IV v2.2 database, we focused on a cohort aged 65 and older, verifying MCI
diagnoses against ICD codes and expert evaluations. The data was partitioned
into training, validation, and testing sets in a 7:2:1 ratio for model
fine-tuning and evaluation, with an additional metastatic cancer dataset from
MIMIC III used to further assess reasoning consistency. GPT-4 demonstrated
superior interpretative capabilities, particularly in response to complex
prompts, yet displayed notable response-reasoning inconsistencies. In contrast,
open-source models like Falcon and LLaMA 2 achieved high accuracy but lacked
explanatory reasoning, underscoring the necessity for further research to
optimize both performance and interpretability. The study emphasizes the
significance of prompt engineering and the need for further exploration into
the unexpected reasoning-response misalignment observed in GPT-4. The results
underscore the promise of incorporating LLMs into healthcare diagnostics,
contingent upon methodological advancements to ensure accuracy and clinical
coherence of AI-generated outputs, thereby improving the trustworthiness of
LLMs for medical decision-making.
},
    year = {2023},
    month = {12},
    url = {http://arxiv.org/pdf/2312.14184v1},
    journal = {arXiv preprint}
}

@article{arxiv:Autocompletion_of_Chief_Complaints_in_the_Electronic_Health_Records
__using_Large_Language_Models,
    title = {Autocompletion of Chief Complaints in the Electronic Health Records
  using Large Language Models},
    author = {K M Sajjadul Islam, Ayesha Siddika Nipu, Praveen Madiraju, Priya Deshpande},
    abstract = {  The Chief Complaint (CC) is a crucial component of a patient's medical record
as it describes the main reason or concern for seeking medical care. It
provides critical information for healthcare providers to make informed
decisions about patient care. However, documenting CCs can be time-consuming
for healthcare providers, especially in busy emergency departments. To address
this issue, an autocompletion tool that suggests accurate and well-formatted
phrases or sentences for clinical notes can be a valuable resource for triage
nurses. In this study, we utilized text generation techniques to develop
machine learning models using CC data. In our proposed work, we train a Long
Short-Term Memory (LSTM) model and fine-tune three different variants of
Biomedical Generative Pretrained Transformers (BioGPT), namely
microsoft/biogpt, microsoft/BioGPT-Large, and microsoft/BioGPT-Large-PubMedQA.
Additionally, we tune a prompt by incorporating exemplar CC sentences,
utilizing the OpenAI API of GPT-4. We evaluate the models' performance based on
the perplexity score, modified BERTScore, and cosine similarity score. The
results show that BioGPT-Large exhibits superior performance compared to the
other models. It consistently achieves a remarkably low perplexity score of
1.65 when generating CC, whereas the baseline LSTM model achieves the best
perplexity score of 170. Further, we evaluate and assess the proposed models'
performance and the outcome of GPT-4.0. Our study demonstrates that utilizing
LLMs such as BioGPT, leads to the development of an effective autocompletion
tool for generating CC documentation in healthcare settings.
},
    year = {2024},
    month = {01},
    url = {http://arxiv.org/pdf/2401.06088v1},
    journal = {arXiv preprint}
}

@article{arxiv:NeuroXVocal:_Detection_and_Explanation_of_Alzheimer's_Disease_through
__Non-invasive_Analysis_of_Picture-prompted_Speech,
    title = {NeuroXVocal: Detection and Explanation of Alzheimer's Disease through
  Non-invasive Analysis of Picture-prompted Speech},
    author = {Nikolaos Ntampakis, Konstantinos Diamantaras, Ioanna Chouvarda, Magda Tsolaki, Vasileios Argyriou, Panagiotis Sarigianndis},
    abstract = {  The early diagnosis of Alzheimer's Disease (AD) through non invasive methods
remains a significant healthcare challenge. We present NeuroXVocal, a novel
dual-component system that not only classifies but also explains potential AD
cases through speech analysis. The classification component (Neuro) processes
three distinct data streams: acoustic features capturing speech patterns and
voice characteristics, textual features extracted from speech transcriptions,
and precomputed embeddings representing linguistic patterns. These streams are
fused through a custom transformer-based architecture that enables robust
cross-modal interactions. The explainability component (XVocal) implements a
Retrieval-Augmented Generation (RAG) approach, leveraging Large Language Models
combined with a domain-specific knowledge base of AD research literature. This
architecture enables XVocal to retrieve relevant clinical studies and research
findings to generate evidence-based context-sensitive explanations of the
acoustic and linguistic markers identified in patient speech. Using the IS2021
ADReSSo Challenge benchmark dataset, our system achieved state-of-the-art
performance with 95.77% accuracy in AD classification, significantly
outperforming previous approaches. The explainability component was
qualitatively evaluated using a structured questionnaire completed by medical
professionals, validating its clinical relevance. NeuroXVocal's unique
combination of high-accuracy classification and interpretable,
literature-grounded explanations demonstrates its potential as a practical tool
for supporting clinical AD diagnosis.
},
    year = {2025},
    month = {02},
    url = {http://arxiv.org/pdf/2502.10108v1},
    journal = {arXiv preprint}
}

@article{arxiv:Knowledge-Infused_Prompting:_Assessing_and_Advancing_Clinical_Text_Data
__Generation_with_Large_Language_Models,
    title = {Knowledge-Infused Prompting: Assessing and Advancing Clinical Text Data
  Generation with Large Language Models},
    author = {Ran Xu, Hejie Cui, Yue Yu, Xuan Kan, Wenqi Shi, Yuchen Zhuang, Wei Jin, Joyce Ho, Carl Yang},
    abstract = {  Clinical natural language processing requires methods that can address
domain-specific challenges, such as complex medical terminology and clinical
contexts. Recently, large language models (LLMs) have shown promise in this
domain. Yet, their direct deployment can lead to privacy issues and are
constrained by resources. To address this challenge, we delve into synthetic
clinical text generation using LLMs for clinical NLP tasks. We propose an
innovative, resource-efficient approach, ClinGen, which infuses knowledge into
the process. Our model involves clinical knowledge extraction and
context-informed LLM prompting. Both clinical topics and writing styles are
drawn from external domain-specific knowledge graphs and LLMs to guide data
generation. Our extensive empirical study across 7 clinical NLP tasks and 16
datasets reveals that ClinGen consistently enhances performance across various
tasks, effectively aligning the distribution of real datasets and significantly
enriching the diversity of generated training instances. Our code is available
at \url{https://github.com/ritaranx/ClinGen}.
},
    year = {2023},
    month = {11},
    url = {http://arxiv.org/pdf/2311.00287v2},
    journal = {arXiv preprint}
}

@article{arxiv:RadAlign:_Advancing_Radiology_Report_Generation_with_Vision-Language
__Concept_Alignment,
    title = {RadAlign: Advancing Radiology Report Generation with Vision-Language
  Concept Alignment},
    author = {Difei Gu, Yunhe Gao, Yang Zhou, Mu Zhou, Dimitris Metaxas},
    abstract = {  Automated chest radiographs interpretation requires both accurate disease
classification and detailed radiology report generation, presenting a
significant challenge in the clinical workflow. Current approaches either focus
on classification accuracy at the expense of interpretability or generate
detailed but potentially unreliable reports through image captioning
techniques. In this study, we present RadAlign, a novel framework that combines
the predictive accuracy of vision-language models (VLMs) with the reasoning
capabilities of large language models (LLMs). Inspired by the radiologist's
workflow, RadAlign first employs a specialized VLM to align visual features
with key medical concepts, achieving superior disease classification with an
average AUC of 0.885 across multiple diseases. These recognized medical
conditions, represented as text-based concepts in the aligned visual-language
space, are then used to prompt LLM-based report generation. Enhanced by a
retrieval-augmented generation mechanism that grounds outputs in similar
historical cases, RadAlign delivers superior report quality with a GREEN score
of 0.678, outperforming state-of-the-art methods' 0.634. Our framework
maintains strong clinical interpretability while reducing hallucinations,
advancing automated medical imaging and report analysis through integrated
predictive and generative AI. Code is available at
https://github.com/difeigu/RadAlign.
},
    year = {2025},
    month = {01},
    url = {http://arxiv.org/pdf/2501.07525v1},
    journal = {arXiv preprint}
}

@article{arxiv:RareAgents:_Advancing_Rare_Disease_Care_through_LLM-Empowered
__Multi-disciplinary_Team,
    title = {RareAgents: Advancing Rare Disease Care through LLM-Empowered
  Multi-disciplinary Team},
    author = {Xuanzhong Chen, Ye Jin, Xiaohao Mao, Lun Wang, Shuyang Zhang, Ting Chen},
    abstract = {  Rare diseases, despite their low individual incidence, collectively impact
around 300 million people worldwide due to the vast number of diseases. The
involvement of multiple organs and systems, and the shortage of specialized
doctors with relevant experience make diagnosing and treating rare diseases
more challenging than common diseases. Recently, agents powered by large
language models (LLMs) have demonstrated notable applications across various
domains. In the medical field, some agent methods have outperformed direct
prompts in question-answering tasks from medical examinations. However, current
agent frameworks are not well-adapted to real-world clinical scenarios,
especially those involving the complex demands of rare diseases. To bridge this
gap, we introduce RareAgents, the first LLM-driven multi-disciplinary team
framework designed specifically for the complex clinical context of rare
diseases. RareAgents integrates advanced Multidisciplinary Team (MDT)
coordination, memory mechanisms, and medical tools utilization, leveraging
Llama-3.1-8B/70B as the base model. Experimental results show that RareAgents
outperforms state-of-the-art domain-specific models, GPT-4o, and current agent
frameworks in differential diagnosis and medication recommendation for rare
diseases. Furthermore, we contribute a novel rare disease dataset,
MIMIC-IV-Ext-Rare, to support further advancements in this field.
},
    year = {2024},
    month = {12},
    url = {http://arxiv.org/pdf/2412.12475v2},
    journal = {arXiv preprint}
}

@article{arxiv:Zebra-Llama:_A_Context-Aware_Large_Language_Model_for_Democratizing_Rare
__Disease_Knowledge,
    title = {Zebra-Llama: A Context-Aware Large Language Model for Democratizing Rare
  Disease Knowledge},
    author = {Karthik Soman, Andrew Langdon, Catalina Villouta, Chinmay Agrawal, Lashaw Salta, Braian Peetoom, Gianmarco Bellucci, Orion J Buske},
    abstract = {  Rare diseases present unique challenges in healthcare, often suffering from
delayed diagnosis and fragmented information landscapes. The scarcity of
reliable knowledge in these conditions poses a distinct challenge for Large
Language Models (LLMs) in supporting clinical management and delivering precise
patient information underscoring the need for focused training on these 'zebra'
cases. We present Zebra-Llama, a specialized context-aware language model with
high precision Retrieval Augmented Generation (RAG) capability, focusing on
Ehlers-Danlos Syndrome (EDS) as our case study. EDS, affecting 1 in 5,000
individuals, exemplifies the complexities of rare diseases with its diverse
symptoms, multiple subtypes, and evolving diagnostic criteria. By implementing
a novel context-aware fine-tuning methodology trained on questions derived from
medical literature, patient experiences, and clinical resources, along with
expertly curated responses, Zebra-Llama demonstrates unprecedented capabilities
in handling EDS-related queries. On a test set of real-world questions
collected from EDS patients and clinicians, medical experts evaluated the
responses generated by both models, revealing Zebra-Llama's substantial
improvements over base model (Llama 3.1-8B-Instruct) in thoroughness (77.5% vs.
70.1%), accuracy (83.0% vs. 78.8%), clarity (74.7% vs. 72.0%) and citation
reliability (70.6% vs. 52.3%). Released as an open-source resource, Zebra-Llama
not only provides more accessible and reliable EDS information but also
establishes a framework for developing specialized AI solutions for other rare
conditions. This work represents a crucial step towards democratizing
expert-level knowledge in rare disease management, potentially transforming how
healthcare providers and patients navigate the complex landscape of rare
diseases.
},
    year = {2024},
    month = {11},
    url = {http://arxiv.org/pdf/2411.02657v1},
    journal = {arXiv preprint}
}

@article{arxiv:MAGDA:_Multi-agent_guideline-driven_diagnostic_assistance,
    title = {MAGDA: Multi-agent guideline-driven diagnostic assistance},
    author = {David Bani-Harouni, Nassir Navab, Matthias Keicher},
    abstract = {  In emergency departments, rural hospitals, or clinics in less developed
regions, clinicians often lack fast image analysis by trained radiologists,
which can have a detrimental effect on patients' healthcare. Large Language
Models (LLMs) have the potential to alleviate some pressure from these
clinicians by providing insights that can help them in their decision-making.
While these LLMs achieve high test results on medical exams showcasing their
great theoretical medical knowledge, they tend not to follow medical
guidelines. In this work, we introduce a new approach for zero-shot
guideline-driven decision support. We model a system of multiple LLM agents
augmented with a contrastive vision-language model that collaborate to reach a
patient diagnosis. After providing the agents with simple diagnostic
guidelines, they will synthesize prompts and screen the image for findings
following these guidelines. Finally, they provide understandable
chain-of-thought reasoning for their diagnosis, which is then self-refined to
consider inter-dependencies between diseases. As our method is zero-shot, it is
adaptable to settings with rare diseases, where training data is limited, but
expert-crafted disease descriptions are available. We evaluate our method on
two chest X-ray datasets, CheXpert and ChestX-ray 14 Longtail, showcasing
performance improvement over existing zero-shot methods and generalizability to
rare diseases.
},
    year = {2024},
    month = {09},
    url = {http://arxiv.org/pdf/2409.06351v1},
    journal = {arXiv preprint}
}

@article{arxiv:Can_Large_Language_Models_Replace_Data_Scientists_in_Clinical_Research?,
    title = {Can Large Language Models Replace Data Scientists in Clinical Research?},
    author = {Zifeng Wang, Benjamin Danek, Ziwei Yang, Zheng Chen, Jimeng Sun},
    abstract = {  Data science plays a critical role in clinical research, but it requires
professionals with expertise in coding and medical data analysis. Large
language models (LLMs) have shown great potential in supporting medical tasks
and performing well in general coding tests. However, these tests do not assess
LLMs' ability to handle data science tasks in medicine, nor do they explore
their practical utility in clinical research. To address this, we developed a
dataset consisting of 293 real-world data science coding tasks, based on 39
published clinical studies, covering 128 tasks in Python and 165 tasks in R.
This dataset simulates realistic clinical research scenarios using patient
data. Our findings reveal that cutting-edge LLMs struggle to generate perfect
solutions, frequently failing to follow input instructions, understand target
data, and adhere to standard analysis practices. Consequently, LLMs are not yet
ready to fully automate data science tasks. We benchmarked advanced adaptation
methods and found two to be particularly effective: chain-of-thought prompting,
which provides a step-by-step plan for data analysis, which led to a 60%
improvement in code accuracy; and self-reflection, enabling LLMs to iteratively
refine their code, yielding a 38% accuracy improvement. Building on these
insights, we developed a platform that integrates LLMs into the data science
workflow for medical professionals. In a user study with five medical doctors,
we found that while LLMs cannot fully automate coding tasks, they significantly
streamline the programming process. We found that 80% of their submitted code
solutions were incorporated from LLM-generated code, with up to 96% reuse in
some cases. Our analysis highlights the potential of LLMs, when integrated into
expert workflows, to enhance data science efficiency in clinical research.
},
    year = {2024},
    month = {10},
    url = {http://arxiv.org/pdf/2410.21591v1},
    journal = {arXiv preprint}
}

@article{arxiv:Can_Large_Language_Models_Logically_Predict_Myocardial_Infarction?
__Evaluation_based_on_UK_Biobank_Cohort,
    title = {Can Large Language Models Logically Predict Myocardial Infarction?
  Evaluation based on UK Biobank Cohort},
    author = {Yuxing Zhi, Yuan Guo, Kai Yuan, Hesong Wang, Heng Xu, Haina Yao, Albert C Yang, Guangrui Huang, Yuping Duan},
    abstract = {  Background: Large language models (LLMs) have seen extraordinary advances
with applications in clinical decision support. However, high-quality evidence
is urgently needed on the potential and limitation of LLMs in providing
accurate clinical decisions based on real-world medical data. Objective: To
evaluate quantitatively whether universal state-of-the-art LLMs (ChatGPT and
GPT-4) can predict the incidence risk of myocardial infarction (MI) with
logical inference, and to further make comparison between various models to
assess the performance of LLMs comprehensively. Methods: In this retrospective
cohort study, 482,310 participants recruited from 2006 to 2010 were initially
included in UK Biobank database and later on resampled into a final cohort of
690 participants. For each participant, tabular data of the risk factors of MI
were transformed into standardized textual descriptions for ChatGPT
recognition. Responses were generated by asking ChatGPT to select a score
ranging from 0 to 10 representing the risk. Chain of Thought (CoT) questioning
was used to evaluate whether LLMs make prediction logically. The predictive
performance of ChatGPT was compared with published medical indices, traditional
machine learning models and other large language models. Conclusions: Current
LLMs are not ready to be applied in clinical medicine fields. Future medical
LLMs are suggested to be expert in medical domain knowledge to understand both
natural languages and quantified medical data, and further make logical
inferences.
},
    year = {2024},
    month = {09},
    url = {http://arxiv.org/pdf/2409.14478v1},
    journal = {arXiv preprint}
}

@article{arxiv:M4CXR:_Exploring_Multi-task_Potentials_of_Multi-modal_Large_Language
__Models_for_Chest_X-ray_Interpretation,
    title = {M4CXR: Exploring Multi-task Potentials of Multi-modal Large Language
  Models for Chest X-ray Interpretation},
    author = {Jonggwon Park, Soobum Kim, Byungmu Yoon, Jihun Hyun, Kyoyun Choi},
    abstract = {  The rapid evolution of artificial intelligence, especially in large language
models (LLMs), has significantly impacted various domains, including
healthcare. In chest X-ray (CXR) analysis, previous studies have employed LLMs,
but with limitations: either underutilizing the multi-tasking capabilities of
LLMs or lacking clinical accuracy. This paper presents M4CXR, a multi-modal LLM
designed to enhance CXR interpretation. The model is trained on a visual
instruction-following dataset that integrates various task-specific datasets in
a conversational format. As a result, the model supports multiple tasks such as
medical report generation (MRG), visual grounding, and visual question
answering (VQA). M4CXR achieves state-of-the-art clinical accuracy in MRG by
employing a chain-of-thought prompting strategy, in which it identifies
findings in CXR images and subsequently generates corresponding reports. The
model is adaptable to various MRG scenarios depending on the available inputs,
such as single-image, multi-image, and multi-study contexts. In addition to
MRG, M4CXR performs visual grounding at a level comparable to specialized
models and also demonstrates outstanding performance in VQA. Both quantitative
and qualitative assessments reveal M4CXR's versatility in MRG, visual
grounding, and VQA, while consistently maintaining clinical accuracy.
},
    year = {2024},
    month = {08},
    url = {http://arxiv.org/pdf/2408.16213v1},
    journal = {arXiv preprint}
}

@article{arxiv:Bias_patterns_in_the_application_of_LLMs_for_clinical_decision_support:
__A_comprehensive_study,
    title = {Bias patterns in the application of LLMs for clinical decision support:
  A comprehensive study},
    author = {Raphael Poulain, Hamed Fayyaz, Rahmatollah Beheshti},
    abstract = {  Large Language Models (LLMs) have emerged as powerful candidates to inform
clinical decision-making processes. While these models play an increasingly
prominent role in shaping the digital landscape, two growing concerns emerge in
healthcare applications: 1) to what extent do LLMs exhibit social bias based on
patients' protected attributes (like race), and 2) how do design choices (like
architecture design and prompting strategies) influence the observed biases? To
answer these questions rigorously, we evaluated eight popular LLMs across three
question-answering (QA) datasets using clinical vignettes (patient
descriptions) standardized for bias evaluations. We employ red-teaming
strategies to analyze how demographics affect LLM outputs, comparing both
general-purpose and clinically-trained models. Our extensive experiments reveal
various disparities (some significant) across protected groups. We also observe
several counter-intuitive patterns such as larger models not being necessarily
less biased and fined-tuned models on medical data not being necessarily better
than the general-purpose models. Furthermore, our study demonstrates the impact
of prompt design on bias patterns and shows that specific phrasing can
influence bias patterns and reflection-type approaches (like Chain of Thought)
can reduce biased outcomes effectively. Consistent with prior studies, we call
on additional evaluations, scrutiny, and enhancement of LLMs used in clinical
decision support applications.
},
    year = {2024},
    month = {04},
    url = {http://arxiv.org/pdf/2404.15149v1},
    journal = {arXiv preprint}
}

@article{arxiv:JingFang:_A_Traditional_Chinese_Medicine_Large_Language_Model_of
__Expert-Level_Medical_Diagnosis_and_Syndrome_Differentiation-Based_Treatment,
    title = {JingFang: A Traditional Chinese Medicine Large Language Model of
  Expert-Level Medical Diagnosis and Syndrome Differentiation-Based Treatment},
    author = {Yehan Yan, Tianhao Ma, Ruotai Li, Xinhan Zheng, Guodong Shan, Chisheng Li},
    abstract = {  Traditional Chinese medicine (TCM) plays a vital role in health protection
and disease treatment, but its practical application requires extensive medical
knowledge and clinical experience. Existing TCM Large Language Models (LLMs)
exhibit critical limitations of uncomprehensive medical consultation and
diagnoses, and inaccurate syndrome differentiation-based treatment. To address
these issues, this study establishes JingFang (JF): a novel TCM Large Language
Model that demonstrates the expert-level capability of medical diagnosis and
syndrome differentiation-based treatment. We innovate a Multi-agent Dynamic
Collaborative Chain-of-Thought Mechanism (MDCCTM) for medical consultation,
enabling JF with effective and accurate diagnostic ability. In addition, a
Syndrome Agent and a Dual-Stage Retrieval Scheme (DSRS) are developed to
significantly enhance the capacity of JF for disease treatment based on
syndrome differentiation. JingFang not only facilitates the application of LLMs
but also promotes the effective practice of TCM in human health protection and
disease treatment.
},
    year = {2025},
    month = {02},
    url = {http://arxiv.org/pdf/2502.04345v1},
    journal = {arXiv preprint}
}

@article{arxiv:SM3-Text-to-Query:_Synthetic_Multi-Model_Medical_Text-to-Query_Benchmark,
    title = {SM3-Text-to-Query: Synthetic Multi-Model Medical Text-to-Query Benchmark},
    author = {Sithursan Sivasubramaniam, Cedric Osei-Akoto, Yi Zhang, Kurt Stockinger, Jonathan Fuerst},
    abstract = {  Electronic health records (EHRs) are stored in various database systems with
different database models on heterogeneous storage architectures, such as
relational databases, document stores, or graph databases. These different
database models have a big impact on query complexity and performance. While
this has been a known fact in database research, its implications for the
growing number of Text-to-Query systems have surprisingly not been investigated
so far. In this paper, we present SM3-Text-to-Query, the first multi-model
medical Text-to-Query benchmark based on synthetic patient data from Synthea,
following the SNOMED-CT taxonomy -- a widely used knowledge graph ontology
covering medical terminology. SM3-Text-to-Query provides data representations
for relational databases (PostgreSQL), document stores (MongoDB), and graph
databases (Neo4j and GraphDB (RDF)), allowing the evaluation across four
popular query languages, namely SQL, MQL, Cypher, and SPARQL. We systematically
and manually develop 408 template questions, which we augment to construct a
benchmark of 10K diverse natural language question/query pairs for these four
query languages (40K pairs overall). On our dataset, we evaluate several common
in-context-learning (ICL) approaches for a set of representative closed and
open-source LLMs. Our evaluation sheds light on the trade-offs between database
models and query languages for different ICL strategies and LLMs. Last,
SM3-Text-to-Query is easily extendable to additional query languages or real,
standard-based patient databases.
},
    year = {2024},
    month = {11},
    url = {http://arxiv.org/pdf/2411.05521v2},
    journal = {arXiv preprint}
}

@article{arxiv:A_Comprehensive_Survey_of_Foundation_Models_in_Medicine,
    title = {A Comprehensive Survey of Foundation Models in Medicine},
    author = {Wasif Khan, Seowung Leem, Kyle B. See, Joshua K. Wong, Shaoting Zhang, Ruogu Fang},
    abstract = {  Foundation models (FMs) are large-scale deep learning models trained on
massive datasets, often using self-supervised learning techniques. These models
serve as a versatile base for a wide range of downstream tasks, including those
in medicine and healthcare. FMs have demonstrated remarkable success across
multiple healthcare domains. However, existing surveys in this field do not
comprehensively cover all areas where FMs have made significant strides. In
this survey, we present a comprehensive review of FMs in medicine, focusing on
their evolution, learning strategies, flagship models, applications, and
associated challenges. We examine how prominent FMs, such as the BERT and GPT
families, are transforming various aspects of healthcare, including clinical
large language models, medical image analysis, and omics research.
Additionally, we provide a detailed taxonomy of FM-enabled healthcare
applications, spanning clinical natural language processing, medical computer
vision, graph learning, and other biology- and omics- related tasks. Despite
the transformative potentials of FMs, they also pose unique challenges. This
survey delves into these challenges and highlights open research questions and
lessons learned to guide researchers and practitioners. Our goal is to provide
valuable insights into the capabilities of FMs in health, facilitating
responsible deployment and mitigating associated risks.
},
    year = {2024},
    month = {06},
    url = {http://arxiv.org/pdf/2406.10729v3},
    journal = {arXiv preprint}
}

@article{arxiv:Eir:_Thai_Medical_Large_Language_Models,
    title = {Eir: Thai Medical Large Language Models},
    author = {Yutthakorn Thiprak, Rungtam Ngodngamthaweesuk, Songtam Ngodngamtaweesuk},
    abstract = {  We present Eir-8B, a large language model with 8 billion parameters,
specifically designed to enhance the accuracy of handling medical tasks in the
Thai language. This model focuses on providing clear and easy-to-understand
answers for both healthcare professionals and patients, thereby improving the
efficiency of diagnosis and treatment processes. Human evaluation was conducted
to ensure that the model adheres to care standards and provides unbiased
answers.
  To prioritize data security, the model is deployed within the hospital's
internal network, ensuring both high security and faster processing speeds. The
internal API connection is secured with encryption and strict authentication
measures to prevent data leaks and unauthorized access.
  We evaluated several open-source large language models with 8 billion
parameters on four medical benchmarks: MedQA, MedMCQA, PubMedQA, and the
medical subset of MMLU. The best-performing baselines were used to develop
Eir-8B. Our evaluation employed multiple questioning strategies, including
zero-shot, few-shot, chain-of-thought reasoning, and ensemble/self-consistency
voting methods. Our model outperformed commercially available Thai-language
large language models by more than 10%. In addition, we developed enhanced
model testing tailored for clinical use in Thai across 18 clinical tasks, where
our model exceeded GPT-4o performance by more than 11%.
},
    year = {2024},
    month = {09},
    url = {http://arxiv.org/pdf/2409.08523v2},
    journal = {arXiv preprint}
}

@article{arxiv:Prompting_Whole_Slide_Image_Based_Genetic_Biomarker_Prediction,
    title = {Prompting Whole Slide Image Based Genetic Biomarker Prediction},
    author = {Ling Zhang, Boxiang Yun, Xingran Xie, Qingli Li, Xinxing Li, Yan Wang},
    abstract = {  Prediction of genetic biomarkers, e.g., microsatellite instability and BRAF
in colorectal cancer is crucial for clinical decision making. In this paper, we
propose a whole slide image (WSI) based genetic biomarker prediction method via
prompting techniques. Our work aims at addressing the following challenges: (1)
extracting foreground instances related to genetic biomarkers from gigapixel
WSIs, and (2) the interaction among the fine-grained pathological components in
WSIs.Specifically, we leverage large language models to generate medical
prompts that serve as prior knowledge in extracting instances associated with
genetic biomarkers. We adopt a coarse-to-fine approach to mine biomarker
information within the tumor microenvironment. This involves extracting
instances related to genetic biomarkers using coarse medical prior knowledge,
grouping pathology instances into fine-grained pathological components and
mining their interactions. Experimental results on two colorectal cancer
datasets show the superiority of our method, achieving 91.49% in AUC for MSI
classification. The analysis further shows the clinical interpretability of our
method. Code is publicly available at
https://github.com/DeepMed-Lab-ECNU/PromptBio.
},
    year = {2024},
    month = {06},
    url = {http://arxiv.org/pdf/2407.09540v1},
    journal = {arXiv preprint}
}

@article{arxiv:Open_Foundation_Models_in_Healthcare:_Challenges,_Paradoxes,_and
__Opportunities_with_GenAI_Driven_Personalized_Prescription,
    title = {Open Foundation Models in Healthcare: Challenges, Paradoxes, and
  Opportunities with GenAI Driven Personalized Prescription},
    author = {Mahdi Alkaeed, Sofiat Abioye, Adnan Qayyum, Yosra Magdi Mekki, Ilhem Berrou, Mohamad Abdallah, Ala Al-Fuqaha, Muhammad Bilal, Junaid Qadir},
    abstract = {  In response to the success of proprietary Large Language Models (LLMs) such
as OpenAI's GPT-4, there is a growing interest in developing open,
non-proprietary LLMs and AI foundation models (AIFMs) for transparent use in
academic, scientific, and non-commercial applications. Despite their inability
to match the refined functionalities of their proprietary counterparts, open
models hold immense potential to revolutionize healthcare applications. In this
paper, we examine the prospects of open-source LLMs and AIFMs for developing
healthcare applications and make two key contributions. Firstly, we present a
comprehensive survey of the current state-of-the-art open-source healthcare
LLMs and AIFMs and introduce a taxonomy of these open AIFMs, categorizing their
utility across various healthcare tasks. Secondly, to evaluate the
general-purpose applications of open LLMs in healthcare, we present a case
study on personalized prescriptions. This task is particularly significant due
to its critical role in delivering tailored, patient-specific medications that
can greatly improve treatment outcomes. In addition, we compare the performance
of open-source models with proprietary models in settings with and without
Retrieval-Augmented Generation (RAG). Our findings suggest that, although less
refined, open LLMs can achieve performance comparable to proprietary models
when paired with grounding techniques such as RAG. Furthermore, to highlight
the clinical significance of LLMs-empowered personalized prescriptions, we
perform subjective assessment through an expert clinician. We also elaborate on
ethical considerations and potential risks associated with the misuse of
powerful LLMs and AIFMs, highlighting the need for a cautious and responsible
implementation in healthcare.
},
    year = {2025},
    month = {02},
    url = {http://arxiv.org/pdf/2502.04356v1},
    journal = {arXiv preprint}
}

@article{arxiv:Stochastic_Parrots_or_ICU_Experts?_Large_Language_Models_in_Critical
__Care_Medicine:_A_Scoping_Review,
    title = {Stochastic Parrots or ICU Experts? Large Language Models in Critical
  Care Medicine: A Scoping Review},
    author = {Tongyue Shi, Jun Ma, Zihan Yu, Haowei Xu, Minqi Xiong, Meirong Xiao, Yilin Li, Huiying Zhao, Guilan Kong},
    abstract = {  With the rapid development of artificial intelligence (AI), large language
models (LLMs) have shown strong capabilities in natural language understanding,
reasoning, and generation, attracting amounts of research interest in applying
LLMs to health and medicine. Critical care medicine (CCM) provides diagnosis
and treatment for critically ill patients who often require intensive
monitoring and interventions in intensive care units (ICUs). Can LLMs be
applied to CCM? Are LLMs just like stochastic parrots or ICU experts in
assisting clinical decision-making? This scoping review aims to provide a
panoramic portrait of the application of LLMs in CCM. Literature in seven
databases, including PubMed, Embase, Scopus, Web of Science, CINAHL, IEEE
Xplore, and ACM Digital Library, were searched from January 1, 2019, to June
10, 2024. Peer-reviewed journal and conference articles that discussed the
application of LLMs in critical care settings were included. From an initial
619 articles, 24 were selected for final review. This review grouped
applications of LLMs in CCM into three categories: clinical decision support,
medical documentation and reporting, and medical education and doctor-patient
communication. LLMs have advantages in handling unstructured data and do not
require manual feature engineering. Meanwhile, applying LLMs to CCM faces
challenges, including hallucinations, poor interpretability, bias and alignment
challenges, and privacy and ethics issues. Future research should enhance model
reliability and interpretability, integrate up-to-date medical knowledge, and
strengthen privacy and ethical guidelines. As LLMs evolve, they could become
key tools in CCM to help improve patient outcomes and optimize healthcare
delivery. This study is the first review of LLMs in CCM, aiding researchers,
clinicians, and policymakers to understand the current status and future
potentials of LLMs in CCM.
},
    year = {2024},
    month = {07},
    url = {http://arxiv.org/pdf/2407.19256v1},
    journal = {arXiv preprint}
}

@article{arxiv:Language_Models_sounds_the_Death_Knell_of_Knowledge_Graphs,
    title = {Language Models sounds the Death Knell of Knowledge Graphs},
    author = {Kunal Suri, Atul Singh, Prakhar Mishra, Swapna Sourav Rout, Rajesh Sabapathy},
    abstract = {  Healthcare domain generates a lot of unstructured and semi-structured text.
Natural Language processing (NLP) has been used extensively to process this
data. Deep Learning based NLP especially Large Language Models (LLMs) such as
BERT have found broad acceptance and are used extensively for many
applications. A Language Model is a probability distribution over a word
sequence. Self-supervised Learning on a large corpus of data automatically
generates deep learning-based language models. BioBERT and Med-BERT are
language models pre-trained for the healthcare domain. Healthcare uses typical
NLP tasks such as question answering, information extraction, named entity
recognition, and search to simplify and improve processes. However, to ensure
robust application of the results, NLP practitioners need to normalize and
standardize them. One of the main ways of achieving normalization and
standardization is the use of Knowledge Graphs. A Knowledge Graph captures
concepts and their relationships for a specific domain, but their creation is
time-consuming and requires manual intervention from domain experts, which can
prove expensive. SNOMED CT (Systematized Nomenclature of Medicine -- Clinical
Terms), Unified Medical Language System (UMLS), and Gene Ontology (GO) are
popular ontologies from the healthcare domain. SNOMED CT and UMLS capture
concepts such as disease, symptoms and diagnosis and GO is the world's largest
source of information on the functions of genes. Healthcare has been dealing
with an explosion in information about different types of drugs, diseases, and
procedures. This paper argues that using Knowledge Graphs is not the best
solution for solving problems in this domain. We present experiments using LLMs
for the healthcare domain to demonstrate that language models provide the same
functionality as knowledge graphs, thereby making knowledge graphs redundant.
},
    year = {2023},
    month = {01},
    url = {http://arxiv.org/pdf/2301.03980v1},
    journal = {arXiv preprint}
}

@article{arxiv:Improving_Medical_Reasoning_through_Retrieval_and_Self-Reflection_with
__Retrieval-Augmented_Large_Language_Models,
    title = {Improving Medical Reasoning through Retrieval and Self-Reflection with
  Retrieval-Augmented Large Language Models},
    author = {Minbyul Jeong, Jiwoong Sohn, Mujeen Sung, Jaewoo Kang},
    abstract = {  Recent proprietary large language models (LLMs), such as GPT-4, have achieved
a milestone in tackling diverse challenges in the biomedical domain, ranging
from multiple-choice questions to long-form generations. To address challenges
that still cannot be handled with the encoded knowledge of LLMs, various
retrieval-augmented generation (RAG) methods have been developed by searching
documents from the knowledge corpus and appending them unconditionally or
selectively to the input of LLMs for generation. However, when applying
existing methods to different domain-specific problems, poor generalization
becomes apparent, leading to fetching incorrect documents or making inaccurate
judgments. In this paper, we introduce Self-BioRAG, a framework reliable for
biomedical text that specializes in generating explanations, retrieving
domain-specific documents, and self-reflecting generated responses. We utilize
84k filtered biomedical instruction sets to train Self-BioRAG that can assess
its generated explanations with customized reflective tokens. Our work proves
that domain-specific components, such as a retriever, domain-related document
corpus, and instruction sets are necessary for adhering to domain-related
instructions. Using three major medical question-answering benchmark datasets,
experimental results of Self-BioRAG demonstrate significant performance gains
by achieving a 7.2% absolute improvement on average over the state-of-the-art
open-foundation model with a parameter size of 7B or less. Overall, we analyze
that Self-BioRAG finds the clues in the question, retrieves relevant documents
if needed, and understands how to answer with information from retrieved
documents and encoded knowledge as a medical expert does. We release our data
and code for training our framework components and model weights (7B and 13B)
to enhance capabilities in biomedical and clinical domains.
},
    year = {2024},
    month = {01},
    url = {http://arxiv.org/pdf/2401.15269v3},
    journal = {arXiv preprint}
}

@article{arxiv:Probabilistic_Medical_Predictions_of_Large_Language_Models,
    title = {Probabilistic Medical Predictions of Large Language Models},
    author = {Bowen Gu, Rishi J. Desai, Kueiyu Joshua Lin, Jie Yang},
    abstract = {  Large Language Models (LLMs) have shown promise in clinical applications
through prompt engineering, allowing flexible clinical predictions. However,
they struggle to produce reliable prediction probabilities, which are crucial
for transparency and decision-making. While explicit prompts can lead LLMs to
generate probability estimates, their numerical reasoning limitations raise
concerns about reliability. We compared explicit probabilities from text
generation to implicit probabilities derived from the likelihood of predicting
the correct label token. Across six advanced open-source LLMs and five medical
datasets, explicit probabilities consistently underperformed implicit
probabilities in discrimination, precision, and recall. This discrepancy is
more pronounced with smaller LLMs and imbalanced datasets, highlighting the
need for cautious interpretation, improved probability estimation methods, and
further research for clinical use of LLMs.
},
    year = {2024},
    month = {08},
    url = {http://arxiv.org/pdf/2408.11316v2},
    journal = {arXiv preprint}
}

@article{arxiv:Practical_Design_and_Benchmarking_of_Generative_AI_Applications_for
__Surgical_Billing_and_Coding,
    title = {Practical Design and Benchmarking of Generative AI Applications for
  Surgical Billing and Coding},
    author = {John C. Rollman, Bruce Rogers, Hamed Zaribafzadeh, Daniel Buckland, Ursula Rogers, Jennifer Gagnon, Ozanan Meireles, Lindsay Jennings, Jim Bennett, Jennifer Nicholson, Nandan Lad, Linda Cendales, Andreas Seas, Alessandro Martinino, E. Shelley Hwang, Allan D. Kirk},
    abstract = {  Background: Healthcare has many manual processes that can benefit from
automation and augmentation with Generative Artificial Intelligence (AI), the
medical billing and coding process. However, current foundational Large
Language Models (LLMs) perform poorly when tasked with generating accurate
International Classification of Diseases, 10th edition, Clinical Modification
(ICD-10-CM) and Current Procedural Terminology (CPT) codes. Additionally, there
are many security and financial challenges in the application of generative AI
to healthcare. We present a strategy for developing generative AI tools in
healthcare, specifically for medical billing and coding, that balances
accuracy, accessibility, and patient privacy.
  Methods: We fine tune the PHI-3 Mini and PHI-3 Medium LLMs using
institutional data and compare the results against the PHI-3 base model, a
PHI-3 RAG application, and GPT-4o. We use the post operative surgical report as
input and the patients billing claim the associated ICD-10, CPT, and Modifier
codes as the target result. Performance is measured by accuracy of code
generation, proportion of invalid codes, and the fidelity of the billing claim
format.
  Results: Both fine-tuned models performed better or as well as GPT-4o. The
Phi-3 Medium fine-tuned model showed the best performance (ICD-10 Recall and
Precision: 72%, 72%; CPT Recall and Precision: 77%, 79%; Modifier Recall and
Precision: 63%, 64%). The Phi-3 Medium fine-tuned model only fabricated 1% of
ICD-10 codes and 0.6% of CPT codes generated.
  Conclusions: Our study shows that a small model that is fine-tuned on
domain-specific data for specific tasks using a simple set of open-source tools
and minimal technological and monetary requirements performs as well as the
larger contemporary consumer models.
},
    year = {2025},
    month = {01},
    url = {http://arxiv.org/pdf/2501.05479v1},
    journal = {arXiv preprint}
}

@article{arxiv:Clinical_Trials_Ontology_Engineering_with_Large_Language_Models,
    title = {Clinical Trials Ontology Engineering with Large Language Models},
    author = {Berkan √áakƒ±r},
    abstract = {  Managing clinical trial information is currently a significant challenge for
the medical industry, as traditional methods are both time-consuming and
costly. This paper proposes a simple yet effective methodology to extract and
integrate clinical trial data in a cost-effective and time-efficient manner.
Allowing the medical industry to stay up-to-date with medical developments.
Comparing time, cost, and quality of the ontologies created by humans, GPT3.5,
GPT4, and Llama3 (8b & 70b). Findings suggest that large language models (LLM)
are a viable option to automate this process both from a cost and time
perspective. This study underscores significant implications for medical
research where real-time data integration from clinical trials could become the
norm.
},
    year = {2024},
    month = {12},
    url = {http://arxiv.org/pdf/2412.14387v1},
    journal = {arXiv preprint}
}

@article{arxiv:Edinburgh_Clinical_NLP_at_MEDIQA-CORR_2024:_Guiding_Large_Language
__Models_with_Hints,
    title = {Edinburgh Clinical NLP at MEDIQA-CORR 2024: Guiding Large Language
  Models with Hints},
    author = {Aryo Pradipta Gema, Chaeeun Lee, Pasquale Minervini, Luke Daines, T. Ian Simpson, Beatrice Alex},
    abstract = {  The MEDIQA-CORR 2024 shared task aims to assess the ability of Large Language
Models (LLMs) to identify and correct medical errors in clinical notes. In this
study, we evaluate the capability of general LLMs, specifically GPT-3.5 and
GPT-4, to identify and correct medical errors with multiple prompting
strategies. Recognising the limitation of LLMs in generating accurate
corrections only via prompting strategies, we propose incorporating error-span
predictions from a smaller, fine-tuned model in two ways: 1) by presenting it
as a hint in the prompt and 2) by framing it as multiple-choice questions from
which the LLM can choose the best correction. We found that our proposed
prompting strategies significantly improve the LLM's ability to generate
corrections. Our best-performing solution with 8-shot + CoT + hints ranked
sixth in the shared task leaderboard. Additionally, our comprehensive analyses
show the impact of the location of the error sentence, the prompted role, and
the position of the multiple-choice option on the accuracy of the LLM. This
prompts further questions about the readiness of LLM to be implemented in
real-world clinical settings.
},
    year = {2024},
    month = {05},
    url = {http://arxiv.org/pdf/2405.18028v1},
    journal = {arXiv preprint}
}

@article{arxiv:SemioLLM:_Assessing_Large_Language_Models_for_Semiological_Analysis_in
__Epilepsy_Research,
    title = {SemioLLM: Assessing Large Language Models for Semiological Analysis in
  Epilepsy Research},
    author = {Meghal Dani, Muthu Jeyanthi Prakash, Zeynep Akata, Stefanie Liebe},
    abstract = {  Large Language Models have shown promising results in their ability to encode
general medical knowledge in standard medical question-answering datasets.
However, their potential application in clinical practice requires evaluation
in domain-specific tasks, where benchmarks are largely missing. In this study
semioLLM, we test the ability of state-of-the-art LLMs (GPT-3.5, GPT-4, Mixtral
8x7B, and Qwen-72chat) to leverage their internal knowledge and reasoning for
epilepsy diagnosis. Specifically, we obtain likelihood estimates linking
unstructured text descriptions of seizures to seizure-generating brain regions,
using an annotated clinical database containing 1269 entries. We evaluate the
LLM's performance, confidence, reasoning, and citation abilities in comparison
to clinical evaluation. Models achieve above-chance classification performance
with prompt engineering significantly improving their outcome, with some models
achieving close-to-clinical performance and reasoning. However, our analyses
also reveal significant pitfalls with several models being overly confident
while showing poor performance, as well as exhibiting citation errors and
hallucinations. In summary, our work provides the first extensive benchmark
comparing current SOTA LLMs in the medical domain of epilepsy and highlights
their ability to leverage unstructured texts from patients' medical history to
aid diagnostic processes in health care.
},
    year = {2024},
    month = {07},
    url = {http://arxiv.org/pdf/2407.03004v1},
    journal = {arXiv preprint}
}

@article{arxiv:Retrieval_Augmented_Thought_Process_for_Private_Data_Handling_in
__Healthcare,
    title = {Retrieval Augmented Thought Process for Private Data Handling in
  Healthcare},
    author = {Thomas Pouplin, Hao Sun, Samuel Holt, Mihaela van der Schaar},
    abstract = {  Large Language Models (LLMs) have demonstrated the strong potential to assist
both clinicians and the general public with their extensive medical knowledge.
However, their application in healthcare is constrained due to concerns about
the privacy of data used in training, which prevents the integration of private
and personal information because of security and ethical issues. Moreover, if
their capabilities can be enhanced with information retrieval to access
up-to-date knowledge, the current integration of LLMs with Information
retrieval lacks robustness to imperfect retrieval, which can hinder their
effectiveness and even reduce overall performance. In this work, we address
this challenge by introducing the Retrieval-Augmented Thought Process (RATP).
Given access to external knowledge, RATP formulates the thought generation of
LLMs as a multiple-step decision process. To optimise such a thought process,
RATP leverages Monte-Carlo Tree Search and learns a proxy reward function that
permits cost-efficient inference. On a private dataset of electronic medical
records, deliberately excluded from any LLM training set, RATP achieves 35%
additional accuracy compared to in-context retrieval-augmented generation for
the question-answering task.
},
    year = {2024},
    month = {02},
    url = {http://arxiv.org/pdf/2402.07812v2},
    journal = {arXiv preprint}
}

@article{arxiv:MedKP:_Medical_Dialogue_with_Knowledge_Enhancement_and_Clinical_Pathway
__Encoding,
    title = {MedKP: Medical Dialogue with Knowledge Enhancement and Clinical Pathway
  Encoding},
    author = {Jiageng Wu, Xian Wu, Yefeng Zheng, Jie Yang},
    abstract = {  With appropriate data selection and training techniques, Large Language
Models (LLMs) have demonstrated exceptional success in various medical
examinations and multiple-choice questions. However, the application of LLMs in
medical dialogue generation-a task more closely aligned with actual medical
practice-has been less explored. This gap is attributed to the insufficient
medical knowledge of LLMs, which leads to inaccuracies and hallucinated
information in the generated medical responses. In this work, we introduce the
Medical dialogue with Knowledge enhancement and clinical Pathway encoding
(MedKP) framework, which integrates an external knowledge enhancement module
through a medical knowledge graph and an internal clinical pathway encoding via
medical entities and physician actions. Evaluated with comprehensive metrics,
our experiments on two large-scale, real-world online medical consultation
datasets (MedDG and KaMed) demonstrate that MedKP surpasses multiple baselines
and mitigates the incidence of hallucinations, achieving a new
state-of-the-art. Extensive ablation studies further reveal the effectiveness
of each component of MedKP. This enhancement advances the development of
reliable, automated medical consultation responses using LLMs, thereby
broadening the potential accessibility of precise and real-time medical
assistance.
},
    year = {2024},
    month = {03},
    url = {http://arxiv.org/pdf/2403.06611v1},
    journal = {arXiv preprint}
}

@article{arxiv:Towards_Interpretable_Radiology_Report_Generation_via_Concept
__Bottlenecks_using_a_Multi-Agentic_RAG,
    title = {Towards Interpretable Radiology Report Generation via Concept
  Bottlenecks using a Multi-Agentic RAG},
    author = {Hasan Md Tusfiqur Alam, Devansh Srivastav, Md Abdul Kadir, Daniel Sonntag},
    abstract = {  Deep learning has advanced medical image classification, but interpretability
challenges hinder its clinical adoption. This study enhances interpretability
in Chest X-ray (CXR) classification by using concept bottleneck models (CBMs)
and a multi-agent Retrieval-Augmented Generation (RAG) system for report
generation. By modeling relationships between visual features and clinical
concepts, we create interpretable concept vectors that guide a multi-agent RAG
system to generate radiology reports, enhancing clinical relevance,
explainability, and transparency. Evaluation of the generated reports using an
LLM-as-a-judge confirmed the interpretability and clinical utility of our
model's outputs. On the COVID-QU dataset, our model achieved 81% classification
accuracy and demonstrated robust report generation performance, with five key
metrics ranging between 84% and 90%. This interpretable multi-agent framework
bridges the gap between high-performance AI and the explainability required for
reliable AI-driven CXR analysis in clinical settings. Our code is available at
https://github.com/tifat58/IRR-with-CBM-RAG.git.
},
    year = {2024},
    month = {12},
    url = {http://arxiv.org/pdf/2412.16086v2},
    journal = {arXiv preprint}
}

@article{arxiv:Using_LLMs_to_label_medical_papers_according_to_the_CIViC_evidence_model,
    title = {Using LLMs to label medical papers according to the CIViC evidence model},
    author = {Markus Hisch, Xing David Wang},
    abstract = {  We introduce the sequence classification problem CIViC Evidence to the field
of medical NLP. CIViC Evidence denotes the multi-label classification problem
of assigning labels of clinical evidence to abstracts of scientific papers
which have examined various combinations of genomic variants, cancer types, and
treatment approaches. We approach CIViC Evidence using different language
models: We fine-tune pretrained checkpoints of BERT and RoBERTa on the CIViC
Evidence dataset and challenge their performance with models of the same
architecture which have been pretrained on domain-specific text. In this
context, we find that BiomedBERT and BioLinkBERT can outperform BERT on CIViC
Evidence (+0.8% and +0.9% absolute improvement in class-support weighted F1
score). All transformer-based models show a clear performance edge when
compared to a logistic regression trained on bigram tf-idf scores (+1.5 - 2.7%
improved F1 score). We compare the aforementioned BERT-like models to OpenAI's
GPT-4 in a few-shot setting (on a small subset of our original test dataset),
demonstrating that, without additional prompt-engineering or fine-tuning, GPT-4
performs worse on CIViC Evidence than our six fine-tuned models (66.1% weighted
F1 score compared to 71.8% for the best fine-tuned model). However, performance
gets reasonably close to the benchmark of a logistic regression model trained
on bigram tf-idf scores (67.7% weighted F1 score).
},
    year = {2024},
    month = {07},
    url = {http://arxiv.org/pdf/2407.04466v1},
    journal = {arXiv preprint}
}

@article{arxiv:Few_shot_chain-of-thought_driven_reasoning_to_prompt_LLMs_for_open_ended
__medical_question_answering,
    title = {Few shot chain-of-thought driven reasoning to prompt LLMs for open ended
  medical question answering},
    author = {Saeel Sandeep Nachane, Ojas Gramopadhye, Prateek Chanda, Ganesh Ramakrishnan, Kshitij Sharad Jadhav, Yatin Nandwani, Dinesh Raghu, Sachindra Joshi},
    abstract = {  In this paper, we propose a modified version of the MedQA-USMLE dataset,
named MEDQA-OPEN, which contains open-ended medical questions without options
to mimic clinical scenarios, along with clinician-approved reasoned answers.
Additionally, we implement a prompt driven by Chain of Thought (CoT) reasoning,
CLINICR, to mirror the prospective process of incremental reasoning, reaching a
correct response to medical questions. We empirically demonstrate how CLINICR
outperforms the state-of-the-art 5-shot CoT-based prompt (Li\'evin et al.,
2022). We also present an approach that mirrors real-life clinical practice by
first exploring multiple differential diagnoses through MCQ-CLINICR and
subsequently narrowing down to a final diagnosis using MCQ-ELIMINATIVE.
Finally, emphasizing the importance of response verification in medical
settings, we utilize a reward model mechanism, replacing the elimination
process performed by MCQ-ELIMINATIVE.
},
    year = {2024},
    month = {03},
    url = {http://arxiv.org/pdf/2403.04890v3},
    journal = {arXiv preprint}
}

@article{arxiv:UMass-BioNLP_at_MEDIQA-M3G_2024:_DermPrompt_--_A_Systematic_Exploration
__of_Prompt_Engineering_with_GPT-4V_for_Dermatological_Diagnosis,
    title = {UMass-BioNLP at MEDIQA-M3G 2024: DermPrompt -- A Systematic Exploration
  of Prompt Engineering with GPT-4V for Dermatological Diagnosis},
    author = {Parth Vashisht, Abhilasha Lodha, Mukta Maddipatla, Zonghai Yao, Avijit Mitra, Zhichao Yang, Junda Wang, Sunjae Kwon, Hong Yu},
    abstract = {  This paper presents our team's participation in the MEDIQA-ClinicalNLP2024
shared task B. We present a novel approach to diagnosing clinical dermatology
cases by integrating large multimodal models, specifically leveraging the
capabilities of GPT-4V under a retriever and a re-ranker framework. Our
investigation reveals that GPT-4V, when used as a retrieval agent, can
accurately retrieve the correct skin condition 85% of the time using
dermatological images and brief patient histories. Additionally, we empirically
show that Naive Chain-of-Thought (CoT) works well for retrieval while Medical
Guidelines Grounded CoT is required for accurate dermatological diagnosis.
Further, we introduce a Multi-Agent Conversation (MAC) framework and show its
superior performance and potential over the best CoT strategy. The experiments
suggest that using naive CoT for retrieval and multi-agent conversation for
critique-based diagnosis, GPT-4V can lead to an early and accurate diagnosis of
dermatological conditions. The implications of this work extend to improving
diagnostic workflows, supporting dermatological education, and enhancing
patient care by providing a scalable, accessible, and accurate diagnostic tool.
},
    year = {2024},
    month = {04},
    url = {http://arxiv.org/pdf/2404.17749v2},
    journal = {arXiv preprint}
}

@article{arxiv:DiReCT:_Diagnostic_Reasoning_for_Clinical_Notes_via_Large_Language
__Models,
    title = {DiReCT: Diagnostic Reasoning for Clinical Notes via Large Language
  Models},
    author = {Bowen Wang, Jiuyang Chang, Yiming Qian, Guoxin Chen, Junhao Chen, Zhouqiang Jiang, Jiahao Zhang, Yuta Nakashima, Hajime Nagahara},
    abstract = {  Large language models (LLMs) have recently showcased remarkable capabilities,
spanning a wide range of tasks and applications, including those in the medical
domain. Models like GPT-4 excel in medical question answering but may face
challenges in the lack of interpretability when handling complex tasks in real
clinical settings. We thus introduce the diagnostic reasoning dataset for
clinical notes (DiReCT), aiming at evaluating the reasoning ability and
interpretability of LLMs compared to human doctors. It contains 511 clinical
notes, each meticulously annotated by physicians, detailing the diagnostic
reasoning process from observations in a clinical note to the final diagnosis.
Additionally, a diagnostic knowledge graph is provided to offer essential
knowledge for reasoning, which may not be covered in the training data of
existing LLMs. Evaluations of leading LLMs on DiReCT bring out a significant
gap between their reasoning ability and that of human doctors, highlighting the
critical need for models that can reason effectively in real-world clinical
scenarios.
},
    year = {2024},
    month = {08},
    url = {http://arxiv.org/pdf/2408.01933v4},
    journal = {arXiv preprint}
}

@article{arxiv:A_Perspective_for_Adapting_Generalist_AI_to_Specialized_Medical_AI
__Applications_and_Their_Challenges,
    title = {A Perspective for Adapting Generalist AI to Specialized Medical AI
  Applications and Their Challenges},
    author = {Zifeng Wang, Hanyin Wang, Benjamin Danek, Ying Li, Christina Mack, Hoifung Poon, Yajuan Wang, Pranav Rajpurkar, Jimeng Sun},
    abstract = {  The integration of Large Language Models (LLMs) into medical applications has
sparked widespread interest across the healthcare industry, from drug discovery
and development to clinical decision support, assisting telemedicine, medical
devices, and healthcare insurance applications. This perspective paper aims to
discuss the inner workings of building LLM-powered medical AI applications and
introduces a comprehensive framework for their development. We review existing
literature and outline the unique challenges of applying LLMs in specialized
medical contexts. Additionally, we introduce a three-step framework to organize
medical LLM research activities: 1) Modeling: breaking down complex medical
workflows into manageable steps for developing medical-specific models; 2)
Optimization: optimizing the model performance with crafted prompts and
integrating external knowledge and tools, and 3) System engineering:
decomposing complex tasks into subtasks and leveraging human expertise for
building medical AI applications. Furthermore, we offer a detailed use case
playbook that describes various LLM-powered medical AI applications, such as
optimizing clinical trial design, enhancing clinical decision support, and
advancing medical imaging analysis. Finally, we discuss various challenges and
considerations for building medical AI applications with LLMs, such as handling
hallucination issues, data ownership and compliance, privacy, intellectual
property considerations, compute cost, sustainability issues, and responsible
AI requirements.
},
    year = {2024},
    month = {10},
    url = {http://arxiv.org/pdf/2411.00024v3},
    journal = {arXiv preprint}
}

@article{arxiv:How_Can_We_Diagnose_and_Treat_Bias_in_Large_Language_Models_for_Clinical
__Decision-Making?,
    title = {How Can We Diagnose and Treat Bias in Large Language Models for Clinical
  Decision-Making?},
    author = {Kenza Benkirane, Jackie Kay, Maria Perez-Ortiz},
    abstract = {  Recent advancements in Large Language Models (LLMs) have positioned them as
powerful tools for clinical decision-making, with rapidly expanding
applications in healthcare. However, concerns about bias remain a significant
challenge in the clinical implementation of LLMs, particularly regarding gender
and ethnicity. This research investigates the evaluation and mitigation of bias
in LLMs applied to complex clinical cases, focusing on gender and ethnicity
biases. We introduce a novel Counterfactual Patient Variations (CPV) dataset
derived from the JAMA Clinical Challenge. Using this dataset, we built a
framework for bias evaluation, employing both Multiple Choice Questions (MCQs)
and corresponding explanations. We explore prompting with eight LLMs and
fine-tuning as debiasing methods. Our findings reveal that addressing social
biases in LLMs requires a multidimensional approach as mitigating gender bias
can occur while introducing ethnicity biases, and that gender bias in LLM
embeddings varies significantly across medical specialities. We demonstrate
that evaluating both MCQ response and explanation processes is crucial, as
correct responses can be based on biased \textit{reasoning}. We provide a
framework for evaluating LLM bias in real-world clinical cases, offer insights
into the complex nature of bias in these models, and present strategies for
bias mitigation.
},
    year = {2024},
    month = {10},
    url = {http://arxiv.org/pdf/2410.16574v1},
    journal = {arXiv preprint}
}

@article{arxiv:Merging_Clinical_Knowledge_into_Large_Language_Models_for_Medical
__Research_and_Applications:_A_Survey,
    title = {Merging Clinical Knowledge into Large Language Models for Medical
  Research and Applications: A Survey},
    author = {Qiyuan Li, Haijiang Liu, Caicai Guo, Deyu Chen, Meng Wang, Feng Gao, Jinguang Gu},
    abstract = {  Clinical knowledge is the collection of information learned from studies on
the causes, prognosis, diagnosis, and treatment of diseases. This type of
knowledge can improve curing performances, and promote physical health. With
the emergence of large language models (LLMs), medical artificial intelligence
(medical AI), which aims to apply academic medical AI systems to real-world
medical scenarios, has entered a new age of development, resulting in excellent
works such as DoctorGPT and Pangu-Drug from academic and industrial researches.
However, the field lacks a comprehensive compendium and comparison of building
medical AI systems from academia and industry. Therefore, this survey focuses
on the building paradigms of medical AI systems including the use of clinical
databases, datasets, training pipelines, integrating medical knowledge graphs,
system applications, and evaluation systems. We hope that this survey can help
relevant practical researchers understand the current performance of academic
models in various fields of healthcare, as well as the potential problems and
future directions for implementing these scientific achievements.
},
    year = {2025},
    month = {02},
    url = {http://arxiv.org/pdf/2502.20988v1},
    journal = {arXiv preprint}
}

@article{arxiv:MedSyn:_LLM-based_Synthetic_Medical_Text_Generation_Framework,
    title = {MedSyn: LLM-based Synthetic Medical Text Generation Framework},
    author = {Gleb Kumichev, Pavel Blinov, Yulia Kuzkina, Vasily Goncharov, Galina Zubkova, Nikolai Zenovkin, Aleksei Goncharov, Andrey Savchenko},
    abstract = {  Generating synthetic text addresses the challenge of data availability in
privacy-sensitive domains such as healthcare. This study explores the
applicability of synthetic data in real-world medical settings. We introduce
MedSyn, a novel medical text generation framework that integrates large
language models with a Medical Knowledge Graph (MKG). We use MKG to sample
prior medical information for the prompt and generate synthetic clinical notes
with GPT-4 and fine-tuned LLaMA models. We assess the benefit of synthetic data
through application in the ICD code prediction task. Our research indicates
that synthetic data can increase the classification accuracy of vital and
challenging codes by up to 17.8% compared to settings without synthetic data.
Furthermore, to provide new data for further research in the healthcare domain,
we present the largest open-source synthetic dataset of clinical notes for the
Russian language, comprising over 41k samples covering 219 ICD-10 codes.
},
    year = {2024},
    month = {08},
    url = {http://arxiv.org/pdf/2408.02056v1},
    journal = {arXiv preprint}
}

@article{arxiv:Improving_Large_Language_Models_for_Clinical_Named_Entity_Recognition
__via_Prompt_Engineering,
    title = {Improving Large Language Models for Clinical Named Entity Recognition
  via Prompt Engineering},
    author = {Yan Hu, Qingyu Chen, Jingcheng Du, Xueqing Peng, Vipina Kuttichi Keloth, Xu Zuo, Yujia Zhou, Zehan Li, Xiaoqian Jiang, Zhiyong Lu, Kirk Roberts, Hua Xu},
    abstract = {  Objective: This study quantifies the capabilities of GPT-3.5 and GPT-4 for
clinical named entity recognition (NER) tasks and proposes task-specific
prompts to improve their performance. Materials and Methods: We evaluated these
models on two clinical NER tasks: (1) to extract medical problems, treatments,
and tests from clinical notes in the MTSamples corpus, following the 2010 i2b2
concept extraction shared task, and (2) identifying nervous system
disorder-related adverse events from safety reports in the vaccine adverse
event reporting system (VAERS). To improve the GPT models' performance, we
developed a clinical task-specific prompt framework that includes (1) baseline
prompts with task description and format specification, (2) annotation
guideline-based prompts, (3) error analysis-based instructions, and (4)
annotated samples for few-shot learning. We assessed each prompt's
effectiveness and compared the models to BioClinicalBERT. Results: Using
baseline prompts, GPT-3.5 and GPT-4 achieved relaxed F1 scores of 0.634, 0.804
for MTSamples, and 0.301, 0.593 for VAERS. Additional prompt components
consistently improved model performance. When all four components were used,
GPT-3.5 and GPT-4 achieved relaxed F1 socres of 0.794, 0.861 for MTSamples and
0.676, 0.736 for VAERS, demonstrating the effectiveness of our prompt
framework. Although these results trail BioClinicalBERT (F1 of 0.901 for the
MTSamples dataset and 0.802 for the VAERS), it is very promising considering
few training samples are needed. Conclusion: While direct application of GPT
models to clinical NER tasks falls short of optimal performance, our
task-specific prompt framework, incorporating medical knowledge and training
samples, significantly enhances GPT models' feasibility for potential clinical
applications.
},
    year = {2023},
    month = {03},
    url = {http://arxiv.org/pdf/2303.16416v3},
    journal = {arXiv preprint}
}

@article{arxiv:Prompting_Large_Language_Models_for_Supporting_the_Differential
__Diagnosis_of_Anemia,
    title = {Prompting Large Language Models for Supporting the Differential
  Diagnosis of Anemia},
    author = {Elisa Castagnari, Lillian Muyama, Adrien Coulet},
    abstract = {  In practice, clinicians achieve a diagnosis by following a sequence of steps,
such as laboratory exams, observations, or imaging. The pathways to reach
diagnosis decisions are documented by guidelines authored by expert
organizations, which guide clinicians to reach a correct diagnosis through
these sequences of steps. While these guidelines are beneficial for following
medical reasoning and consolidating medical knowledge, they have some
drawbacks. They often fail to address patients with uncommon conditions due to
their focus on the majority population, and are slow and costly to update,
making them unsuitable for rapidly emerging diseases or new practices. Inspired
by clinical guidelines, our study aimed to develop pathways similar to those
that can be obtained in clinical guidelines. We tested three Large Language
Models (LLMs) -Generative Pretrained Transformer 4 (GPT-4), Large Language
Model Meta AI (LLaMA), and Mistral -on a synthetic yet realistic dataset to
differentially diagnose anemia and its subtypes. By using advanced prompting
techniques to enhance the decision-making process, we generated diagnostic
pathways using these models. Experimental results indicate that LLMs hold huge
potential in clinical pathway discovery from patient data, with GPT-4
exhibiting the best performance in all conducted experiments.
},
    year = {2024},
    month = {09},
    url = {http://arxiv.org/pdf/2409.15377v1},
    journal = {arXiv preprint}
}

@article{arxiv:Contextual_Evaluation_of_Large_Language_Models_for_Classifying_Tropical
__and_Infectious_Diseases,
    title = {Contextual Evaluation of Large Language Models for Classifying Tropical
  and Infectious Diseases},
    author = {Mercy Asiedu, Nenad Tomasev, Chintan Ghate, Tiya Tiyasirichokchai, Awa Dieng, Oluwatosin Akande, Geoffrey Siwo, Steve Adudans, Sylvanus Aitkins, Odianosen Ehiakhamen, Eric Ndombi, Katherine Heller},
    abstract = {  While large language models (LLMs) have shown promise for medical question
answering, there is limited work focused on tropical and infectious
disease-specific exploration. We build on an opensource tropical and infectious
diseases (TRINDs) dataset, expanding it to include demographic and semantic
clinical and consumer augmentations yielding 11000+ prompts. We evaluate LLM
performance on these, comparing generalist and medical LLMs, as well as LLM
outcomes to human experts. We demonstrate through systematic experimentation,
the benefit of contextual information such as demographics, location, gender,
risk factors for optimal LLM response. Finally we develop a prototype of
TRINDs-LM, a research tool that provides a playground to navigate how context
impacts LLM outputs for health.
},
    year = {2024},
    month = {09},
    url = {http://arxiv.org/pdf/2409.09201v3},
    journal = {arXiv preprint}
}

@article{arxiv:Assertion_Detection_Large_Language_Model_In-context_Learning_LoRA
__Fine-tuning,
    title = {Assertion Detection Large Language Model In-context Learning LoRA
  Fine-tuning},
    author = {Yuelyu Ji, Zeshui Yu, Yanshan Wang},
    abstract = {  In this study, we aim to address the task of assertion detection when
extracting medical concepts from clinical notes, a key process in clinical
natural language processing (NLP). Assertion detection in clinical NLP usually
involves identifying assertion types for medical concepts in the clinical text,
namely certainty (whether the medical concept is positive, negated, possible,
or hypothetical), temporality (whether the medical concept is for present or
the past history), and experiencer (whether the medical concept is described
for the patient or a family member). These assertion types are essential for
healthcare professionals to quickly and clearly understand the context of
medical conditions from unstructured clinical texts, directly influencing the
quality and outcomes of patient care. Although widely used, traditional
methods, particularly rule-based NLP systems and machine learning or deep
learning models, demand intensive manual efforts to create patterns and tend to
overlook less common assertion types, leading to an incomplete understanding of
the context. To address this challenge, our research introduces a novel
methodology that utilizes Large Language Models (LLMs) pre-trained on a vast
array of medical data for assertion detection. We enhanced the current method
with advanced reasoning techniques, including Tree of Thought (ToT), Chain of
Thought (CoT), and Self-Consistency (SC), and refine it further with Low-Rank
Adaptation (LoRA) fine-tuning. We first evaluated the model on the i2b2 2010
assertion dataset. Our method achieved a micro-averaged F-1 of 0.89, with 0.11
improvements over the previous works. To further assess the generalizability of
our approach, we extended our evaluation to a local dataset that focused on
sleep concept extraction. Our approach achieved an F-1 of 0.74, which is 0.31
higher than the previous method.
},
    year = {2024},
    month = {01},
    url = {http://arxiv.org/pdf/2401.17602v1},
    journal = {arXiv preprint}
}

@article{arxiv:MedDiT:_A_Knowledge-Controlled_Diffusion_Transformer_Framework_for
__Dynamic_Medical_Image_Generation_in_Virtual_Simulated_Patient,
    title = {MedDiT: A Knowledge-Controlled Diffusion Transformer Framework for
  Dynamic Medical Image Generation in Virtual Simulated Patient},
    author = {Yanzeng Li, Cheng Zeng, Jinchao Zhang, Jie Zhou, Lei Zou},
    abstract = {  Medical education relies heavily on Simulated Patients (SPs) to provide a
safe environment for students to practice clinical skills, including medical
image analysis. However, the high cost of recruiting qualified SPs and the lack
of diverse medical imaging datasets have presented significant challenges. To
address these issues, this paper introduces MedDiT, a novel
knowledge-controlled conversational framework that can dynamically generate
plausible medical images aligned with simulated patient symptoms, enabling
diverse diagnostic skill training. Specifically, MedDiT integrates various
patient Knowledge Graphs (KGs), which describe the attributes and symptoms of
patients, to dynamically prompt Large Language Models' (LLMs) behavior and
control the patient characteristics, mitigating hallucination during medical
conversation. Additionally, a well-tuned Diffusion Transformer (DiT) model is
incorporated to generate medical images according to the specified patient
attributes in the KG. In this paper, we present the capabilities of MedDiT
through a practical demonstration, showcasing its ability to act in diverse
simulated patient cases and generate the corresponding medical images. This can
provide an abundant and interactive learning experience for students, advancing
medical education by offering an immersive simulation platform for future
healthcare professionals. The work sheds light on the feasibility of
incorporating advanced technologies like LLM, KG, and DiT in education
applications, highlighting their potential to address the challenges faced in
simulated patient-based medical education.
},
    year = {2024},
    month = {08},
    url = {http://arxiv.org/pdf/2408.12236v1},
    journal = {arXiv preprint}
}

@article{arxiv:KG-Rank:_Enhancing_Large_Language_Models_for_Medical_QA_with_Knowledge
__Graphs_and_Ranking_Techniques,
    title = {KG-Rank: Enhancing Large Language Models for Medical QA with Knowledge
  Graphs and Ranking Techniques},
    author = {Rui Yang, Haoran Liu, Edison Marrese-Taylor, Qingcheng Zeng, Yu He Ke, Wanxin Li, Lechao Cheng, Qingyu Chen, James Caverlee, Yutaka Matsuo, Irene Li},
    abstract = {  Large language models (LLMs) have demonstrated impressive generative
capabilities with the potential to innovate in medicine. However, the
application of LLMs in real clinical settings remains challenging due to the
lack of factual consistency in the generated content. In this work, we develop
an augmented LLM framework, KG-Rank, which leverages a medical knowledge graph
(KG) along with ranking and re-ranking techniques, to improve the factuality of
long-form question answering (QA) in the medical domain. Specifically, when
receiving a question, KG-Rank automatically identifies medical entities within
the question and retrieves the related triples from the medical KG to gather
factual information. Subsequently, KG-Rank innovatively applies multiple
ranking techniques to refine the ordering of these triples, providing more
relevant and precise information for LLM inference. To the best of our
knowledge, KG-Rank is the first application of KG combined with ranking models
in medical QA specifically for generating long answers. Evaluation on four
selected medical QA datasets demonstrates that KG-Rank achieves an improvement
of over 18% in ROUGE-L score. Additionally, we extend KG-Rank to open domains,
including law, business, music, and history, where it realizes a 14%
improvement in ROUGE-L score, indicating the effectiveness and great potential
of KG-Rank.
},
    year = {2024},
    month = {03},
    url = {http://arxiv.org/pdf/2403.05881v3},
    journal = {arXiv preprint}
}

@article{arxiv:ClinicalBench:_Can_LLMs_Beat_Traditional_ML_Models_in_Clinical
__Prediction?,
    title = {ClinicalBench: Can LLMs Beat Traditional ML Models in Clinical
  Prediction?},
    author = {Canyu Chen, Jian Yu, Shan Chen, Che Liu, Zhongwei Wan, Danielle Bitterman, Fei Wang, Kai Shu},
    abstract = {  Large Language Models (LLMs) hold great promise to revolutionize current
clinical systems for their superior capacities on medical text processing tasks
and medical licensing exams. Meanwhile, traditional ML models such as SVM and
XGBoost have still been mainly adopted in clinical prediction tasks. An
emerging question is Can LLMs beat traditional ML models in clinical
prediction? Thus, we build a new benchmark ClinicalBench to comprehensively
study the clinical predictive modeling capacities of both general-purpose and
medical LLMs, and compare them with traditional ML models. ClinicalBench
embraces three common clinical prediction tasks, two databases, 14
general-purpose LLMs, 8 medical LLMs, and 11 traditional ML models. Through
extensive empirical investigation, we discover that both general-purpose and
medical LLMs, even with different model scales, diverse prompting or
fine-tuning strategies, still cannot beat traditional ML models in clinical
prediction yet, shedding light on their potential deficiency in clinical
reasoning and decision-making. We call for caution when practitioners adopt
LLMs in clinical applications. ClinicalBench can be utilized to bridge the gap
between LLMs' development for healthcare and real-world clinical practice.
},
    year = {2024},
    month = {11},
    url = {http://arxiv.org/pdf/2411.06469v1},
    journal = {arXiv preprint}
}

@article{arxiv:RareBench:_Can_LLMs_Serve_as_Rare_Diseases_Specialists?,
    title = {RareBench: Can LLMs Serve as Rare Diseases Specialists?},
    author = {Xuanzhong Chen, Xiaohao Mao, Qihan Guo, Lun Wang, Shuyang Zhang, Ting Chen},
    abstract = {  Generalist Large Language Models (LLMs), such as GPT-4, have shown
considerable promise in various domains, including medical diagnosis. Rare
diseases, affecting approximately 300 million people worldwide, often have
unsatisfactory clinical diagnosis rates primarily due to a lack of experienced
physicians and the complexity of differentiating among many rare diseases. In
this context, recent news such as "ChatGPT correctly diagnosed a 4-year-old's
rare disease after 17 doctors failed" underscore LLMs' potential, yet
underexplored, role in clinically diagnosing rare diseases. To bridge this
research gap, we introduce RareBench, a pioneering benchmark designed to
systematically evaluate the capabilities of LLMs on 4 critical dimensions
within the realm of rare diseases. Meanwhile, we have compiled the largest
open-source dataset on rare disease patients, establishing a benchmark for
future studies in this domain. To facilitate differential diagnosis of rare
diseases, we develop a dynamic few-shot prompt methodology, leveraging a
comprehensive rare disease knowledge graph synthesized from multiple knowledge
bases, significantly enhancing LLMs' diagnostic performance. Moreover, we
present an exhaustive comparative study of GPT-4's diagnostic capabilities
against those of specialist physicians. Our experimental findings underscore
the promising potential of integrating LLMs into the clinical diagnostic
process for rare diseases. This paves the way for exciting possibilities in
future advancements in this field.
},
    year = {2024},
    month = {02},
    url = {http://arxiv.org/pdf/2402.06341v2},
    journal = {arXiv preprint}
}

@article{arxiv:EyeGPT:_Ophthalmic_Assistant_with_Large_Language_Models,
    title = {EyeGPT: Ophthalmic Assistant with Large Language Models},
    author = {Xiaolan Chen, Ziwei Zhao, Weiyi Zhang, Pusheng Xu, Le Gao, Mingpu Xu, Yue Wu, Yinwen Li, Danli Shi, Mingguang He},
    abstract = {  Artificial intelligence (AI) has gained significant attention in healthcare
consultation due to its potential to improve clinical workflow and enhance
medical communication. However, owing to the complex nature of medical
information, large language models (LLM) trained with general world knowledge
might not possess the capability to tackle medical-related tasks at an expert
level. Here, we introduce EyeGPT, a specialized LLM designed specifically for
ophthalmology, using three optimization strategies including role-playing,
finetuning, and retrieval-augmented generation. In particular, we proposed a
comprehensive evaluation framework that encompasses a diverse dataset, covering
various subspecialties of ophthalmology, different users, and diverse inquiry
intents. Moreover, we considered multiple evaluation metrics, including
accuracy, understandability, trustworthiness, empathy, and the proportion of
hallucinations. By assessing the performance of different EyeGPT variants, we
identify the most effective one, which exhibits comparable levels of
understandability, trustworthiness, and empathy to human ophthalmologists (all
Ps>0.05). Overall, ur study provides valuable insights for future research,
facilitating comprehensive comparisons and evaluations of different strategies
for developing specialized LLMs in ophthalmology. The potential benefits
include enhancing the patient experience in eye care and optimizing
ophthalmologists' services.
},
    year = {2024},
    month = {02},
    url = {http://arxiv.org/pdf/2403.00840v1},
    journal = {arXiv preprint}
}

@article{arxiv:Do_Physicians_Know_How_to_Prompt?_The_Need_for_Automatic_Prompt
__Optimization_Help_in_Clinical_Note_Generation,
    title = {Do Physicians Know How to Prompt? The Need for Automatic Prompt
  Optimization Help in Clinical Note Generation},
    author = {Zonghai Yao, Ahmed Jaafar, Beining Wang, Zhichao Yang, Hong Yu},
    abstract = {  This study examines the effect of prompt engineering on the performance of
Large Language Models (LLMs) in clinical note generation. We introduce an
Automatic Prompt Optimization (APO) framework to refine initial prompts and
compare the outputs of medical experts, non-medical experts, and APO-enhanced
GPT3.5 and GPT4. Results highlight GPT4 APO's superior performance in
standardizing prompt quality across clinical note sections. A human-in-the-loop
approach shows that experts maintain content quality post-APO, with a
preference for their own modifications, suggesting the value of expert
customization. We recommend a two-phase optimization process, leveraging
APO-GPT4 for consistency and expert input for personalization.
},
    year = {2023},
    month = {11},
    url = {http://arxiv.org/pdf/2311.09684v3},
    journal = {arXiv preprint}
}

@article{arxiv:Performance_of_large_language_models_in_numerical_vs._semantic_medical
__knowledge:_Benchmarking_on_evidence-based_Q&As,
    title = {Performance of large language models in numerical vs. semantic medical
  knowledge: Benchmarking on evidence-based Q&As},
    author = {Eden Avnat, Michal Levy, Daniel Herstain, Elia Yanko, Daniel Ben Joya, Michal Tzuchman Katz, Dafna Eshel, Sahar Laros, Yael Dagan, Shahar Barami, Joseph Mermelstein, Shahar Ovadia, Noam Shomron, Varda Shalev, Raja-Elie E. Abdulnour},
    abstract = {  Clinical problem-solving requires processing of semantic medical knowledge
such as illness scripts and numerical medical knowledge of diagnostic tests for
evidence-based decision-making. As large language models (LLMs) show promising
results in many aspects of language-based clinical practice, their ability to
generate non-language evidence-based answers to clinical questions is
inherently limited by tokenization. Therefore, we evaluated LLMs' performance
on two question types: numeric (correlating findings) and semantic
(differentiating entities) while examining differences within and between LLMs
in medical aspects and comparing their performance to humans. To generate
straightforward multi-choice questions and answers (QAs) based on
evidence-based medicine (EBM), we used a comprehensive medical knowledge graph
(encompassed data from more than 50,00 peer-reviewed articles) and created the
"EBMQA". EBMQA contains 105,000 QAs labeled with medical and non-medical topics
and classified into numerical or semantic questions. We benchmarked this
dataset using more than 24,500 QAs on two state-of-the-art LLMs: Chat-GPT4 and
Claude3-Opus. We evaluated the LLMs accuracy on semantic and numerical question
types and according to sub-labeled topics. For validation, six medical experts
were tested on 100 numerical EBMQA questions. We found that both LLMs excelled
more in semantic than numerical QAs, with Claude3 surpassing GPT4 in numerical
QAs. However, both LLMs showed inter and intra gaps in different medical
aspects and remained inferior to humans. Thus, their medical advice should be
addressed carefully.
},
    year = {2024},
    month = {06},
    url = {http://arxiv.org/pdf/2406.03855v3},
    journal = {arXiv preprint}
}

@article{arxiv:Clinical_Camel:_An_Open_Expert-Level_Medical_Language_Model_with
__Dialogue-Based_Knowledge_Encoding,
    title = {Clinical Camel: An Open Expert-Level Medical Language Model with
  Dialogue-Based Knowledge Encoding},
    author = {Augustin Toma, Patrick R. Lawler, Jimmy Ba, Rahul G. Krishnan, Barry B. Rubin, Bo Wang},
    abstract = {  We present Clinical Camel, an open large language model (LLM) explicitly
tailored for clinical research. Fine-tuned from LLaMA-2 using QLoRA, Clinical
Camel achieves state-of-the-art performance across medical benchmarks among
openly available medical LLMs. Leveraging efficient single-GPU training,
Clinical Camel surpasses GPT-3.5 in five-shot evaluations on all assessed
benchmarks, including 64.3% on the USMLE Sample Exam (compared to 58.5% for
GPT-3.5), 77.9% on PubMedQA (compared to 60.2%), 60.7% on MedQA (compared to
53.6%), and 54.2% on MedMCQA (compared to 51.0%). In addition to these
benchmarks, Clinical Camel demonstrates its broader capabilities, such as
synthesizing plausible clinical notes. This work introduces dialogue-based
knowledge encoding, a novel method to synthesize conversational data from dense
medical texts. While benchmark results are encouraging, extensive and rigorous
human evaluation across diverse clinical scenarios is imperative to ascertain
safety before implementation. By openly sharing Clinical Camel, we hope to
foster transparent and collaborative research, working towards the safe
integration of LLMs within the healthcare domain. Significant challenges
concerning reliability, bias, and the potential for outdated knowledge persist.
Nonetheless, the transparency provided by an open approach reinforces the
scientific rigor essential for future clinical applications.
},
    year = {2023},
    month = {05},
    url = {http://arxiv.org/pdf/2305.12031v2},
    journal = {arXiv preprint}
}

@article{arxiv:Benchmarking_Large_Language_Models_on_Answering_and_Explaining
__Challenging_Medical_Questions,
    title = {Benchmarking Large Language Models on Answering and Explaining
  Challenging Medical Questions},
    author = {Hanjie Chen, Zhouxiang Fang, Yash Singla, Mark Dredze},
    abstract = {  LLMs have demonstrated impressive performance in answering medical questions,
such as achieving passing scores on medical licensing examinations. However,
medical board exams or general clinical questions do not capture the complexity
of realistic clinical cases. Moreover, the lack of reference explanations means
we cannot easily evaluate the reasoning of model decisions, a crucial component
of supporting doctors in making complex medical decisions. To address these
challenges, we construct two new datasets: JAMA Clinical Challenge and
Medbullets.\footnote{Datasets and code are available at
\url{https://github.com/HanjieChen/ChallengeClinicalQA}.} JAMA Clinical
Challenge consists of questions based on challenging clinical cases, while
Medbullets comprises simulated clinical questions. Both datasets are structured
as multiple-choice question-answering tasks, accompanied by expert-written
explanations. We evaluate seven LLMs on the two datasets using various prompts.
Experiments demonstrate that our datasets are harder than previous benchmarks.
In-depth automatic and human evaluations of model-generated explanations
provide insights into the promise and deficiency of LLMs for explainable
medical QA.
},
    year = {2024},
    month = {02},
    url = {http://arxiv.org/pdf/2402.18060v5},
    journal = {arXiv preprint}
}

@article{arxiv:Biomedical_Large_Languages_Models_Seem_not_to_be_Superior_to_Generalist
__Models_on_Unseen_Medical_Data,
    title = {Biomedical Large Languages Models Seem not to be Superior to Generalist
  Models on Unseen Medical Data},
    author = {Felix J. Dorfner, Amin Dada, Felix Busch, Marcus R. Makowski, Tianyu Han, Daniel Truhn, Jens Kleesiek, Madhumita Sushil, Jacqueline Lammert, Lisa C. Adams, Keno K. Bressem},
    abstract = {  Large language models (LLMs) have shown potential in biomedical applications,
leading to efforts to fine-tune them on domain-specific data. However, the
effectiveness of this approach remains unclear. This study evaluates the
performance of biomedically fine-tuned LLMs against their general-purpose
counterparts on a variety of clinical tasks. We evaluated their performance on
clinical case challenges from the New England Journal of Medicine (NEJM) and
the Journal of the American Medical Association (JAMA) and on several clinical
tasks (e.g., information extraction, document summarization, and clinical
coding). Using benchmarks specifically chosen to be likely outside the
fine-tuning datasets of biomedical models, we found that biomedical LLMs mostly
perform inferior to their general-purpose counterparts, especially on tasks not
focused on medical knowledge. While larger models showed similar performance on
case tasks (e.g., OpenBioLLM-70B: 66.4% vs. Llama-3-70B-Instruct: 65% on JAMA
cases), smaller biomedical models showed more pronounced underperformance
(e.g., OpenBioLLM-8B: 30% vs. Llama-3-8B-Instruct: 64.3% on NEJM cases).
Similar trends were observed across the CLUE (Clinical Language Understanding
Evaluation) benchmark tasks, with general-purpose models often performing
better on text generation, question answering, and coding tasks. Our results
suggest that fine-tuning LLMs to biomedical data may not provide the expected
benefits and may potentially lead to reduced performance, challenging
prevailing assumptions about domain-specific adaptation of LLMs and
highlighting the need for more rigorous evaluation frameworks in healthcare AI.
Alternative approaches, such as retrieval-augmented generation, may be more
effective in enhancing the biomedical capabilities of LLMs without compromising
their general knowledge.
},
    year = {2024},
    month = {08},
    url = {http://arxiv.org/pdf/2408.13833v1},
    journal = {arXiv preprint}
}

@article{arxiv:Open_(Clinical)_LLMs_are_Sensitive_to_Instruction_Phrasings,
    title = {Open (Clinical) LLMs are Sensitive to Instruction Phrasings},
    author = {Alberto Mario Ceballos Arroyo, Monica Munnangi, Jiuding Sun, Karen Y. C. Zhang, Denis Jered McInerney, Byron C. Wallace, Silvio Amir},
    abstract = {  Instruction-tuned Large Language Models (LLMs) can perform a wide range of
tasks given natural language instructions to do so, but they are sensitive to
how such instructions are phrased. This issue is especially concerning in
healthcare, as clinicians are unlikely to be experienced prompt engineers and
the potential consequences of inaccurate outputs are heightened in this domain.
  This raises a practical question: How robust are instruction-tuned LLMs to
natural variations in the instructions provided for clinical NLP tasks? We
collect prompts from medical doctors across a range of tasks and quantify the
sensitivity of seven LLMs -- some general, others specialized -- to natural
(i.e., non-adversarial) instruction phrasings. We find that performance varies
substantially across all models, and that -- perhaps surprisingly --
domain-specific models explicitly trained on clinical data are especially
brittle, compared to their general domain counterparts. Further, arbitrary
phrasing differences can affect fairness, e.g., valid but distinct instructions
for mortality prediction yield a range both in overall performance, and in
terms of differences between demographic groups.
},
    year = {2024},
    month = {07},
    url = {http://arxiv.org/pdf/2407.09429v1},
    journal = {arXiv preprint}
}

@article{arxiv:Prompt-Guided_Generation_of_Structured_Chest_X-Ray_Report_Using_a
__Pre-trained_LLM,
    title = {Prompt-Guided Generation of Structured Chest X-Ray Report Using a
  Pre-trained LLM},
    author = {Hongzhao Li, Hongyu Wang, Xia Sun, Hua He, Jun Feng},
    abstract = {  Medical report generation automates radiology descriptions from images,
easing the burden on physicians and minimizing errors. However, current methods
lack structured outputs and physician interactivity for clear, clinically
relevant reports. Our method introduces a prompt-guided approach to generate
structured chest X-ray reports using a pre-trained large language model (LLM).
First, we identify anatomical regions in chest X-rays to generate focused
sentences that center on key visual elements, thereby establishing a structured
report foundation with anatomy-based sentences. We also convert the detected
anatomy into textual prompts conveying anatomical comprehension to the LLM.
Additionally, the clinical context prompts guide the LLM to emphasize
interactivity and clinical requirements. By integrating anatomy-focused
sentences and anatomy/clinical prompts, the pre-trained LLM can generate
structured chest X-ray reports tailored to prompted anatomical regions and
clinical contexts. We evaluate using language generation and clinical
effectiveness metrics, demonstrating strong performance.
},
    year = {2024},
    month = {04},
    url = {http://arxiv.org/pdf/2404.11209v1},
    journal = {arXiv preprint}
}

@article{arxiv:Lightweight_Large_Language_Model_for_Medication_Enquiry:_Med-Pal,
    title = {Lightweight Large Language Model for Medication Enquiry: Med-Pal},
    author = {Kabilan Elangovan, Jasmine Chiat Ling Ong, Liyuan Jin, Benjamin Jun Jie Seng, Yu Heng Kwan, Lit Soo Tan, Ryan Jian Zhong, Justina Koi Li Ma, YuHe Ke, Nan Liu, Kathleen M Giacomini, Daniel Shu Wei Ting},
    abstract = {  Large Language Models (LLMs) have emerged as a potential solution to assist
digital health development with patient education, commonly medication-related
enquires. We trained and validated Med-Pal, a medication domain-specific
LLM-chatbot fine-tuned with a fine-grained and expert curated dataset from a
selection of five light-weighted open-source LLMs of smaller parameter size (7
billion or less) regarding computational constraints and prioritizing
operational efficiency. A multi-disciplinary team performed a clinical
evaluation of LLMs responses using the SCORE criteria, focusing on safety,
accuracy, bias, reproducibility, and ease of understanding. Best performing
light-weighted LLM was chosen as Med-Pal for further engineering with
guard-railing using adversarial prompting. Med-Pal and existing light-weighted
LLMs, including pretrained Biomistral and finetuned Meerkat, were validated on
an independent dataset on a broad range of medication-related questions (231 in
total), 12 different question types across 14 different medication classes.
Mistral-7b emerged as the top performer among selected lightweight LLMs,
achieving the highest median score of 14 and 71.9% high-quality responses in
accuracy and safety domains, hence chosen as the backbone LLM for Med-Pal. When
compared against Biomistral, Med-pal outperformed in generating responses
appropriate for patient communication, with significant reductions bias and
errors typical of general LLMs. Comparable performance was observed when
comparing Med-Pal with Meerkat. Med-Pal showcases the feasibility of developing
and employing fine-tuned light-weighted LLMs to enhance digital health
communications.
},
    year = {2024},
    month = {07},
    url = {http://arxiv.org/pdf/2407.12822v1},
    journal = {arXiv preprint}
}

@article{arxiv:Unlocking_Multimodal_Integration_in_EHRs:_A_Prompt_Learning_Framework
__for_Language_and_Time_Series_Fusion,
    title = {Unlocking Multimodal Integration in EHRs: A Prompt Learning Framework
  for Language and Time Series Fusion},
    author = {Shuai Niu, Jing Ma, Hongzhan Lin, Liang Bai, Zhihua Wang, Wei Bi, Yida Xu, Guo Li, Xian Yang},
    abstract = {  Large language models (LLMs) have shown remarkable performance in
vision-language tasks, but their application in the medical field remains
underexplored, particularly for integrating structured time series data with
unstructured clinical notes. In clinical practice, dynamic time series data
such as lab test results capture critical temporal patterns, while clinical
notes provide rich semantic context. Merging these modalities is challenging
due to the inherent differences between continuous signals and discrete text.
To bridge this gap, we introduce ProMedTS, a novel self-supervised multimodal
framework that employs prompt-guided learning to unify these heterogeneous data
types. Our approach leverages lightweight anomaly detection to generate anomaly
captions that serve as prompts, guiding the encoding of raw time series data
into informative embeddings. These embeddings are aligned with textual
representations in a shared latent space, preserving fine-grained temporal
nuances alongside semantic insights. Furthermore, our framework incorporates
tailored self-supervised objectives to enhance both intra- and inter-modal
alignment. We evaluate ProMedTS on disease diagnosis tasks using real-world
datasets, and the results demonstrate that our method consistently outperforms
state-of-the-art approaches.
},
    year = {2025},
    month = {02},
    url = {http://arxiv.org/pdf/2502.13509v1},
    journal = {arXiv preprint}
}

@article{arxiv:EEG_Emotion_Copilot:_Optimizing_Lightweight_LLMs_for_Emotional_EEG
__Interpretation_with_Assisted_Medical_Record_Generation,
    title = {EEG Emotion Copilot: Optimizing Lightweight LLMs for Emotional EEG
  Interpretation with Assisted Medical Record Generation},
    author = {Hongyu Chen, Weiming Zeng, Chengcheng Chen, Luhui Cai, Fei Wang, Yuhu Shi, Lei Wang, Wei Zhang, Yueyang Li, Hongjie Yan, Wai Ting Siok, Nizhuan Wang},
    abstract = {  In the fields of affective computing (AC) and brain-machine interface (BMI),
the analysis of physiological and behavioral signals to discern individual
emotional states has emerged as a critical research frontier. While deep
learning-based approaches have made notable strides in EEG emotion recognition,
particularly in feature extraction and pattern recognition, significant
challenges persist in achieving end-to-end emotion computation, including
real-time processing, individual adaptation, and seamless user interaction.
This paper presents the EEG Emotion Copilot, a system optimizing a lightweight
large language model (LLM) with 0.5B parameters operating in a local setting,
which first recognizes emotional states directly from EEG signals, subsequently
generates personalized diagnostic and treatment suggestions, and finally
supports the automation of assisted electronic medical records. Specifically,
we demonstrate the critical techniques in the novel data structure of prompt,
model pruning and fine-tuning training, and deployment strategies aiming at
improving real-time performance and computational efficiency. Extensive
experiments show that our optimized lightweight LLM-based copilot achieves an
enhanced intuitive interface for participant interaction, superior accuracy of
emotion recognition and assisted electronic medical records generation, in
comparison to such models with similar scale parameters or large-scale
parameters such as 1.5B, 1.8B, 3B and 7B. In summary, through these efforts,
the proposed copilot is expected to advance the application of AC in the
medical domain, offering innovative solution to mental health monitoring. The
codes will be released at https://github.com/NZWANG/EEG_Emotion_Copilot.
},
    year = {2024},
    month = {09},
    url = {http://arxiv.org/pdf/2410.00166v2},
    journal = {arXiv preprint}
}

@article{arxiv:Generating_medically-accurate_summaries_of_patient-provider_dialogue:_A
__multi-stage_approach_using_large_language_models,
    title = {Generating medically-accurate summaries of patient-provider dialogue: A
  multi-stage approach using large language models},
    author = {Varun Nair, Elliot Schumacher, Anitha Kannan},
    abstract = {  A medical provider's summary of a patient visit serves several critical
purposes, including clinical decision-making, facilitating hand-offs between
providers, and as a reference for the patient. An effective summary is required
to be coherent and accurately capture all the medically relevant information in
the dialogue, despite the complexity of patient-generated language. Even minor
inaccuracies in visit summaries (for example, summarizing "patient does not
have a fever" when a fever is present) can be detrimental to the outcome of
care for the patient.
  This paper tackles the problem of medical conversation summarization by
discretizing the task into several smaller dialogue-understanding tasks that
are sequentially built upon. First, we identify medical entities and their
affirmations within the conversation to serve as building blocks. We study
dynamically constructing few-shot prompts for tasks by conditioning on relevant
patient information and use GPT-3 as the backbone for our experiments. We also
develop GPT-derived summarization metrics to measure performance against
reference summaries quantitatively. Both our human evaluation study and metrics
for medical correctness show that summaries generated using this approach are
clinically accurate and outperform the baseline approach of summarizing the
dialog in a zero-shot, single-prompt setting.
},
    year = {2023},
    month = {05},
    url = {http://arxiv.org/pdf/2305.05982v1},
    journal = {arXiv preprint}
}

@article{arxiv:Towards_Evaluating_and_Building_Versatile_Large_Language_Models_for
__Medicine,
    title = {Towards Evaluating and Building Versatile Large Language Models for
  Medicine},
    author = {Chaoyi Wu, Pengcheng Qiu, Jinxin Liu, Hongfei Gu, Na Li, Ya Zhang, Yanfeng Wang, Weidi Xie},
    abstract = {  In this study, we present MedS-Bench, a comprehensive benchmark designed to
evaluate the performance of large language models (LLMs) in clinical contexts.
Unlike existing benchmarks that focus on multiple-choice question answering,
MedS-Bench spans 11 high-level clinical tasks, including clinical report
summarization, treatment recommendations, diagnosis, named entity recognition,
and medical concept explanation, among others. We evaluated six leading LLMs,
e.g., MEDITRON, Mistral, InternLM 2, Llama 3, GPT-4, and Claude-3.5 using
few-shot prompting, and found that even the most sophisticated models struggle
with these complex tasks. To address these limitations, we developed MedS-Ins,
a large-scale instruction tuning dataset for medicine. MedS-Ins comprises 58
medically oriented language corpora, totaling 13.5 million samples across 122
tasks. To demonstrate the dataset's utility, we conducted a proof-of-concept
experiment by performing instruction tuning on a lightweight, open-source
medical language model. The resulting model, MMedIns-Llama 3, significantly
outperformed existing models across nearly all clinical tasks. To promote
further advancements in the application of LLMs to clinical challenges, we have
made the MedS-Ins dataset fully accessible and invite the research community to
contribute to its expansion.Additionally, we have launched a dynamic
leaderboard for MedS-Bench, which we plan to regularly update the test set to
track progress and enhance the adaptation of general LLMs to the medical
domain. Leaderboard: https://henrychur.github.io/MedS-Bench/. Github:
https://github.com/MAGIC-AI4Med/MedS-Ins.
},
    year = {2024},
    month = {08},
    url = {http://arxiv.org/pdf/2408.12547v2},
    journal = {arXiv preprint}
}

@article{arxiv:ClinicalAgent:_Clinical_Trial_Multi-Agent_System_with_Large_Language
__Model-based_Reasoning,
    title = {ClinicalAgent: Clinical Trial Multi-Agent System with Large Language
  Model-based Reasoning},
    author = {Ling Yue, Sixue Xing, Jintai Chen, Tianfan Fu},
    abstract = {  Large Language Models (LLMs) and multi-agent systems have shown impressive
capabilities in natural language tasks but face challenges in clinical trial
applications, primarily due to limited access to external knowledge.
Recognizing the potential of advanced clinical trial tools that aggregate and
predict based on the latest medical data, we propose an integrated solution to
enhance their accessibility and utility. We introduce Clinical Agent System
(ClinicalAgent), a clinical multi-agent system designed for clinical trial
tasks, leveraging GPT-4, multi-agent architectures, LEAST-TO-MOST, and ReAct
reasoning technology. This integration not only boosts LLM performance in
clinical contexts but also introduces novel functionalities. The proposed
method achieves competitive predictive performance in clinical trial outcome
prediction (0.7908 PR-AUC), obtaining a 0.3326 improvement over the standard
prompt Method. Publicly available code can be found at
https://anonymous.4open.science/r/ClinicalAgent-6671.
},
    year = {2024},
    month = {04},
    url = {http://arxiv.org/pdf/2404.14777v2},
    journal = {arXiv preprint}
}

@article{arxiv:Critique_of_Impure_Reason:_Unveiling_the_reasoning_behaviour_of_medical
__Large_Language_Models,
    title = {Critique of Impure Reason: Unveiling the reasoning behaviour of medical
  Large Language Models},
    author = {Shamus Sim, Tyrone Chen},
    abstract = {  Background: Despite the current ubiquity of Large Language Models (LLMs)
across the medical domain, there is a surprising lack of studies which address
their reasoning behaviour. We emphasise the importance of understanding
reasoning behaviour as opposed to high-level prediction accuracies, since it is
equivalent to explainable AI (XAI) in this context. In particular, achieving
XAI in medical LLMs used in the clinical domain will have a significant impact
across the healthcare sector. Results: Therefore, we define the concept of
reasoning behaviour in the specific context of medical LLMs. We then categorise
and discuss the current state of the art of methods which evaluate reasoning
behaviour in medical LLMs. Finally, we propose theoretical frameworks which can
empower medical professionals or machine learning engineers to gain insight
into the low-level reasoning operations of these previously obscure models.
Conclusion: The subsequent increased transparency and trust in medical machine
learning models by clinicians as well as patients will accelerate the
integration, application as well as further development of medical AI for the
healthcare system as a whole
},
    year = {2024},
    month = {12},
    url = {http://arxiv.org/pdf/2412.15748v1},
    journal = {arXiv preprint}
}

@article{arxiv:Demystifying_Large_Language_Models_for_Medicine:_A_Primer,
    title = {Demystifying Large Language Models for Medicine: A Primer},
    author = {Qiao Jin, Nicholas Wan, Robert Leaman, Shubo Tian, Zhizheng Wang, Yifan Yang, Zifeng Wang, Guangzhi Xiong, Po-Ting Lai, Qingqing Zhu, Benjamin Hou, Maame Sarfo-Gyamfi, Gongbo Zhang, Aidan Gilson, Balu Bhasuran, Zhe He, Aidong Zhang, Jimeng Sun, Chunhua Weng, Ronald M. Summers, Qingyu Chen, Yifan Peng, Zhiyong Lu},
    abstract = {  Large language models (LLMs) represent a transformative class of AI tools
capable of revolutionizing various aspects of healthcare by generating
human-like responses across diverse contexts and adapting to novel tasks
following human instructions. Their potential application spans a broad range
of medical tasks, such as clinical documentation, matching patients to clinical
trials, and answering medical questions. In this primer paper, we propose an
actionable guideline to help healthcare professionals more efficiently utilize
LLMs in their work, along with a set of best practices. This approach consists
of several main phases, including formulating the task, choosing LLMs, prompt
engineering, fine-tuning, and deployment. We start with the discussion of
critical considerations in identifying healthcare tasks that align with the
core capabilities of LLMs and selecting models based on the selected task and
data, performance requirements, and model interface. We then review the
strategies, such as prompt engineering and fine-tuning, to adapt standard LLMs
to specialized medical tasks. Deployment considerations, including regulatory
compliance, ethical guidelines, and continuous monitoring for fairness and
bias, are also discussed. By providing a structured step-by-step methodology,
this tutorial aims to equip healthcare professionals with the tools necessary
to effectively integrate LLMs into clinical practice, ensuring that these
powerful technologies are applied in a safe, reliable, and impactful manner.
},
    year = {2024},
    month = {10},
    url = {http://arxiv.org/pdf/2410.18856v3},
    journal = {arXiv preprint}
}

@article{arxiv:The_Limited_Impact_of_Medical_Adaptation_of_Large_Language_and
__Vision-Language_Models,
    title = {The Limited Impact of Medical Adaptation of Large Language and
  Vision-Language Models},
    author = {Daniel P. Jeong, Pranav Mani, Saurabh Garg, Zachary C. Lipton, Michael Oberst},
    abstract = {  Several recent works seek to adapt general-purpose large language models
(LLMs) and vision-language models (VLMs) for medical applications through
continued pretraining on publicly available biomedical corpora. These works
typically claim that such domain-adaptive pretraining improves performance on
various downstream medical tasks, such as answering medical exam questions. In
this paper, we compare ten "medical" LLMs and two VLMs against their
corresponding base models, arriving at a different conclusion: all medical VLMs
and nearly all medical LLMs fail to consistently improve over their base models
in the zero-/few-shot prompting and supervised fine-tuning regimes for medical
question answering (QA). For instance, on clinical-note-based QA tasks in the
3-shot setting, medical LLMs outperform their base models in only 26.7% of
cases, reach a (statistical) tie in 16.7% of cases, and perform significantly
worse in the remaining 56.7% of cases. Our conclusions are based on (i)
comparing each medical model directly against its base model; (ii) optimizing
the prompts for each model separately in zero-/few-shot prompting; and (iii)
accounting for statistical uncertainty in comparisons. Our findings suggest
that state-of-the-art general-domain models may already exhibit strong medical
knowledge and reasoning capabilities, and offer recommendations to strengthen
the conclusions of future studies.
},
    year = {2024},
    month = {11},
    url = {http://arxiv.org/pdf/2411.08870v2},
    journal = {arXiv preprint}
}

@article{arxiv:Evaluating_the_Impact_of_a_Specialized_LLM_on_Physician_Experience_in
__Clinical_Decision_Support:_A_Comparison_of_Ask_Avo_and_ChatGPT-4,
    title = {Evaluating the Impact of a Specialized LLM on Physician Experience in
  Clinical Decision Support: A Comparison of Ask Avo and ChatGPT-4},
    author = {Daniel Jung, Alex Butler, Joongheum Park, Yair Saperstein},
    abstract = {  The use of Large language models (LLMs) to augment clinical decision support
systems is a topic with rapidly growing interest, but current shortcomings such
as hallucinations and lack of clear source citations make them unreliable for
use in the clinical environment. This study evaluates Ask Avo, an LLM-derived
software by AvoMD that incorporates a proprietary Language Model Augmented
Retrieval (LMAR) system, in-built visual citation cues, and prompt engineering
designed for interactions with physicians, against ChatGPT-4 in end-user
experience for physicians in a simulated clinical scenario environment. Eight
clinical questions derived from medical guideline documents in various
specialties were prompted to both models by 62 study participants, with each
response rated on trustworthiness, actionability, relevancy, comprehensiveness,
and friendly format from 1 to 5. Ask Avo significantly outperformed ChatGPT-4
in all criteria: trustworthiness (4.52 vs. 3.34, p<0.001), actionability (4.41
vs. 3.19, p<0.001), relevancy (4.55 vs. 3.49, p<0.001), comprehensiveness (4.50
vs. 3.37, p<0.001), and friendly format (4.52 vs. 3.60, p<0.001). Our findings
suggest that specialized LLMs designed with the needs of clinicians in mind can
offer substantial improvements in user experience over general-purpose LLMs.
Ask Avo's evidence-based approach tailored to clinician needs shows promise in
the adoption of LLM-augmented clinical decision support software.
},
    year = {2024},
    month = {09},
    url = {http://arxiv.org/pdf/2409.15326v1},
    journal = {arXiv preprint}
}

@article{arxiv:Caught_in_the_Web_of_Words:_Do_LLMs_Fall_for_Spin_in_Medical_Literature?,
    title = {Caught in the Web of Words: Do LLMs Fall for Spin in Medical Literature?},
    author = {Hye Sun Yun, Karen Y. C. Zhang, Ramez Kouzy, Iain J. Marshall, Junyi Jessy Li, Byron C. Wallace},
    abstract = {  Medical research faces well-documented challenges in translating novel
treatments into clinical practice. Publishing incentives encourage researchers
to present "positive" findings, even when empirical results are equivocal.
Consequently, it is well-documented that authors often spin study results,
especially in article abstracts. Such spin can influence clinician
interpretation of evidence and may affect patient care decisions. In this
study, we ask whether the interpretation of trial results offered by Large
Language Models (LLMs) is similarly affected by spin. This is important since
LLMs are increasingly being used to trawl through and synthesize published
medical evidence. We evaluated 22 LLMs and found that they are across the board
more susceptible to spin than humans. They might also propagate spin into their
outputs: We find evidence, e.g., that LLMs implicitly incorporate spin into
plain language summaries that they generate. We also find, however, that LLMs
are generally capable of recognizing spin, and can be prompted in a way to
mitigate spin's impact on LLM outputs.
},
    year = {2025},
    month = {02},
    url = {http://arxiv.org/pdf/2502.07963v1},
    journal = {arXiv preprint}
}

@article{arxiv:Structured_Extraction_of_Real_World_Medical_Knowledge_using_LLMs_for
__Summarization_and_Search,
    title = {Structured Extraction of Real World Medical Knowledge using LLMs for
  Summarization and Search},
    author = {Edward Kim, Manil Shrestha, Richard Foty, Tom DeLay, Vicki Seyfert-Margolis},
    abstract = {  Creation and curation of knowledge graphs can accelerate disease discovery
and analysis in real-world data. While disease ontologies aid in biological
data annotation, codified categories (SNOMED-CT, ICD10, CPT) may not capture
patient condition nuances or rare diseases. Multiple disease definitions across
data sources complicate ontology mapping and disease clustering. We propose
creating patient knowledge graphs using large language model extraction
techniques, allowing data extraction via natural language rather than rigid
ontological hierarchies. Our method maps to existing ontologies (MeSH,
SNOMED-CT, RxNORM, HPO) to ground extracted entities.
  Using a large ambulatory care EHR database with 33.6M patients, we
demonstrate our method through the patient search for Dravet syndrome, which
received ICD10 recognition in October 2020. We describe our construction of
patient-specific knowledge graphs and symptom-based patient searches. Using
confirmed Dravet syndrome ICD10 codes as ground truth, we employ LLM-based
entity extraction to characterize patients in grounded ontologies. We then
apply this method to identify Beta-propeller protein-associated
neurodegeneration (BPAN) patients, demonstrating real-world discovery where no
ground truth exists.
},
    year = {2024},
    month = {12},
    url = {http://arxiv.org/pdf/2412.15256v1},
    journal = {arXiv preprint}
}

@article{arxiv:IryoNLP_at_MEDIQA-CORR_2024:_Tackling_the_Medical_Error_Detection_&
__Correction_Task_On_the_Shoulders_of_Medical_Agents,
    title = {IryoNLP at MEDIQA-CORR 2024: Tackling the Medical Error Detection &
  Correction Task On the Shoulders of Medical Agents},
    author = {Jean-Philippe Corbeil},
    abstract = {  In natural language processing applied to the clinical domain, utilizing
large language models has emerged as a promising avenue for error detection and
correction on clinical notes, a knowledge-intensive task for which annotated
data is scarce. This paper presents MedReAct'N'MedReFlex, which leverages a
suite of four LLM-based medical agents. The MedReAct agent initiates the
process by observing, analyzing, and taking action, generating trajectories to
guide the search to target a potential error in the clinical notes.
Subsequently, the MedEval agent employs five evaluators to assess the targeted
error and the proposed correction. In cases where MedReAct's actions prove
insufficient, the MedReFlex agent intervenes, engaging in reflective analysis
and proposing alternative strategies. Finally, the MedFinalParser agent formats
the final output, preserving the original style while ensuring the integrity of
the error correction process. One core component of our method is our RAG
pipeline based on our ClinicalCorp corpora. Among other well-known sources
containing clinical guidelines and information, we preprocess and release the
open-source MedWiki dataset for clinical RAG application. Our results
demonstrate the central role of our RAG approach with ClinicalCorp leveraged
through the MedReAct'N'MedReFlex framework. It achieved the ninth rank on the
MEDIQA-CORR 2024 final leaderboard.
},
    year = {2024},
    month = {04},
    url = {http://arxiv.org/pdf/2404.15488v1},
    journal = {arXiv preprint}
}

@article{arxiv:KEDRec-LM:_A_Knowledge-distilled_Explainable_Drug_Recommendation_Large
__Language_Model,
    title = {KEDRec-LM: A Knowledge-distilled Explainable Drug Recommendation Large
  Language Model},
    author = {Kai Zhang, Rui Zhu, Shutian Ma, Jingwei Xiong, Yejin Kim, Fabricio Murai, Xiaozhong Liu},
    abstract = {  Drug discovery is a critical task in biomedical natural language processing
(NLP), yet explainable drug discovery remains underexplored. Meanwhile, large
language models (LLMs) have shown remarkable abilities in natural language
understanding and generation. Leveraging LLMs for explainable drug discovery
has the potential to improve downstream tasks and real-world applications. In
this study, we utilize open-source drug knowledge graphs, clinical trial data,
and PubMed publications to construct a comprehensive dataset for the
explainable drug discovery task, named \textbf{expRxRec}. Furthermore, we
introduce \textbf{KEDRec-LM}, an instruction-tuned LLM which distills knowledge
from rich medical knowledge corpus for drug recommendation and rationale
generation. To encourage further research in this area, we will publicly
release\footnote{A copy is attached with this submission} both the dataset and
KEDRec-LM.
},
    year = {2025},
    month = {02},
    url = {http://arxiv.org/pdf/2502.20350v1},
    journal = {arXiv preprint}
}

@article{arxiv:Advice_for_Diabetes_Self-Management_by_ChatGPT_Models:_Challenges_and
__Recommendations,
    title = {Advice for Diabetes Self-Management by ChatGPT Models: Challenges and
  Recommendations},
    author = {Waqar Hussain, John Grundy},
    abstract = {  Given their ability for advanced reasoning, extensive contextual
understanding, and robust question-answering abilities, large language models
have become prominent in healthcare management research. Despite adeptly
handling a broad spectrum of healthcare inquiries, these models face
significant challenges in delivering accurate and practical advice for chronic
conditions such as diabetes. We evaluate the responses of ChatGPT versions 3.5
and 4 to diabetes patient queries, assessing their depth of medical knowledge
and their capacity to deliver personalized, context-specific advice for
diabetes self-management. Our findings reveal discrepancies in accuracy and
embedded biases, emphasizing the models' limitations in providing tailored
advice unless activated by sophisticated prompting techniques. Additionally, we
observe that both models often provide advice without seeking necessary
clarification, a practice that can result in potentially dangerous advice. This
underscores the limited practical effectiveness of these models without human
oversight in clinical settings. To address these issues, we propose a
commonsense evaluation layer for prompt evaluation and incorporating
disease-specific external memory using an advanced Retrieval Augmented
Generation technique. This approach aims to improve information quality and
reduce misinformation risks, contributing to more reliable AI applications in
healthcare settings. Our findings seek to influence the future direction of AI
in healthcare, enhancing both the scope and quality of its integration.
},
    year = {2025},
    month = {01},
    url = {http://arxiv.org/pdf/2501.07931v1},
    journal = {arXiv preprint}
}

@article{arxiv:WangLab_at_MEDIQA-CORR_2024:_Optimized_LLM-based_Programs_for_Medical
__Error_Detection_and_Correction,
    title = {WangLab at MEDIQA-CORR 2024: Optimized LLM-based Programs for Medical
  Error Detection and Correction},
    author = {Augustin Toma, Ronald Xie, Steven Palayew, Patrick R. Lawler, Bo Wang},
    abstract = {  Medical errors in clinical text pose significant risks to patient safety. The
MEDIQA-CORR 2024 shared task focuses on detecting and correcting these errors
across three subtasks: identifying the presence of an error, extracting the
erroneous sentence, and generating a corrected sentence. In this paper, we
present our approach that achieved top performance in all three subtasks. For
the MS dataset, which contains subtle errors, we developed a retrieval-based
system leveraging external medical question-answering datasets. For the UW
dataset, reflecting more realistic clinical notes, we created a pipeline of
modules to detect, localize, and correct errors. Both approaches utilized the
DSPy framework for optimizing prompts and few-shot examples in large language
model (LLM) based programs. Our results demonstrate the effectiveness of LLM
based programs for medical error correction. However, our approach has
limitations in addressing the full diversity of potential errors in medical
documentation. We discuss the implications of our work and highlight future
research directions to advance the robustness and applicability of medical
error detection and correction systems.
},
    year = {2024},
    month = {04},
    url = {http://arxiv.org/pdf/2404.14544v1},
    journal = {arXiv preprint}
}

@article{arxiv:Can_LLMs_Correct_Physicians,_Yet?_Investigating_Effective_Interaction
__Methods_in_the_Medical_Domain,
    title = {Can LLMs Correct Physicians, Yet? Investigating Effective Interaction
  Methods in the Medical Domain},
    author = {Burcu Sayin, Pasquale Minervini, Jacopo Staiano, Andrea Passerini},
    abstract = {  We explore the potential of Large Language Models (LLMs) to assist and
potentially correct physicians in medical decision-making tasks. We evaluate
several LLMs, including Meditron, Llama2, and Mistral, to analyze the ability
of these models to interact effectively with physicians across different
scenarios. We consider questions from PubMedQA and several tasks, ranging from
binary (yes/no) responses to long answer generation, where the answer of the
model is produced after an interaction with a physician. Our findings suggest
that prompt design significantly influences the downstream accuracy of LLMs and
that LLMs can provide valuable feedback to physicians, challenging incorrect
diagnoses and contributing to more accurate decision-making. For example, when
the physician is accurate 38% of the time, Mistral can produce the correct
answer, improving accuracy up to 74% depending on the prompt being used, while
Llama2 and Meditron models exhibit greater sensitivity to prompt choice. Our
analysis also uncovers the challenges of ensuring that LLM-generated
suggestions are pertinent and useful, emphasizing the need for further research
in this area.
},
    year = {2024},
    month = {03},
    url = {http://arxiv.org/pdf/2403.20288v2},
    journal = {arXiv preprint}
}

@article{arxiv:Superhuman_performance_of_a_large_language_model_on_the_reasoning_tasks
__of_a_physician,
    title = {Superhuman performance of a large language model on the reasoning tasks
  of a physician},
    author = {Peter G. Brodeur, Thomas A. Buckley, Zahir Kanjee, Ethan Goh, Evelyn Bin Ling, Priyank Jain, Stephanie Cabral, Raja-Elie Abdulnour, Adrian Haimovich, Jason A. Freed, Andrew Olson, Daniel J. Morgan, Jason Hom, Robert Gallo, Eric Horvitz, Jonathan Chen, Arjun K. Manrai, Adam Rodman},
    abstract = {  Performance of large language models (LLMs) on medical tasks has
traditionally been evaluated using multiple choice question benchmarks.
However, such benchmarks are highly constrained, saturated with repeated
impressive performance by LLMs, and have an unclear relationship to performance
in real clinical scenarios. Clinical reasoning, the process by which physicians
employ critical thinking to gather and synthesize clinical data to diagnose and
manage medical problems, remains an attractive benchmark for model performance.
Prior LLMs have shown promise in outperforming clinicians in routine and
complex diagnostic scenarios. We sought to evaluate OpenAI's o1-preview model,
a model developed to increase run-time via chain of thought processes prior to
generating a response. We characterize the performance of o1-preview with five
experiments including differential diagnosis generation, display of diagnostic
reasoning, triage differential diagnosis, probabilistic reasoning, and
management reasoning, adjudicated by physician experts with validated
psychometrics. Our primary outcome was comparison of the o1-preview output to
identical prior experiments that have historical human controls and benchmarks
of previous LLMs. Significant improvements were observed with differential
diagnosis generation and quality of diagnostic and management reasoning. No
improvements were observed with probabilistic reasoning or triage differential
diagnosis. This study highlights o1-preview's ability to perform strongly on
tasks that require complex critical thinking such as diagnosis and management
while its performance on probabilistic reasoning tasks was similar to past
models. New robust benchmarks and scalable evaluation of LLM capabilities
compared to human physicians are needed along with trials evaluating AI in real
clinical settings.
},
    year = {2024},
    month = {12},
    url = {http://arxiv.org/pdf/2412.10849v1},
    journal = {arXiv preprint}
}

@article{arxiv:CLEFT:_Language-Image_Contrastive_Learning_with_Efficient_Large_Language
__Model_and_Prompt_Fine-Tuning,
    title = {CLEFT: Language-Image Contrastive Learning with Efficient Large Language
  Model and Prompt Fine-Tuning},
    author = {Yuexi Du, Brian Chang, Nicha C. Dvornek},
    abstract = {  Recent advancements in Contrastive Language-Image Pre-training (CLIP) have
demonstrated notable success in self-supervised representation learning across
various tasks. However, the existing CLIP-like approaches often demand
extensive GPU resources and prolonged training times due to the considerable
size of the model and dataset, making them poor for medical applications, in
which large datasets are not always common. Meanwhile, the language model
prompts are mainly manually derived from labels tied to images, potentially
overlooking the richness of information within training samples. We introduce a
novel language-image Contrastive Learning method with an Efficient large
language model and prompt Fine-Tuning (CLEFT) that harnesses the strengths of
the extensive pre-trained language and visual models. Furthermore, we present
an efficient strategy for learning context-based prompts that mitigates the gap
between informative clinical diagnostic data and simple class labels. Our
method demonstrates state-of-the-art performance on multiple chest X-ray and
mammography datasets compared with various baselines. The proposed parameter
efficient framework can reduce the total trainable model size by 39% and reduce
the trainable language model to only 4% compared with the current BERT encoder.
},
    year = {2024},
    month = {07},
    url = {http://arxiv.org/pdf/2407.21011v1},
    journal = {arXiv preprint}
}

@article{arxiv:MedPromptExtract_(Medical_Data_Extraction_Tool):_Anonymization_and
__Hi-fidelity_Automated_data_extraction_using_NLP_and_prompt_engineering,
    title = {MedPromptExtract (Medical Data Extraction Tool): Anonymization and
  Hi-fidelity Automated data extraction using NLP and prompt engineering},
    author = {Roomani Srivastava, Suraj Prasad, Lipika Bhat, Sarvesh Deshpande, Barnali Das, Kshitij Jadhav},
    abstract = {  Introduction: The labour-intensive nature of data extraction from sources
like discharge summaries (DS) poses significant obstacles to the digitisation
of medical records particularly for low- and middle-income countries (LMICs).
In this paper we present a completely automated method MedPromptExtract to
efficiently extract data from DS while maintaining confidentiality. Methods:
The source of data was Discharge Summaries (DS) from Kokilaben Dhirubhai Ambani
Hospital (KDAH) of patients having Acute Kidney Injury (AKI). A pre-existing
tool EIGEN which leverages semi-supervised learning techniques for
high-fidelity information extraction was used to anonymize the DS, Natural
Language Processing (NLP) was used to extract data from regular fields. We used
Prompt Engineering and Large Language Model(LLM) to extract custom clinical
information from free flowing text describing the patients stay in the
hospital. Twelve features associated with occurrence of AKI were extracted. The
LLM responses were validated against clinicians annotations. Results: The
MedPromptExtracttool first subjected DS to the anonymization pipeline which
took three seconds per summary. Successful anonymization was verified by
clinicians, thereafter NLP pipeline extracted structured text from the
anonymized pdfs at the rate of 0.2 seconds per summary with 100%
accuracy.Finally DS were analysed by the LLM pipeline using Gemini Pro for the
twelve features. Accuracy metrics were calculated by comparing model responses
to clinicians annotations with seven features achieving AUCs above 0.9,
indicating high fidelity of the extraction process. Conclusion:
MedPromptExtract serves as an automated adaptable tool for efficient data
extraction from medical records with a dynamic user interface. Keywords:
Digitizing Medical Records, Automated Anonymisation, Information Retrieval,
Large Language Models, Prompt Engineering
},
    year = {2024},
    month = {05},
    url = {http://arxiv.org/pdf/2405.02664v3},
    journal = {arXiv preprint}
}

@article{arxiv:Rx_Strategist:_Prescription_Verification_using_LLM_Agents_System,
    title = {Rx Strategist: Prescription Verification using LLM Agents System},
    author = {Phuc Phan Van, Dat Nguyen Minh, An Dinh Ngoc, Huy Phan Thanh},
    abstract = {  To protect patient safety, modern pharmaceutical complexity demands strict
prescription verification. We offer a new approach - Rx Strategist - that makes
use of knowledge graphs and different search strategies to enhance the power of
Large Language Models (LLMs) inside an agentic framework. This multifaceted
technique allows for a multi-stage LLM pipeline and reliable information
retrieval from a custom-built active ingredient database. Different facets of
prescription verification, such as indication, dose, and possible drug
interactions, are covered in each stage of the pipeline. We alleviate the
drawbacks of monolithic LLM techniques by spreading reasoning over these
stages, improving correctness and reliability while reducing memory demands.
Our findings demonstrate that Rx Strategist surpasses many current LLMs,
achieving performance comparable to that of a highly experienced clinical
pharmacist. In the complicated world of modern medications, this combination of
LLMs with organized knowledge and sophisticated search methods presents a
viable avenue for reducing prescription errors and enhancing patient outcomes.
},
    year = {2024},
    month = {09},
    url = {http://arxiv.org/pdf/2409.03440v1},
    journal = {arXiv preprint}
}

@article{arxiv:Medchain:_Bridging_the_Gap_Between_LLM_Agents_and_Clinical_Practice
__through_Interactive_Sequential_Benchmarking,
    title = {Medchain: Bridging the Gap Between LLM Agents and Clinical Practice
  through Interactive Sequential Benchmarking},
    author = {Jie Liu, Wenxuan Wang, Zizhan Ma, Guolin Huang, Yihang SU, Kao-Jung Chang, Wenting Chen, Haoliang Li, Linlin Shen, Michael Lyu},
    abstract = {  Clinical decision making (CDM) is a complex, dynamic process crucial to
healthcare delivery, yet it remains a significant challenge for artificial
intelligence systems. While Large Language Model (LLM)-based agents have been
tested on general medical knowledge using licensing exams and knowledge
question-answering tasks, their performance in the CDM in real-world scenarios
is limited due to the lack of comprehensive testing datasets that mirror actual
medical practice. To address this gap, we present MedChain, a dataset of 12,163
clinical cases that covers five key stages of clinical workflow. MedChain
distinguishes itself from existing benchmarks with three key features of
real-world clinical practice: personalization, interactivity, and
sequentiality. Further, to tackle real-world CDM challenges, we also propose
MedChain-Agent, an AI system that integrates a feedback mechanism and a
MCase-RAG module to learn from previous cases and adapt its responses.
MedChain-Agent demonstrates remarkable adaptability in gathering information
dynamically and handling sequential clinical tasks, significantly outperforming
existing approaches. The relevant dataset and code will be released upon
acceptance of this paper.
},
    year = {2024},
    month = {12},
    url = {http://arxiv.org/pdf/2412.01605v1},
    journal = {arXiv preprint}
}

@article{arxiv:ChatGPT-HealthPrompt._Harnessing_the_Power_of_XAI_in_Prompt-Based
__Healthcare_Decision_Support_using_ChatGPT,
    title = {ChatGPT-HealthPrompt. Harnessing the Power of XAI in Prompt-Based
  Healthcare Decision Support using ChatGPT},
    author = {Fatemeh Nazary, Yashar Deldjoo, Tommaso Di Noia},
    abstract = {  This study presents an innovative approach to the application of large
language models (LLMs) in clinical decision-making, focusing on OpenAI's
ChatGPT. Our approach introduces the use of contextual prompts-strategically
designed to include task description, feature description, and crucially,
integration of domain knowledge-for high-quality binary classification tasks
even in data-scarce scenarios. The novelty of our work lies in the utilization
of domain knowledge, obtained from high-performing interpretable ML models, and
its seamless incorporation into prompt design. By viewing these ML models as
medical experts, we extract key insights on feature importance to aid in
decision-making processes. This interplay of domain knowledge and AI holds
significant promise in creating a more insightful diagnostic tool.
  Additionally, our research explores the dynamics of zero-shot and few-shot
prompt learning based on LLMs. By comparing the performance of OpenAI's ChatGPT
with traditional supervised ML models in different data conditions, we aim to
provide insights into the effectiveness of prompt engineering strategies under
varied data availability. In essence, this paper bridges the gap between AI and
healthcare, proposing a novel methodology for LLMs application in clinical
decision support systems. It highlights the transformative potential of
effective prompt design, domain knowledge integration, and flexible learning
approaches in enhancing automated decision-making.
},
    year = {2023},
    month = {08},
    url = {http://arxiv.org/pdf/2308.09731v1},
    journal = {arXiv preprint}
}

@article{arxiv:Large_Language_Models_Encode_Clinical_Knowledge,
    title = {Large Language Models Encode Clinical Knowledge},
    author = {Karan Singhal, Shekoofeh Azizi, Tao Tu, S. Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, Perry Payne, Martin Seneviratne, Paul Gamble, Chris Kelly, Nathaneal Scharli, Aakanksha Chowdhery, Philip Mansfield, Blaise Aguera y Arcas, Dale Webster, Greg S. Corrado, Yossi Matias, Katherine Chou, Juraj Gottweis, Nenad Tomasev, Yun Liu, Alvin Rajkomar, Joelle Barral, Christopher Semturs, Alan Karthikesalingam, Vivek Natarajan},
    abstract = {  Large language models (LLMs) have demonstrated impressive capabilities in
natural language understanding and generation, but the quality bar for medical
and clinical applications is high. Today, attempts to assess models' clinical
knowledge typically rely on automated evaluations on limited benchmarks. There
is no standard to evaluate model predictions and reasoning across a breadth of
tasks. To address this, we present MultiMedQA, a benchmark combining six
existing open question answering datasets spanning professional medical exams,
research, and consumer queries; and HealthSearchQA, a new free-response dataset
of medical questions searched online. We propose a framework for human
evaluation of model answers along multiple axes including factuality,
precision, possible harm, and bias. In addition, we evaluate PaLM (a
540-billion parameter LLM) and its instruction-tuned variant, Flan-PaLM, on
MultiMedQA. Using a combination of prompting strategies, Flan-PaLM achieves
state-of-the-art accuracy on every MultiMedQA multiple-choice dataset (MedQA,
MedMCQA, PubMedQA, MMLU clinical topics), including 67.6% accuracy on MedQA (US
Medical License Exam questions), surpassing prior state-of-the-art by over 17%.
However, human evaluation reveals key gaps in Flan-PaLM responses. To resolve
this we introduce instruction prompt tuning, a parameter-efficient approach for
aligning LLMs to new domains using a few exemplars. The resulting model,
Med-PaLM, performs encouragingly, but remains inferior to clinicians. We show
that comprehension, recall of knowledge, and medical reasoning improve with
model scale and instruction prompt tuning, suggesting the potential utility of
LLMs in medicine. Our human evaluations reveal important limitations of today's
models, reinforcing the importance of both evaluation frameworks and method
development in creating safe, helpful LLM models for clinical applications.
},
    year = {2022},
    month = {12},
    url = {http://arxiv.org/pdf/2212.13138v1},
    journal = {arXiv preprint}
}

@article{arxiv:The_use_of_large_language_models_to_enhance_cancer_clinical_trial
__educational_materials,
    title = {The use of large language models to enhance cancer clinical trial
  educational materials},
    author = {Mingye Gao, Aman Varshney, Shan Chen, Vikram Goddla, Jack Gallifant, Patrick Doyle, Claire Novack, Maeve Dillon-Martin, Teresia Perkins, Xinrong Correia, Erik Duhaime, Howard Isenstein, Elad Sharon, Lisa Soleymani Lehmann, David Kozono, Brian Anthony, Dmitriy Dligach, Danielle S. Bitterman},
    abstract = {  Cancer clinical trials often face challenges in recruitment and engagement
due to a lack of participant-facing informational and educational resources.
This study investigated the potential of Large Language Models (LLMs),
specifically GPT4, in generating patient-friendly educational content from
clinical trial informed consent forms. Using data from ClinicalTrials.gov, we
employed zero-shot learning for creating trial summaries and one-shot learning
for developing multiple-choice questions, evaluating their effectiveness
through patient surveys and crowdsourced annotation. Results showed that
GPT4-generated summaries were both readable and comprehensive, and may improve
patients' understanding and interest in clinical trials. The multiple-choice
questions demonstrated high accuracy and agreement with crowdsourced
annotators. For both resource types, hallucinations were identified that
require ongoing human oversight. The findings demonstrate the potential of LLMs
"out-of-the-box" to support the generation of clinical trial education
materials with minimal trial-specific engineering, but implementation with a
human-in-the-loop is still needed to avoid misinformation risks.
},
    year = {2024},
    month = {12},
    url = {http://arxiv.org/pdf/2412.01955v2},
    journal = {arXiv preprint}
}

@article{arxiv:An_adapted_large_language_model_facilitates_multiple_medical_tasks_in
__diabetes_care,
    title = {An adapted large language model facilitates multiple medical tasks in
  diabetes care},
    author = {Lai Wei, Zhen Ying, Muyang He, Yutong Chen, Qian Yang, Yanzhe Hong, Jiaping Lu, Xiaoying Li, Weiran Huang, Ying Chen},
    abstract = {  Diabetes is a chronic disease that poses a significant global health burden,
and optimizing diabetes management requires multi-stakeholder collaboration.
Large language models (LLMs) have shown promise in various healthcare
scenarios, but their effectiveness across a diverse range of diabetes tasks
remains unproven. In this study, we introduced a framework to train and
validate diabetes-specific LLMs. We first developed a comprehensive data
processing pipeline that includes data collection, filtering, augmentation and
refinement. This approach contributes to creating a high-quality,
diabetes-specific dataset, and several evaluation benchmarks entirely from
scratch. Utilizing the collected training dataset, we fine-tuned a
diabetes-specific LLM family that demonstrated state-of-the-art proficiency in
understanding and processing various diabetes tasks compared to other LLMs.
Furthermore, clinical studies showed the potential applications of our models
in diabetes care, including providing personalized healthcare, assisting
medical education, and streamlining clinical tasks. In conclusion, our study
introduced a framework to develop and evaluate a diabetes-specific LLM family,
and highlighted its potential to enhance clinical practice and provide
personalized, data-driven support for diabetes support when facing different
end users. The code is provided via GitHub at
https://github.com/waltonfuture/Diabetica.
},
    year = {2024},
    month = {09},
    url = {http://arxiv.org/pdf/2409.13191v1},
    journal = {arXiv preprint}
}

@article{arxiv:MedG-KRP:_Medical_Graph_Knowledge_Representation_Probing,
    title = {MedG-KRP: Medical Graph Knowledge Representation Probing},
    author = {Gabriel R. Rosenbaum, Lavender Yao Jiang, Ivaxi Sheth, Jaden Stryker, Anton Alyakin, Daniel Alexander Alber, Nicolas K. Goff, Young Joon Fred Kwon, John Markert, Mustafa Nasir-Moin, Jan Moritz Niehues, Karl L. Sangwon, Eunice Yang, Eric Karl Oermann},
    abstract = {  Large language models (LLMs) have recently emerged as powerful tools, finding
many medical applications. LLMs' ability to coalesce vast amounts of
information from many sources to generate a response-a process similar to that
of a human expert-has led many to see potential in deploying LLMs for clinical
use. However, medicine is a setting where accurate reasoning is paramount. Many
researchers are questioning the effectiveness of multiple choice question
answering (MCQA) benchmarks, frequently used to test LLMs. Researchers and
clinicians alike must have complete confidence in LLMs' abilities for them to
be deployed in a medical setting. To address this need for understanding, we
introduce a knowledge graph (KG)-based method to evaluate the biomedical
reasoning abilities of LLMs. Essentially, we map how LLMs link medical concepts
in order to better understand how they reason. We test GPT-4, Llama3-70b, and
PalmyraMed-70b, a specialized medical model. We enlist a panel of medical
students to review a total of 60 LLM-generated graphs and compare these graphs
to BIOS, a large biomedical KG. We observe GPT-4 to perform best in our human
review but worst in our ground truth comparison; vice-versa with PalmyraMed,
the medical model. Our work provides a means of visualizing the medical
reasoning pathways of LLMs so they can be implemented in clinical settings
safely and effectively.
},
    year = {2024},
    month = {12},
    url = {http://arxiv.org/pdf/2412.10982v2},
    journal = {arXiv preprint}
}

@article{arxiv:Med-R$^2$:_Crafting_Trustworthy_LLM_Physicians_through_Retrieval_and
__Reasoning_of_Evidence-Based_Medicine,
    title = {Med-R$^2$: Crafting Trustworthy LLM Physicians through Retrieval and
  Reasoning of Evidence-Based Medicine},
    author = {Keer Lu, Zheng Liang, Da Pan, Shusen Zhang, Xin Wu, Weipeng Chen, Zenan Zhou, Guosheng Dong, Bin Cui, Wentao Zhang},
    abstract = {  In recent years, Large Language Models (LLMs) have exhibited remarkable
capabilities in clinical scenarios. However, despite their potential, existing
works face challenges when applying LLMs to medical settings. Strategies
relying on training with medical datasets are highly cost-intensive and may
suffer from outdated training data. Leveraging external knowledge bases is a
suitable alternative, yet it faces obstacles such as limited retrieval
precision and poor effectiveness in answer extraction. These issues
collectively prevent LLMs from demonstrating the expected level of proficiency
in mastering medical expertise. To address these challenges, we introduce
Med-R^2, a novel LLM physician framework that adheres to the Evidence-Based
Medicine (EBM) process, efficiently integrating retrieval mechanisms as well as
the selection and reasoning processes of evidence, thereby enhancing the
problem-solving capabilities of LLMs in healthcare scenarios and fostering a
trustworthy LLM physician. Our comprehensive experiments indicate that Med-R^2
achieves a 14.87\% improvement over vanilla RAG methods and even a 3.59\%
enhancement compared to fine-tuning strategies, without incurring additional
training costs.
},
    year = {2025},
    month = {01},
    url = {http://arxiv.org/pdf/2501.11885v3},
    journal = {arXiv preprint}
}

@article{arxiv:The_Capability_of_Large_Language_Models_to_Measure_Psychiatric
__Functioning,
    title = {The Capability of Large Language Models to Measure Psychiatric
  Functioning},
    author = {Isaac R. Galatzer-Levy, Daniel McDuff, Vivek Natarajan, Alan Karthikesalingam, Matteo Malgaroli},
    abstract = {  The current work investigates the capability of Large language models (LLMs)
that are explicitly trained on large corpuses of medical knowledge (Med-PaLM 2)
to predict psychiatric functioning from patient interviews and clinical
descriptions without being trained to do so. To assess this, n = 145 depression
and n =115 PTSD assessments and n = 46 clinical case studies across high
prevalence/high comorbidity disorders (Depressive, Anxiety, Psychotic, trauma
and stress, Addictive disorders) were analyzed using prompts to extract
estimated clinical scores and diagnoses. Results demonstrate that Med-PaLM 2 is
capable of assessing psychiatric functioning across a range of psychiatric
conditions with the strongest performance being the prediction of depression
scores based on standardized assessments (Accuracy range= 0.80 - 0.84) which
were statistically indistinguishable from human clinical raters t(1,144) =
1.20; p = 0.23. Results show the potential for general clinical language models
to flexibly predict psychiatric risk based on free descriptions of functioning
from both patients and clinicians.
},
    year = {2023},
    month = {08},
    url = {http://arxiv.org/pdf/2308.01834v1},
    journal = {arXiv preprint}
}

@article{arxiv:Enhancing_Medical_Task_Performance_in_GPT-4V:_A_Comprehensive_Study_on
__Prompt_Engineering_Strategies,
    title = {Enhancing Medical Task Performance in GPT-4V: A Comprehensive Study on
  Prompt Engineering Strategies},
    author = {Pengcheng Chen, Ziyan Huang, Zhongying Deng, Tianbin Li, Yanzhou Su, Haoyu Wang, Jin Ye, Yu Qiao, Junjun He},
    abstract = {  OpenAI's latest large vision-language model (LVLM), GPT-4V(ision), has piqued
considerable interest for its potential in medical applications. Despite its
promise, recent studies and internal reviews highlight its underperformance in
specialized medical tasks. This paper explores the boundary of GPT-4V's
capabilities in medicine, particularly in processing complex imaging data from
endoscopies, CT scans, and MRIs etc. Leveraging open-source datasets, we
assessed its foundational competencies, identifying substantial areas for
enhancement. Our research emphasizes prompt engineering, an often-underutilized
strategy for improving AI responsiveness. Through iterative testing, we refined
the model's prompts, significantly improving its interpretative accuracy and
relevance in medical imaging. From our comprehensive evaluations, we distilled
10 effective prompt engineering techniques, each fortifying GPT-4V's medical
acumen. These methodical enhancements facilitate more reliable, precise, and
clinically valuable insights from GPT-4V, advancing its operability in critical
healthcare environments. Our findings are pivotal for those employing AI in
medicine, providing clear, actionable guidance on harnessing GPT-4V's full
diagnostic potential.
},
    year = {2023},
    month = {12},
    url = {http://arxiv.org/pdf/2312.04344v2},
    journal = {arXiv preprint}
}

@article{arxiv:Advancing_Healthcare_Automation:_Multi-Agent_System_for_Medical
__Necessity_Justification,
    title = {Advancing Healthcare Automation: Multi-Agent System for Medical
  Necessity Justification},
    author = {Himanshu Pandey, Akhil Amod,  Shivang},
    abstract = {  Prior Authorization delivers safe, appropriate, and cost-effective care that
is medically justified with evidence-based guidelines. However, the process
often requires labor-intensive manual comparisons between patient medical
records and clinical guidelines, that is both repetitive and time-consuming.
Recent developments in Large Language Models (LLMs) have shown potential in
addressing complex medical NLP tasks with minimal supervision. This paper
explores the application of Multi-Agent System (MAS) that utilize specialized
LLM agents to automate Prior Authorization task by breaking them down into
simpler and manageable sub-tasks. Our study systematically investigates the
effects of various prompting strategies on these agents and benchmarks the
performance of different LLMs. We demonstrate that GPT-4 achieves an accuracy
of 86.2% in predicting checklist item-level judgments with evidence, and 95.6%
in determining overall checklist judgment. Additionally, we explore how these
agents can contribute to explainability of steps taken in the process, thereby
enhancing trust and transparency in the system.
},
    year = {2024},
    month = {04},
    url = {http://arxiv.org/pdf/2404.17977v2},
    journal = {arXiv preprint}
}

@article{arxiv:Med42-v2:_A_Suite_of_Clinical_LLMs,
    title = {Med42-v2: A Suite of Clinical LLMs},
    author = {Cl√©ment Christophe, Praveen K Kanithi, Tathagata Raha, Shadab Khan, Marco AF Pimentel},
    abstract = {  Med42-v2 introduces a suite of clinical large language models (LLMs) designed
to address the limitations of generic models in healthcare settings. These
models are built on Llama3 architecture and fine-tuned using specialized
clinical data. They underwent multi-stage preference alignment to effectively
respond to natural prompts. While generic models are often preference-aligned
to avoid answering clinical queries as a precaution, Med42-v2 is specifically
trained to overcome this limitation, enabling its use in clinical settings.
Med42-v2 models demonstrate superior performance compared to the original
Llama3 models in both 8B and 70B parameter configurations and GPT-4 across
various medical benchmarks. These LLMs are developed to understand clinical
queries, perform reasoning tasks, and provide valuable assistance in clinical
environments. The models are now publicly available at
\href{https://huggingface.co/m42-health}{https://huggingface.co/m42-health}.
},
    year = {2024},
    month = {08},
    url = {http://arxiv.org/pdf/2408.06142v1},
    journal = {arXiv preprint}
}

@article{arxiv:Harnessing_the_Power_of_Semi-Structured_Knowledge_and_LLMs_with
__Triplet-Based_Prefiltering_for_Question_Answering,
    title = {Harnessing the Power of Semi-Structured Knowledge and LLMs with
  Triplet-Based Prefiltering for Question Answering},
    author = {Derian Boer, Fabian Koch, Stefan Kramer},
    abstract = {  Large Language Models (LLMs) frequently lack domain-specific knowledge and
even fine-tuned models tend to hallucinate. Hence, more reliable models that
can include external knowledge are needed. We present a pipeline, 4StepFocus,
and specifically a preprocessing step, that can substantially improve the
answers of LLMs. This is achieved by providing guided access to external
knowledge making use of the model's ability to capture relational context and
conduct rudimentary reasoning by themselves. The method narrows down
potentially correct answers by triplets-based searches in a semi-structured
knowledge base in a direct, traceable fashion, before switching to latent
representations for ranking those candidates based on unstructured data. This
distinguishes it from related methods that are purely based on latent
representations. 4StepFocus consists of the steps: 1) Triplet generation for
extraction of relational data by an LLM, 2) substitution of variables in those
triplets to narrow down answer candidates employing a knowledge graph, 3)
sorting remaining candidates with a vector similarity search involving
associated non-structured data, 4) reranking the best candidates by the LLM
with background data provided. Experiments on a medical, a product
recommendation, and an academic paper search test set demonstrate that this
approach is indeed a powerful augmentation. It not only adds relevant traceable
background information from information retrieval, but also improves
performance considerably in comparison to state-of-the-art methods. This paper
presents a novel, largely unexplored direction and therefore provides a wide
range of future work opportunities. Used source code is available at
https://github.com/kramerlab/4StepFocus.
},
    year = {2024},
    month = {09},
    url = {http://arxiv.org/pdf/2409.00861v1},
    journal = {arXiv preprint}
}

@article{arxiv:From_Large_Language_Models_to_Knowledge_Graphs_for_Biomarker_Discovery
__in_Cancer,
    title = {From Large Language Models to Knowledge Graphs for Biomarker Discovery
  in Cancer},
    author = {Md. Rezaul Karim, Lina Molinas Comet, Md Shajalal, Oya Deniz Beyan, Dietrich Rebholz-Schuhmann, Stefan Decker},
    abstract = {  Domain experts often rely on most recent knowledge for apprehending and
disseminating specific biological processes that help them design strategies
for developing prevention and therapeutic decision-making in various disease
scenarios. A challenging scenarios for artificial intelligence (AI) is using
biomedical data (e.g., texts, imaging, omics, and clinical) to provide
diagnosis and treatment recommendations for cancerous conditions.~Data and
knowledge about biomedical entities like cancer, drugs, genes, proteins, and
their mechanism is spread across structured (knowledge bases (KBs)) and
unstructured (e.g., scientific articles) sources. A large-scale knowledge graph
(KG) can be constructed by integrating and extracting facts about semantically
interrelated entities and relations. Such a KG not only allows exploration and
question answering (QA) but also enables domain experts to deduce new
knowledge. However, exploring and querying large-scale KGs is tedious for
non-domain users due to their lack of understanding of the data assets and
semantic technologies. In this paper, we develop a domain KG to leverage
cancer-specific biomarker discovery and interactive QA. For this, we
constructed a domain ontology called OncoNet Ontology (ONO), which enables
semantic reasoning for validating gene-disease (different types of cancer)
relations. The KG is further enriched by harmonizing the ONO, metadata,
controlled vocabularies, and biomedical concepts from scientific articles by
employing BioBERT- and SciBERT-based information extractors. Further, since the
biomedical domain is evolving, where new findings often replace old ones,
without having access to up-to-date scientific findings, there is a high chance
an AI system exhibits concept drift while providing diagnosis and treatment.
Therefore, we fine-tune the KG using large language models (LLMs) based on more
recent articles and KBs.
},
    year = {2023},
    month = {10},
    url = {http://arxiv.org/pdf/2310.08365v2},
    journal = {arXiv preprint}
}

@article{arxiv:MedTsLLM:_Leveraging_LLMs_for_Multimodal_Medical_Time_Series_Analysis,
    title = {MedTsLLM: Leveraging LLMs for Multimodal Medical Time Series Analysis},
    author = {Nimeesha Chan, Felix Parker, William Bennett, Tianyi Wu, Mung Yao Jia, James Fackler, Kimia Ghobadi},
    abstract = {  The complexity and heterogeneity of data in many real-world applications pose
significant challenges for traditional machine learning and signal processing
techniques. For instance, in medicine, effective analysis of diverse
physiological signals is crucial for patient monitoring and clinical
decision-making and yet highly challenging. We introduce MedTsLLM, a general
multimodal large language model (LLM) framework that effectively integrates
time series data and rich contextual information in the form of text to analyze
physiological signals, performing three tasks with clinical relevance: semantic
segmentation, boundary detection, and anomaly detection in time series. These
critical tasks enable deeper analysis of physiological signals and can provide
actionable insights for clinicians. We utilize a reprogramming layer to align
embeddings of time series patches with a pretrained LLM's embedding space and
make effective use of raw time series, in conjunction with textual context.
Given the multivariate nature of medical datasets, we develop methods to handle
multiple covariates. We additionally tailor the text prompt to include
patient-specific information. Our model outperforms state-of-the-art baselines,
including deep learning models, other LLMs, and clinical methods across
multiple medical domains, specifically electrocardiograms and respiratory
waveforms. MedTsLLM presents a promising step towards harnessing the power of
LLMs for medical time series analysis that can elevate data-driven tools for
clinicians and improve patient outcomes.
},
    year = {2024},
    month = {08},
    url = {http://arxiv.org/pdf/2408.07773v1},
    journal = {arXiv preprint}
}

@article{arxiv:Can_Large_Language_Models_Augment_a_Biomedical_Ontology_with_missing
__Concepts_and_Relations?,
    title = {Can Large Language Models Augment a Biomedical Ontology with missing
  Concepts and Relations?},
    author = {Antonio Zaitoun, Tomer Sagi, Szymon Wilk, Mor Peleg},
    abstract = {  Ontologies play a crucial role in organizing and representing knowledge.
However, even current ontologies do not encompass all relevant concepts and
relationships. Here, we explore the potential of large language models (LLM) to
expand an existing ontology in a semi-automated fashion. We demonstrate our
approach on the biomedical ontology SNOMED-CT utilizing semantic relation types
from the widely used UMLS semantic network. We propose a method that uses
conversational interactions with an LLM to analyze clinical practice guidelines
(CPGs) and detect the relationships among the new medical concepts that are not
present in SNOMED-CT. Our initial experimentation with the conversational
prompts yielded promising preliminary results given a manually generated gold
standard, directing our future potential improvements.
},
    year = {2023},
    month = {11},
    url = {http://arxiv.org/pdf/2311.06858v1},
    journal = {arXiv preprint}
}

@article{arxiv:MedGPTEval:_A_Dataset_and_Benchmark_to_Evaluate_Responses_of_Large
__Language_Models_in_Medicine,
    title = {MedGPTEval: A Dataset and Benchmark to Evaluate Responses of Large
  Language Models in Medicine},
    author = {Jie Xu, Lu Lu, Sen Yang, Bilin Liang, Xinwei Peng, Jiali Pang, Jinru Ding, Xiaoming Shi, Lingrui Yang, Huan Song, Kang Li, Xin Sun, Shaoting Zhang},
    abstract = {  METHODS: First, a set of evaluation criteria is designed based on a
comprehensive literature review. Second, existing candidate criteria are
optimized for using a Delphi method by five experts in medicine and
engineering. Third, three clinical experts design a set of medical datasets to
interact with LLMs. Finally, benchmarking experiments are conducted on the
datasets. The responses generated by chatbots based on LLMs are recorded for
blind evaluations by five licensed medical experts. RESULTS: The obtained
evaluation criteria cover medical professional capabilities, social
comprehensive capabilities, contextual capabilities, and computational
robustness, with sixteen detailed indicators. The medical datasets include
twenty-seven medical dialogues and seven case reports in Chinese. Three
chatbots are evaluated, ChatGPT by OpenAI, ERNIE Bot by Baidu Inc., and Doctor
PuJiang (Dr. PJ) by Shanghai Artificial Intelligence Laboratory. Experimental
results show that Dr. PJ outperforms ChatGPT and ERNIE Bot in both
multiple-turn medical dialogue and case report scenarios.
},
    year = {2023},
    month = {05},
    url = {http://arxiv.org/pdf/2305.07340v1},
    journal = {arXiv preprint}
}

@article{arxiv:MEDIMP:_3D_Medical_Images_with_clinical_Prompts_from_limited_tabular
__data_for_renal_transplantation,
    title = {MEDIMP: 3D Medical Images with clinical Prompts from limited tabular
  data for renal transplantation},
    author = {Leo Milecki, Vicky Kalogeiton, Sylvain Bodard, Dany Anglicheau, Jean-Michel Correas, Marc-Olivier Timsit, Maria Vakalopoulou},
    abstract = {  Renal transplantation emerges as the most effective solution for end-stage
renal disease. Occurring from complex causes, a substantial risk of transplant
chronic dysfunction persists and may lead to graft loss. Medical imaging plays
a substantial role in renal transplant monitoring in clinical practice.
However, graft supervision is multi-disciplinary, notably joining nephrology,
urology, and radiology, while identifying robust biomarkers from such
high-dimensional and complex data for prognosis is challenging. In this work,
taking inspiration from the recent success of Large Language Models (LLMs), we
propose MEDIMP -- Medical Images with clinical Prompts -- a model to learn
meaningful multi-modal representations of renal transplant Dynamic
Contrast-Enhanced Magnetic Resonance Imaging (DCE MRI) by incorporating
structural clinicobiological data after translating them into text prompts.
MEDIMP is based on contrastive learning from joint text-image paired embeddings
to perform this challenging task. Moreover, we propose a framework that
generates medical prompts using automatic textual data augmentations from LLMs.
Our goal is to learn meaningful manifolds of renal transplant DCE MRI,
interesting for the prognosis of the transplant or patient status (2, 3, and 4
years after the transplant), fully exploiting the limited available multi-modal
data most efficiently. Extensive experiments and comparisons with other renal
transplant representation learning methods with limited data prove the
effectiveness of MEDIMP in a relevant clinical setting, giving new directions
toward medical prompts. Our code is available at
https://github.com/leomlck/MEDIMP.
},
    year = {2023},
    month = {03},
    url = {http://arxiv.org/pdf/2303.12445v2},
    journal = {arXiv preprint}
}

@article{arxiv:Leveraging_Professional_Radiologists'_Expertise_to_Enhance_LLMs'
__Evaluation_for_Radiology_Reports,
    title = {Leveraging Professional Radiologists' Expertise to Enhance LLMs'
  Evaluation for Radiology Reports},
    author = {Qingqing Zhu, Xiuying Chen, Qiao Jin, Benjamin Hou, Tejas Sudharshan Mathai, Pritam Mukherjee, Xin Gao, Ronald M Summers, Zhiyong Lu},
    abstract = {  In radiology, Artificial Intelligence (AI) has significantly advanced report
generation, but automatic evaluation of these AI-produced reports remains
challenging. Current metrics, such as Conventional Natural Language Generation
(NLG) and Clinical Efficacy (CE), often fall short in capturing the semantic
intricacies of clinical contexts or overemphasize clinical details, undermining
report clarity. To overcome these issues, our proposed method synergizes the
expertise of professional radiologists with Large Language Models (LLMs), like
GPT-3.5 and GPT-4 1. Utilizing In-Context Instruction Learning (ICIL) and Chain
of Thought (CoT) reasoning, our approach aligns LLM evaluations with
radiologist standards, enabling detailed comparisons between human and AI
generated reports. This is further enhanced by a Regression model that
aggregates sentence evaluation scores. Experimental results show that our
"Detailed GPT-4 (5-shot)" model achieves a 0.48 score, outperforming the METEOR
metric by 0.19, while our "Regressed GPT-4" model shows even greater alignment
with expert evaluations, exceeding the best existing metric by a 0.35 margin.
Moreover, the robustness of our explanations has been validated through a
thorough iterative strategy. We plan to publicly release annotations from
radiology experts, setting a new standard for accuracy in future assessments.
This underscores the potential of our approach in enhancing the quality
assessment of AI-driven medical reports.
},
    year = {2024},
    month = {01},
    url = {http://arxiv.org/pdf/2401.16578v3},
    journal = {arXiv preprint}
}

@article{arxiv:Towards_Expert-Level_Medical_Question_Answering_with_Large_Language
__Models,
    title = {Towards Expert-Level Medical Question Answering with Large Language
  Models},
    author = {Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le Hou, Kevin Clark, Stephen Pfohl, Heather Cole-Lewis, Darlene Neal, Mike Schaekermann, Amy Wang, Mohamed Amin, Sami Lachgar, Philip Mansfield, Sushant Prakash, Bradley Green, Ewa Dominowska, Blaise Aguera y Arcas, Nenad Tomasev, Yun Liu, Renee Wong, Christopher Semturs, S. Sara Mahdavi, Joelle Barral, Dale Webster, Greg S. Corrado, Yossi Matias, Shekoofeh Azizi, Alan Karthikesalingam, Vivek Natarajan},
    abstract = {  Recent artificial intelligence (AI) systems have reached milestones in "grand
challenges" ranging from Go to protein-folding. The capability to retrieve
medical knowledge, reason over it, and answer medical questions comparably to
physicians has long been viewed as one such grand challenge.
  Large language models (LLMs) have catalyzed significant progress in medical
question answering; Med-PaLM was the first model to exceed a "passing" score in
US Medical Licensing Examination (USMLE) style questions with a score of 67.2%
on the MedQA dataset. However, this and other prior work suggested significant
room for improvement, especially when models' answers were compared to
clinicians' answers. Here we present Med-PaLM 2, which bridges these gaps by
leveraging a combination of base LLM improvements (PaLM 2), medical domain
finetuning, and prompting strategies including a novel ensemble refinement
approach.
  Med-PaLM 2 scored up to 86.5% on the MedQA dataset, improving upon Med-PaLM
by over 19% and setting a new state-of-the-art. We also observed performance
approaching or exceeding state-of-the-art across MedMCQA, PubMedQA, and MMLU
clinical topics datasets.
  We performed detailed human evaluations on long-form questions along multiple
axes relevant to clinical applications. In pairwise comparative ranking of 1066
consumer medical questions, physicians preferred Med-PaLM 2 answers to those
produced by physicians on eight of nine axes pertaining to clinical utility (p
< 0.001). We also observed significant improvements compared to Med-PaLM on
every evaluation axis (p < 0.001) on newly introduced datasets of 240 long-form
"adversarial" questions to probe LLM limitations.
  While further studies are necessary to validate the efficacy of these models
in real-world settings, these results highlight rapid progress towards
physician-level performance in medical question answering.
},
    year = {2023},
    month = {05},
    url = {http://arxiv.org/pdf/2305.09617v1},
    journal = {arXiv preprint}
}

@article{arxiv:PromptMind_Team_at_MEDIQA-CORR_2024:_Improving_Clinical_Text_Correction
__with_Error_Categorization_and_LLM_Ensembles,
    title = {PromptMind Team at MEDIQA-CORR 2024: Improving Clinical Text Correction
  with Error Categorization and LLM Ensembles},
    author = {Satya Kesav Gundabathula, Sriram R Kolar},
    abstract = {  This paper describes our approach to the MEDIQA-CORR shared task, which
involves error detection and correction in clinical notes curated by medical
professionals. This task involves handling three subtasks: detecting the
presence of errors, identifying the specific sentence containing the error, and
correcting it. Through our work, we aim to assess the capabilities of Large
Language Models (LLMs) trained on a vast corpora of internet data that contain
both factual and unreliable information. We propose to comprehensively address
all subtasks together, and suggest employing a unique prompt-based in-context
learning strategy. We will evaluate its efficacy in this specialized task
demanding a combination of general reasoning and medical knowledge. In medical
systems where prediction errors can have grave consequences, we propose
leveraging self-consistency and ensemble methods to enhance error correction
and error detection performance.
},
    year = {2024},
    month = {05},
    url = {http://arxiv.org/pdf/2405.08373v1},
    journal = {arXiv preprint}
}

